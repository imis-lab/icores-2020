key,summary,description,assignee
HADOOP-15994,Upgrade Jackson2 to the latest version,Now Jackson 2.9.5 is used and it is vulnerable (CVE-2018-11307). Let's upgrade to 2.9.6 or upper.,jack-lee
HADOOP-15990,S3AFileSystem.verifyBucketExists to move to s3.doesBucketExistV2,"in S3AFileSystem.initialize(), we check for the bucket existing with verifyBucketExists(), which calls s3.doesBucketExist(). But that doesn't check for auth issues. 

s3. doesBucketExistV2() does at least validate credentials, and should be switched to. This will help things fail faster 

See SPARK-24000

(this is a dupe of HADOOP-15409; moving off git PRs so we can get yetus to test everything)",jack-lee
HADOOP-15989,Synchronized at CompositeService#removeService is not required,"Synchronization at CompositeService#removeService method level is not required.

{code}
protected synchronized boolean removeService(Service service) {
synchronized (serviceList) {
return serviceList.remove(service);
}
}
{code}",prabhu joseph
HADOOP-15988,Should be able to set empty directory flag to TRUE in DynamoDBMetadataStore#innerGet when using authoritative directory listings,"We have the following comment and implementation in DynamoDBMetadataStore:
{noformat}
        // When this class has support for authoritative
        // (fully-cached) directory listings, we may also be able to answer
        // TRUE here.  Until then, we don't know if we have full listing or
        // not, thus the UNKNOWN here:
        meta.setIsEmptyDirectory(
            hasChildren ? Tristate.FALSE : Tristate.UNKNOWN);
{noformat}

We have authoritative listings now in dynamo since HADOOP-15621, so we should resolve this comment, implement the solution and test it. ",gabor.bota
HADOOP-15987,ITestDynamoDBMetadataStore should check if test ddb table set properly before initializing the test,"The jira covers the following:
* We should assert that the table name is configured when DynamoDBMetadataStore is used for testing, so the test should fail if it's not configured.
* We should assert that the test table is not the same as the production table, as the test table could be modified and destroyed multiple times during the test.
* This behavior should be added to the testing docs.

[Assume from junit doc|http://junit.sourceforge.net/javadoc/org/junit/Assume.html]:
{noformat}
A set of methods useful for stating assumptions about the conditions in which a test is meaningful. 
A failed assumption does not mean the code is broken, but that the test provides no useful information. 
The default JUnit runner treats tests with failing assumptions as ignored.
{noformat}

A failed assert will cause test failure instead of just skipping the test.",gabor.bota
HADOOP-15985,LightWeightGSet.computeCapacity() doesn't correctly account for CompressedOops,"In this line: [https://github.com/apache/hadoop/blob/a55d6bba71c81c1c4e9d8cd11f55c78f10a548b0/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/LightWeightGSet.java#L391], instead of checking if the platform is 32- or 64-bit, it should check if Unsafe.ARRAY_OBJECT_INDEX_SCALE is 4 or 8.

The result is that on 64-bit platforms, when Compressed Oops are on, LightWeightGSet is two times denser than it is configured to be.",jack-lee
HADOOP-15982,Support configurable trash location,"Currently some customer has users accounts that are functional ids (fid) to manage application and application data under the path /data/FID. These fid's also get a home directory under /user path. The user's home directories are limited with space quota 60 G. When these fids delete data, due to customer deletion policy they are placed in /user/<FID>/.Trash location and run over quota.

For now they are increasing quotas for these functional users, but considering growing applications they would like the .Trash location to be configurable or something like  /trash/\{userid} that is owned by the user.

What should the configurable path look like to make this happen? For example, some thoughts may relate whether we want to configure it for per user or per cluster, etc.

Here is current behavior:

fs.TrashPolicyDefault: Moved: 'hdfs://ns1/user/hdfs/test/1.txt to trash at: hdfs://ns1/user/hdfs/.Trash/Current/user/hdfs/test/1.txt

for path under encryption zone:

fs.TrashPolicyDefault: Moved: 'hdfs://ns1/scale/2.txt' to trash at hdfs://ns1/scale/.Trash/hdfs/Current/scale/2.txt

 ",ghuangups
HADOOP-15981,Add mutual TLS support for RPC,"The RPC server should allow optionally enabling mutual TLS as 1st class authentication.  If enabled, a client cert may provide the user's identity or fallback to kerberos or token.  Essentially the placeholder CERTIFICATE authentication method will be implemented and offered as an authentication method during connection negotiation.",daryn
HADOOP-15980,Enable TLS in RPC client/server,"Once the RPC client and server can be configured to use Netty, the TLS engine can be added to the channel pipeline.  The server should allow QoS-like functionality to determine if TLS is mandatory or optional for a client.",daryn
HADOOP-15979,Add Netty support to the RPC client,Adding Netty will allow later using a native TLS transport layer with much better performance than that offered by Java's SSLEngine.,daryn
HADOOP-15978,Add Netty support to the RPC server,Adding Netty will allow later using a native TLS transport layer with much better performance than that offered by Java's SSLEngine.,daryn
HADOOP-15977,RPC support for TLS,Umbrella ticket to track adding TLS and mutual TLS support to RPC.,daryn
HADOOP-15975,ABFS: remove timeout check for DELETE and RENAME,"Currently, ABFS rename and delete is doing a timeout check, which will fail the request for rename/delete when the target contains tons of file/dirs.

Because timeout check is already there for each HTTP call, we should remove the timeout check in RENAME and DELETE.",danielzhou
HADOOP-15974,Upgrade Curator version to 2.13.0 to fix ZK tests,"TestLeaderElectorService hangs waiting for the TestingZooKeeperServer to start and eventually gets killed by the surefire timeout.
",ajisakaa
HADOOP-15973,Configuration: Included properties are not cached if resource is a stream,"If a configuration resource is a bufferedinputstream and the resource has an included xml file, the properties from the included file are read and stored in the properties of the configuration, but they are not stored in the resource cache. So, if a later resource is added to the config and the properties are recalculated from the first resource, the included properties are lost.",eepayne
HADOOP-15972,ABFS: update LIST_MAX_RESULTS,"This will be the temporary fix, as the service fix take much longer time to roll out.",danielzhou
HADOOP-15970,Upgrade plexus-utils from 2.0.5 to 3.1.0,"Apache Hadoop uses plexus-utils 2.0.5 and it is vulnerable.
https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2017-1000487

Let's update the version or remove the usage of this library.",ajisakaa
HADOOP-15969,ABFS: getNamespaceEnabled can fail blocking user access thru ACLs,"The Get Filesystem Properties operation requires Read permission to the Filesystem.  Read permission to the Filesystem can only be granted thru RBAC, Shared Key, or SAS.  This prevents giving low privilege users access to specific files or directories within the filesystem.  An administrator should be able to set an ACL on a file granting read permission to a user, without giving them read permission to the entire Filesystem.

Fortunately there is another way to determine if HNS is enabled.  The Get Path Access Control (getAclStatus) operation only requires traversal access, and for the root folder / all authenticated users have traversal access.",danielzhou
HADOOP-15968,ABFS: add try catch for UGI failure when initializing ABFS,"There are cases that primary group doesn't exist for the user, need to add try catch when fetching usergroup info.",danielzhou
HADOOP-15966,Hadoop Kerberos broken on macos as java.security.krb5.realm is reset: Null realm name (601),"HADOOP-8719 may have been valid at some point, but since then the world has moved on

* nobody should be using Hadoop without Kerberos
* kerberos support on java 8 + macos works just fine

The clause to reset the java system properties is enough to stop kerberos working, even
when the user is kinited in.

Propose: remove/revert the changes. If people want it, they can set the options in {{HADOOP_CLIENT_OPTS}}",stevel@apache.org
HADOOP-15965,Upgrade to ADLS SDK which has major performance improvement for ingress/egress,"Upgrade ADLS SDK to version 2.3.2 which has major improvements
 # Add special handling for 404 errors when requesting tokens from MSI
 # Fix liststatus response parsing when filestatus object contains array in one field.
 # Use wildfly openssl native binding with Java. This is a workaround to [https://bugs.openjdk.java.net/browse/JDK-8046943]issue. 2X performance boost over HTTPS. Similar to HADOOP-15669",vishwajeet.dusane
HADOOP-15962,FileUtils Small Buffer Size,"Note sure if this code is even being used, but it implements a copy routing utilizing a 2K buffer.  Modern JVM uses 8K, but 4K should be minimum.  Also, there are libraries for this stuff.

{code:java|title=FileUtil.java}
    int count;
    byte data[] = new byte[2048];
    try (BufferedOutputStream outputStream = new BufferedOutputStream(
        new FileOutputStream(outputFile));) {

      while ((count = tis.read(data)) != -1) {
        outputStream.write(data, 0, count);
      }

      outputStream.flush();
    }
{code}

I also fixed a couple of check-style warnings.",belugabehr
HADOOP-15960,Update guava to 27.0-jre in hadoop-common,com.google.guava:guava should be upgraded to 27.0-jre due to new CVE's found [CVE-2018-10237|https://nvd.nist.gov/vuln/detail/CVE-2018-10237].,gabor.bota
HADOOP-15959,revert HADOOP-12751,"HADOOP-12751 doesn't quite work right. Revert.

(this patch is so jenkins can do the test runs)",stevel@apache.org
HADOOP-15958,Revisiting LICENSE and NOTICE files,"Originally reported from [~jmclean]:
* NOTICE file incorrectly lists copyrights that shouldn't be there and mentions licenses such as MIT, BSD, and public domain that should be mentioned in LICENSE only.
* It's better to have a separate LICENSE and NOTICE for the source and binary releases.

http://www.apache.org/dev/licensing-howto.html",ajisakaa
HADOOP-15957,WASB: Add asterisk wildcard support for PageBlobDirSet,"In WASB, property ""*fs.azure.page.blob.dir*"" only support literal directory name.
We need to add support for wildcard '*' to represent for any directory name.
For example, the following pattern should be supported:

{code:java}
/dir1/dir2 
/dir1/*/dir3
/dir1/*/*/dir4
/dir1/*/*/file
{code}
",danielzhou
HADOOP-15954,ABFS: Enable owner and group conversion for MSI and login user using OAuth,"Add support for overwriting owner and group in set/get operations to be the service principal id when OAuth is used. Add support for upn short name format.

 

Add Standard Transformer for SharedKey / Service 

Add interface provides an extensible model for customizing the acquisition of Identity Transformer.",danielzhou
HADOOP-15953,AliyunOSS: make AliyunCredentialsProvider compatible with the required constructor,"Hadoop aliyun module uses AliyunCredentialsProvider as default CredentialsProvider if user do not set fs.oss.credentials.provider. However, if user set fs.oss.credentials.provider to org.apache.hadoop.fs.aliyun.oss.AliyunCredentialsProvider, exception will be thrown like below:
{code:java}
java.io.IOException: org.apache.hadoop.fs.aliyun.oss.AliyunCredentialsProvider constructor exception. A class specified in fs.oss.credentials.provider must provide an accessible constructor accepting URI and Configuration, or an accessible default constructor.

 at org.apache.hadoop.fs.aliyun.oss.AliyunOSSUtils.getCredentialsProvider(AliyunOSSUtils.java:131)
 at org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore.initialize(AliyunOSSFileSystemStore.java:154)
 at org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem.initialize(AliyunOSSFileSystem.java:344)
 at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3302)
 at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)
 at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3351)
 at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3319)
 at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)
 at org.apache.hadoop.fs.contract.AbstractBondedFSContract.init(AbstractBondedFSContract.java:72)
 at org.apache.hadoop.fs.contract.AbstractFSContractTestBase.setup(AbstractFSContractTestBase.java:177)
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:498)
 at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
 at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
 at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
 at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
 at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
 at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
 at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298)
 at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292)
 at java.util.concurrent.FutureTask.run(FutureTask.java:266)
 at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.NoSuchMethodException: org.apache.hadoop.fs.aliyun.oss.AliyunCredentialsProvider.<init>()
 at java.lang.Class.getConstructor0(Class.java:3082)
 at java.lang.Class.getDeclaredConstructor(Class.java:2178)
 at org.apache.hadoop.fs.aliyun.oss.AliyunOSSUtils.getCredentialsProvider(AliyunOSSUtils.java:125)
 ... 23 more
{code}
Because AliyunCredentialsProvider does not have corresponding constructor.
{code:java}
public class AliyunCredentialsProvider implements CredentialsProvider {
  private Credentials credentials = null;

  public AliyunCredentialsProvider(Configuration conf) throws IOException {

--------------------------------------------------
try {
  credentials =
      (CredentialsProvider)credClass.getDeclaredConstructor(
          URI.class, Configuration.class).newInstance(uri, conf);
} catch (NoSuchMethodException | SecurityException e) {
  credentials =
      (CredentialsProvider)credClass.getDeclaredConstructor()
      .newInstance();
}
{code}
 

*Although the documentation says you should provide a `specified class must provide an accessible constructor accepting java.net.URI and org.apache.hadoop.conf.Configuration` if you set this configuration*, we should make 

*AliyunCredentialsProvider* compatible with this.
{code:java}
<property>
  <name>fs.oss.credentials.provider</name>
  <description>
    Class name of a credentials provider that implements
    com.aliyun.oss.common.auth.CredentialsProvider. Omit if using access/secret keys
    or another authentication mechanism. The specified class must provide an
    accessible constructor accepting java.net.URI and
    org.apache.hadoop.conf.Configuration, or an accessible default constructor.
  </description>
</property>
{code}",wujinhu
HADOOP-15950,Failover for LdapGroupsMapping,"Currently, LdapGroupsMapping supports only a single ldap server url, this can obviously cause issues if the ldap instance goes down. This JIRA attempts to improve this by allowing users to list multiple ldap server urls, and performing a failover if we detect any issues.",lukmajercak
HADOOP-15948,Inconsistency in get and put syntax if filename/dirname contains space,"Inconsistency in get and put syntax if file/fdir name contains space. 

While copying file/dir from local to HDFS, space needs to be represented with %20. However, the same representation does not work for copying file to Local. Expectaion is to have same syntax for both get and put.

test:/ # mkdir /opt/
 test:/ # mkdir /opt/test\ space
 test:/ # vi /opt/test\ space/test\ file.txt
 test:/ # ll /opt/test\ space/
 total 4
 -rw-r--r-- 1 root root 7 Sep 12 18:37 test file.txt
 test:/ #
 *test:/ # hadoop fs -put /opt/test\ space/ /tmp/*
 *put: unexpected URISyntaxException*
 test:/ #
 *test:/ # hadoop fs -put /opt/test%20space/ /tmp/*
 test:/ #
 test:/ # hadoop fs -ls /tmp
 drwxr-xr-x - user1 hadoop 0 2018-09-12 18:38 /tmp/test space
 test:/ #
 *test:/ # hadoop fs -get /tmp/test%20space /srv/*
 *get: `/tmp/test%20space': No such file or directory*
 test:/ #
 *test:/ # hadoop fs -get /tmp/test\ space /srv/*
 test:/ # ll /srv/test\ space/
 total 4
 -rw-r--r-- 1 root root 7 Sep 12 18:39 test file.txt",ayushtkn
HADOOP-15947,Fix ITestDynamoDBMetadataStore test error issues,"When running regression hadoop-aws integration tests for HADOOP-15370, I got the following errors in ITestDynamoDBMetadataStore: 
{noformat}
[ERROR] Tests run: 40, Failures: 4, Errors: 2, Skipped: 0, Time elapsed: 177.303 s <<< FAILURE! - in org.apache.hadoop.fs.s3a.s3guard.ITestDynamoDBMetadataStore
[ERROR] testBatchWrite(org.apache.hadoop.fs.s3a.s3guard.ITestDynamoDBMetadataStore)  Time elapsed: 1.262 s  <<< ERROR!
java.lang.NullPointerException
	at org.apache.hadoop.fs.s3a.s3guard.ITestDynamoDBMetadataStore.doTestBatchWrite(ITestDynamoDBMetadataStore.java:365) (...)

[ERROR] testProvisionTable(org.apache.hadoop.fs.s3a.s3guard.ITestDynamoDBMetadataStore)  Time elapsed: 11.394 s  <<< FAILURE!
java.lang.AssertionError: expected:<20> but was:<10> (...)
org.apache.hadoop.fs.s3a.s3guard.ITestDynamoDBMetadataStore.testProvisionTable(ITestDynamoDBMetadataStore.java:594) (...)

[ERROR] testPruneUnsetsAuthoritative(org.apache.hadoop.fs.s3a.s3guard.ITestDynamoDBMetadataStore)  Time elapsed: 2.323 s  <<< ERROR!
java.lang.NullPointerException
	at org.apache.hadoop.fs.s3a.s3guard.MetadataStoreTestBase.testPruneUnsetsAuthoritative(MetadataStoreTestBase.java:737) (...)

[ERROR] testDeleteSubtree(org.apache.hadoop.fs.s3a.s3guard.ITestDynamoDBMetadataStore)  Time elapsed: 2.377 s  <<< FAILURE!
java.lang.AssertionError: Directory /ADirectory2 is null in cache (...)

[ERROR] testPutNew(org.apache.hadoop.fs.s3a.s3guard.ITestDynamoDBMetadataStore)  Time elapsed: 1.466 s  <<< FAILURE!
java.lang.AssertionError: Directory /da2 is null in cache (...)

[ERROR] testDeleteSubtreeHostPath(org.apache.hadoop.fs.s3a.s3guard.ITestDynamoDBMetadataStore)  Time elapsed: 2.378 s  <<< FAILURE!
java.lang.AssertionError: Directory s3a://cloudera-dev-gabor-ireland/ADirectory2 is null in cache (...)
{noformat}

I create this jira to handle and fix all of these issues.",gabor.bota
HADOOP-15945,ABFS: replace $superuser with local user,"if the owner of file/path is $superUser, we should  replace it with local user.",danielzhou
HADOOP-15943,AliyunOSS: add missing owner & group attributes for oss FileStatus,"Owner & group attributes are missing when you list oss objects via hadoop command:
{code:java}
Found 6 items
drwxrwxrwx - 0 2018-08-01 21:37 /1024
drwxrwxrwx - 0 2018-10-30 11:07 /50
-rw-rw-rw- 1 94070 2018-11-08 21:48 /a
-rw-rw-rw- 1 2441079322 2018-10-31 10:14 /lineitem.csv
drwxrwxrwx - 0 1970-01-01 08:00 /tmp
drwxrwxrwx - 0 1970-01-01 08:00 /user
{code}
 

The result should like below(hadoop fs -ls hdfs://master:8020/):
{code:java}
Found 5 items
drwxr-xr-x - hbase hbase 0 2018-11-18 17:31 hdfs://master:8020/hbase
drwxrwxrwt - hdfs supergroup 0 2018-10-30 11:07 hdfs://master:8020/tmp
drwxr-xr-x - hdfs supergroup 0 2018-10-30 10:39 hdfs://master:8020/user
{code}
However, oss objects do not have owner & group attributes like hadoop files,  so we assume both owner & group are the current user at the time the FS was instantiated.",wujinhu
HADOOP-15940,"ABFS: For HNS account, avoid unnecessary get call when doing Rename","When rename, there is always a GET dst file status call, this is not necessary.",danielzhou
HADOOP-15939,Filter overlapping objenesis class in hadoop-client-minicluster ,"As mentioned here and found in with latest Jenkins shadedclient.

Jenkins does not provide a detailed output file for the failure though. But it can be reproed with the following cmd:
{code:java}
mvn verify -fae --batch-mode -am -pl hadoop-client-modules/hadoop-client-check-invariants -pl hadoop-client-modules/hadoop-client-check-test-invariants -pl hadoop-client-modules/hadoop-client-integration-tests -Dtest=NoUnitTests -Dmaven.javadoc.skip=true -Dcheckstyle.skip=true -Dfindbugs.skip=true

{code}
Error Message:
{code:java}
[WARNING] objenesis-1.0.jar, mockito-all-1.8.5.jar define 30 overlapping classes: 

[WARNING]   - org.objenesis.ObjenesisBase

[WARNING]   - org.objenesis.instantiator.gcj.GCJInstantiator

[WARNING]   - org.objenesis.ObjenesisHelper

[WARNING]   - org.objenesis.instantiator.jrockit.JRockitLegacyInstantiator

[WARNING]   - org.objenesis.instantiator.sun.SunReflectionFactoryInstantiator

[WARNING]   - org.objenesis.instantiator.ObjectInstantiator

[WARNING]   - org.objenesis.instantiator.gcj.GCJInstantiatorBase$DummyStream

[WARNING]   - org.objenesis.instantiator.basic.ObjectStreamClassInstantiator

[WARNING]   - org.objenesis.ObjenesisException

[WARNING]   - org.objenesis.Objenesis

[WARNING]   - 20 more...

[WARNING] maven-shade-plugin has detected that some class files are

[WARNING] present in two or more JARs. When this happens, only one

[WARNING] single version of the class is copied to the uber jar.

[WARNING] Usually this is not harmful and you can skip these warnings,

[WARNING] otherwise try to manually exclude artifacts based on

[WARNING] mvn dependency:tree -Ddetail=true and the above output.

[WARNING] See [http://maven.apache.org/plugins/maven-shade-plugin/]

[INFO] Replacing original artifact with shaded artifact.

{code}
 ",xyao
HADOOP-15936,[JDK 11] MiniDFSClusterManager & MiniHadoopClusterManager compilation fails due to the usage of '_' as identifier,"{code:xml}
[ERROR] COMPILATION ERROR :
[INFO] -------------------------------------------------------------
[ERROR] /hadoop/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/test/MiniDFSClusterManager.java:[130,37] as of release 9, '_' is a keyword, and may not be used as an identifier
[INFO] 1 error
{code}

{code:xml}
[ERROR] COMPILATION ERROR :
[INFO] -------------------------------------------------------------
[ERROR] /hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/MiniHadoopClusterManager.java:[140,37] as of release 9, '_' is a keyword, and may not be used as an identifier
[INFO] 1 error
{code}
",zvenczel
HADOOP-15934,ABFS: make retry policy configurable,"Currently the retry policy parameter is hard coded, should make it configurable for user.",danielzhou
HADOOP-15932,Oozie unable to create sharelib in s3a filesystem,"Oozie server unable to start cause of below exception.
s3a expects a file to copy it in store but sharelib is a folder containing all the needed components jars.
Hence throws the exception :
_Not a file: /usr/hdp/current/oozie-server/share/lib_

{code:java}
[oozie@sg-hdp1 ~]$ /usr/hdp/current/oozie-server/bin/oozie-setup.sh sharelib create -fs s3a://hdp -locallib /usr/hdp/current/oozie-server/share
  setting OOZIE_CONFIG=${OOZIE_CONFIG:-/usr/hdp/current/oozie-client/conf}
  setting CATALINA_BASE=${CATALINA_BASE:-/usr/hdp/current/oozie-client/oozie-server}
  setting CATALINA_TMPDIR=${CATALINA_TMPDIR:-/var/tmp/oozie}
  setting OOZIE_CATALINA_HOME=/usr/lib/bigtop-tomcat
  setting JAVA_HOME=/usr/jdk64/jdk1.8.0_112
  setting JRE_HOME=${JAVA_HOME}
  setting CATALINA_OPTS=""$CATALINA_OPTS -Xmx2048m""
  setting OOZIE_LOG=/var/log/oozie
  setting CATALINA_PID=/var/run/oozie/oozie.pid
  setting OOZIE_DATA=/hadoop/oozie/data
  setting OOZIE_HTTP_PORT=11000
  setting OOZIE_ADMIN_PORT=11001
  setting JAVA_LIBRARY_PATH=/usr/hdp/3.0.0.0-1634/hadoop/lib/native/Linux-amd64-64
  setting OOZIE_CLIENT_OPTS=""${OOZIE_CLIENT_OPTS} -Doozie.connection.retry.count=5 ""
  setting OOZIE_CONFIG=${OOZIE_CONFIG:-/usr/hdp/current/oozie-client/conf}
  setting CATALINA_BASE=${CATALINA_BASE:-/usr/hdp/current/oozie-client/oozie-server}
  setting CATALINA_TMPDIR=${CATALINA_TMPDIR:-/var/tmp/oozie}
  setting OOZIE_CATALINA_HOME=/usr/lib/bigtop-tomcat
  setting JAVA_HOME=/usr/jdk64/jdk1.8.0_112
  setting JRE_HOME=${JAVA_HOME}
  setting CATALINA_OPTS=""$CATALINA_OPTS -Xmx2048m""
  setting OOZIE_LOG=/var/log/oozie
  setting CATALINA_PID=/var/run/oozie/oozie.pid
  setting OOZIE_DATA=/hadoop/oozie/data
  setting OOZIE_HTTP_PORT=11000
  setting OOZIE_ADMIN_PORT=11001
  setting JAVA_LIBRARY_PATH=/usr/hdp/3.0.0.0-1634/hadoop/lib/native/Linux-amd64-64
  setting OOZIE_CLIENT_OPTS=""${OOZIE_CLIENT_OPTS} -Doozie.connection.retry.count=5 ""
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/hdp/3.0.0.0-1634/oozie/lib/slf4j-simple-1.6.6.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/hdp/3.0.0.0-1634/oozie/libserver/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/hdp/3.0.0.0-1634/oozie/libserver/slf4j-log4j12-1.6.6.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.SimpleLoggerFactory]
518 [main] WARN org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
605 [main] INFO org.apache.hadoop.conf.Configuration.deprecation - mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir
619 [main] INFO org.apache.hadoop.security.SecurityUtil - Updating Configuration
the destination path for sharelib is: /user/oozie/share/lib/lib_20181114154552
log4j:WARN No appenders could be found for logger (org.apache.htrace.core.Tracer).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
1118 [main] WARN org.apache.hadoop.metrics2.impl.MetricsConfig - Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties
1172 [main] INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl - Scheduled Metric snapshot period at 10 second(s).
1172 [main] INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl - s3a-file-system metrics system started
2255 [main] INFO org.apache.hadoop.conf.Configuration.deprecation - fs.s3a.server-side-encryption-key is deprecated. Instead, use fs.s3a.server-side-encryption.key

Error: Not a file: /usr/hdp/current/oozie-server/share/lib

Stack trace for the error was (for debug purposes):
--------------------------------------
java.io.FileNotFoundException: Not a file: /usr/hdp/current/oozie-server/share/lib
	at org.apache.hadoop.fs.s3a.S3AFileSystem.innerCopyFromLocalFile(S3AFileSystem.java:2375)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.copyFromLocalFile(S3AFileSystem.java:2339)
	at org.apache.hadoop.fs.FileSystem.copyFromLocalFile(FileSystem.java:2386)
	at org.apache.oozie.tools.OozieSharelibCLI.run(OozieSharelibCLI.java:182)
	at org.apache.oozie.tools.OozieSharelibCLI.main(OozieSharelibCLI.java:67)
--------------------------------------

2268 [pool-2-thread-1] INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl - Stopping s3a-file-system metrics system...
2268 [pool-2-thread-1] INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl - s3a-file-system metrics system stopped.
2268 [pool-2-thread-1] INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl - s3a-file-system metrics system shutdown complete.
{code}
 
These logs are with DEBUG mode on.
_log4j.logger.org.apache.hadoop.fs.s3a=DEBUG_",stevel@apache.org
HADOOP-15931,support 'hadoop key create' with user specified key material,{{hadoop key create}} command should support creation of keys with user specified key material.,vinayrpet
HADOOP-15930,Exclude MD5 checksum files from release artifact,"create-release script creates md5 checksum files, but now md5 checksum is useless.

https://www.apache.org/dev/release-distribution.html#sigs-and-sums
bq. For new releases, PMCs MUST supply SHA-256 and/or SHA-512; and SHOULD NOT supply MD5 or SHA-1. Existing releases do not need to be changed.",ajisakaa
HADOOP-15926,Document upgrading the section in NOTICE.txt when upgrading the version of AWS SDK,"Reported by [~stevel@apache.org]

bq. Hadoop 3.2 + has a section in hadoop-tools/hadoop-aws/src/site/markdown/tools/hadoop-aws/testing.md about what to do when updating the SDK...this needs to be added there. Anyone fancy supplying a patch?

https://issues.apache.org/jira/browse/HADOOP-15899?focusedCommentId=16675121&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16675121",dineshchitlangia
HADOOP-15925,The config and log of gpg-agent are removed in create-release script,"The config file and log file of gpg-agent are located at {{patchprocess}} directory, and then, {{git clean -xdf}} removes the directory. That way the config and log of gpg-agent are lost.",dineshchitlangia
HADOOP-15923,create-release script should set max-cache-ttl as well as default-cache-ttl for gpg-agent,"create-release script sets default-cache-ttl for gpg-agent to 14400 (4 hours), however, max-cache-ttl is not set and the default value is 7200 (2 hours).
If mvn full install takes more than 2 hours, gpg-agent will fail to sign.",ajisakaa
HADOOP-15922,DelegationTokenAuthenticationFilter get wrong doAsUser since it does not decode URL,"DelegationTokenAuthenticationFilter get wrong doAsUser when proxy user from client is complete kerberos name (e.g., user/hostname@REALM.COM, actually it is acceptable), because DelegationTokenAuthenticationFilter does not decode DOAS parameter in URL which is encoded by {{URLEncoder}} at client.
e.g. KMS as example:
a. KMSClientProvider creates connection to KMS Server using DelegationTokenAuthenticatedURL#openConnection.
b. If KMSClientProvider is a doAsUser, KMSClientProvider will put {{doas}} with url encoded user as one parameter of http request. 
{code:java}
    // proxyuser
    if (doAs != null) {
      extraParams.put(DO_AS, URLEncoder.encode(doAs, ""UTF-8""));
    }
{code}
c. when KMS server receives the request, it does not decode the proxy user.

As result, KMS Server will get the wrong proxy user if this proxy user is complete Kerberos Name or it includes some special character. Some other authentication and authorization exception will throws next to it.",hexiaoqiao
HADOOP-15920,"get patch for S3a nextReadPos(), through Yetus",,jack-lee
HADOOP-15919,AliyunOSS: Enable Yarn to use OSS,Uses DelegateToFileSystem to expose AliyunOSSFileSystem as an AbstractFileSystem,wujinhu
HADOOP-15918,Namenode gets stuck when deleting large dir in trash,"Similar to the situation discussed in HDFS-13671, Namenode gets stuck for a long time when deleting trash dir with a large mount of data. We found log in namenode:

{quote}

2018-06-08 20:00:59,042 INFO namenode.FSNamesystem (FSNamesystemLock.java:writeUnlock(252)) - FSNamesystem write lock held for 23018 ms via
java.lang.Thread.getStackTrace(Thread.java:1552)
org.apache.hadoop.util.StringUtils.getStackTrace(StringUtils.java:1033)
org.apache.hadoop.hdfs.server.namenode.FSNamesystemLock.writeUnlock(FSNamesystemLock.java:254)
org.apache.hadoop.hdfs.server.namenode.FSNamesystem.writeUnlock(FSNamesystem.java:1567)
org.apache.hadoop.hdfs.server.namenode.FSNamesystem.delete(FSNamesystem.java:2820)
org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.delete(NameNodeRpcServer.java:1047)

{quote}

One simple solution is to avoid deleting large data in one delete RPC call. We implement a trashPolicy that divide the delete operation into several delete RPCs, and each single deletion would not delete too many files.

Any thought? [~linyiqun]",tao jie
HADOOP-15917,AliyunOSS: fix incorrect ReadOps and WriteOps in statistics,,wujinhu
HADOOP-15916,Upgrade Maven Surefire plugin to 3.0.0-M1,Recently all the unit tests are failing. This is caused by the latest Java 8 issue reported at SUREFIRE-1588 and fixed in Maven Surefire plugin 3.0.0-M1. We need to update the plugin.,ajisakaa
HADOOP-15914,hadoop jar command has no help argument,"{{hadoop jar --help}} and {{hadoop jar help}} commands show outputs like this:
{noformat}
WARNING: Use ""yarn jar"" to launch YARN applications.
JAR does not exist or is not a normal file: /root/--help
{noformat}
Only if called with no arguments: {{hadoop jar}} we get the usage text, but even in that case we get:
{noformat}
WARNING: Use ""yarn jar"" to launch YARN applications.
RunJar jarFile [mainClass] args...
{noformat}
Where RunJar is wrapped by the hadoop script (so it should not be displayed).

{{hadoop --help}} displays the following:
{noformat}
jar <jar>     run a jar file. NOTE: please use ""yarn jar"" to launch YARN applications, not this command.
{noformat}
which is fine, but {{CommandsManual.md}} tells a bit more information about the usage of this command:
{noformat}
Usage: hadoop jar <jar> [mainClass] args...
{noformat}

My suggestion is to add a {{--help}} option to the {{hadoop jar}} command that would display this message.",adam.antal
HADOOP-15912,start-build-env.sh still creates an invalid /etc/sudoers.d/hadoop-build-${USER_ID} file entry after HADOOP-15802,"{noformat}
RUN echo -e ""${USER_NAME}\tALL=NOPASSWD: ALL"" > ""/etc/sudoers.d/hadoop-build-${USER_ID}""
{noformat}
creates
{noformat}
-e <USER_NAME>	ALL=NOPASSWD: ALL
{noformat}",ajisakaa
HADOOP-15909,KeyProvider class should implement Closeable,KeyProviders and the extensions have close() methods. The classes should implement Closeable that will allow try-with-resources to work and help catch original exceptions instead of finally blocks and exception masking that follows that.,kshukla
HADOOP-15908,hadoop-build-tools jar is downloaded from remote repository instead of using from local,"HADOOP-12893 added ""maven-remote-resources-plugin"" to hadoop-project/pom.xml to verify LICENSE.txt and NOTICE.txt files which includes ""hadoop-build-tools"" remote resource bundles. 
{code}
<plugin>
 <groupId>org.apache.maven.plugins</groupId>
 <artifactId>maven-remote-resources-plugin</artifactId>
 <version>${maven-remote-resources-plugin.version}</version>
 <configuration>
 <resourceBundles>
 <resourceBundle>org.apache.hadoop:hadoop-build-tools:${hadoop.version}</resourceBundle>
 </resourceBundles>
 </configuration>
 <dependencies>
 <dependency>
 <groupId>org.apache.hadoop</groupId>
 <artifactId>hadoop-build-tools</artifactId>
 <version>${hadoop.version}</version>
 </dependency>
 </dependencies>
 <executions>
 <execution>
 <goals>
 <goal>process</goal>
 </goals>
 </execution>
 </executions>
 </plugin>
{code}

If we build only some module we always download "" hadoop-build-tools"" from maven repository.

For example run:
cd hadoop-common-project/
mvn test
Then we will get the following output:
{noformat}
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hadoop-annotations ---
Downloading from apache.snapshots: http://repository.apache.org/snapshots/org/apache/hadoop/hadoop-build-tools/3.3.0-SNAPSHOT/maven-metadata.xml
Downloaded from apache.snapshots: http://repository.apache.org/snapshots/org/apache/hadoop/hadoop-build-tools/3.3.0-SNAPSHOT/maven-metadata.xml (791 B at 684 B/s)
Downloading from apache.snapshots: http://repository.apache.org/snapshots/org/apache/hadoop/hadoop-main/3.3.0-SNAPSHOT/maven-metadata.xml
Downloaded from apache.snapshots: http://repository.apache.org/snapshots/org/apache/hadoop/hadoop-main/3.3.0-SNAPSHOT/maven-metadata.xml (609 B at 547 B/s)
Downloading from apache.snapshots.https: https://repository.apache.org/content/repositories/snapshots/org/apache/hadoop/hadoop-build-tools/3.3.0-SNAPSHOT/maven-metadata.xml
Downloaded from apache.snapshots.https: https://repository.apache.org/content/repositories/snapshots/org/apache/hadoop/hadoop-build-tools/3.3.0-SNAPSHOT/maven-metadata.xml (791 B at 343 B/s)
Downloading from apache.snapshots.https: https://repository.apache.org/content/repositories/snapshots/org/apache/hadoop/hadoop-build-tools/3.3.0-SNAPSHOT/hadoop-build-tools-3.3.0-20181022.232020-179.jar
Downloaded from apache.snapshots.https: https://repository.apache.org/content/repositories/snapshots/org/apache/hadoop/hadoop-build-tools/3.3.0-SNAPSHOT/hadoop-build-tools-3.3.0-20181022.232020-179.jar (0 B at 0 B/s)
{noformat}

If ""hadoop-build-tools"" jar doesn't exist in maven repository (for example we try to build new version locally before repository will be created ) we can't build some module:
For example run:
cd hadoop-common-project/
mvn test
Then we will get the following output:
{noformat}
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-remote-resources-plugin:1.5:process (default) on project hadoop-annotations: Execution default of goal org.apache.maven.plugins:maven-remote-resources-plugin:1.5:process failed: Plugin org.apache.maven.plugins:maven-remote-resources-plugin:1.5 or one of its dependencies could not be resolved: Failure to find org.apache.hadoop:hadoop-build-tools:jar:3.2.0 in https://repo.maven.apache.org/maven2 was cached in the local repository, resolution will not be reattempted until the update interval of central has elapsed or updates are forced -> [Help 1]
{noformat}

Therefore, we need to limit execution of the Remote Resources Plugin only in the root directory in which the build was run.
To accomplish this, we can use the ""runOnlyAtExecutionRoot parameter""
From maven documentation http://maven.apache.org/plugins/maven-remote-resources-plugin/usage.html",oshevchenko
HADOOP-15907,Add missing maven modules in BUILDING.txt,"In the Maven main modules section, I found the missing main module YARN.",jiwq
HADOOP-15904,"[JDK 11] Javadoc build failed due to ""bad use of '>'"" in hadoop-hdfs","{noformat}
$ mvn javadoc:javadoc -pl hadoop-hdfs-project/hadoop-hdfs
...
[ERROR] /hadoop/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/planner/package-info.java:32: error: bad use of '>'
[ERROR]  *   // Creates a plan , like move 20 GB data from v1 -> v2
{noformat}",tasanuma0829
HADOOP-15903,Allow HttpServer2 to discover resources in /static when symlinks are used,"Currently, we instantiate /static with the default settings.
However, if this folder is behind a symbolic link, this won't load.l
This is exactly the same issue and solution as described in GEODE-5445.",elgoiri
HADOOP-15902,[JDK 11] Specify the HTML version of Javadoc to 4.01,"From JDK9, Javadoc has options to specify the HTML version.
{noformat}
$ javadoc --help
...
-html4        Generate HTML 4.01 output
-html5        Generate HTML 5 output
{noformat}
JDK8 supports only HTML 4.01 output. The default html version of JDK9 and JDK10 is 4.01. That of JDK11 is 5. Let's unify the version to 4.01 for now.",tasanuma0829
HADOOP-15901,IPC Client and Server should use Time.monotonicNow() for elapsed times.,"Client.java and Server.java  uses {{Time.now()}} to calculate the elapsed times/timeouts. This could result in undesired results when system clock's time changes.

{{Time.monotonicNow()}} should be used for elapsed time calculations within same JVM.

 ",vinayrpet
HADOOP-15900,Update JSch versions in LICENSE.txt,"HADOOP-14100 upgraded JSch from 0.1.51 to 0.1.54, however, LICENSE.txt was not updated.",ajisakaa
HADOOP-15899,Update AWS Java SDK versions in NOTICE.txt," 
The version of AWS Java SDK documented in NOTICE.txt and the version bundled in binary tarball are different in the most of branches.
|| ||pom.xml||NOTICE.txt||
|trunk|1.11.375|1.11.134|
|branch-3.2|1.11.375|1.11.134|
|branch-3.1|1.11.271|1.11.134|
|branch-3.0|1.11.271|1.11.134|
|branch-2|1.11.199|1.10.6|
|branch-2.9|1.11.199|1.10.6|
|branch-2.8|1.10.6|1.10.6|

 ",ajisakaa
HADOOP-15897,Port range binding fails due to socket bind race condition,"Java's {{ServerSocket#bind}} does both a bind and listen. At a system level, multiple processes may bind to the same port but only one may listen. Java sockets are left in an unrecoverable state when a process loses the race to listen first.

Servers that compete over a listening port range (ex. App Master) will fail the entire range after a collision.  The IPC layer should make a better effort to recover from failed binds.",daryn
HADOOP-15896,Refine Kerberos based AuthenticationHandler to check proxyuser ACL,"JWTRedirectAuthenticationHandler is based on KerberosAuthenticationHandler, and authentication method in KerberosAuthenticationHandler basically do this:

 {code}
String clientPrincipal = gssContext.getSrcName().toString();
        KerberosName kerberosName = new KerberosName(clientPrincipal);
        String userName = kerberosName.getShortName();
        token = new AuthenticationToken(userName, clientPrincipal, getType());
        response.setStatus(HttpServletResponse.SC_OK);
        LOG.trace(""SPNEGO completed for client principal [{}]"",
            clientPrincipal);
{code}

It obtains the short name of the client principal and respond OK.  This is fine for verifying end user.  However, in proxy user case (knox), this authentication is insufficient because knox principal name is: knox/host1.example.com@EXAMPLE.COM . KerberosAuthenticationHandler will gladly confirm that knox is knox.  Even if the knox/host1.example.com@EXAMPLE.COM is used from botnet.rogueresearchlab.tld host.  KerberosAuthenticationHandler may not need to change, if it does not have plan to support proxy, and ignores instance name of kerberos principal.  For JWTRedirectAuthenticationHandler which is designed for proxy use case.  It should check remote host matches the clientPrincipal instance name, without this check, it makes Kerberos vulnerable.",lmccay
HADOOP-15895,[JDK9+] Add missing javax.annotation-api dependency to hadoop-yarn-csi,"Javadoc build fails in hadoop-yarn-csi due to missing {{javax.annotation}}.
{noformat}
$ mvn javadoc:javadoc --projects hadoop-yarn-project/hadoop-yarn/hadoop-yarn-csi
...
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-javadoc-plugin:3.0.1:jar (module-javadocs) on project hadoop-yarn-csi: MavenReportException: Error while generating Javadoc:
[ERROR] Exit code: 1 - /hadoop/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-csi/target/generated-sources/protobuf/grpc-java/csi/v0/IdentityGrpc.java:20: error: cannot find symbol
[ERROR] @javax.annotation.Generated(
[ERROR]                  ^
[ERROR]   symbol:   class Generated
[ERROR]   location: package javax.annotation
[ERROR] /hadoop/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-csi/target/generated-sources/protobuf/grpc-java/csi/v0/ControllerGrpc.java:20: error: cannot find symbol
[ERROR] @javax.annotation.Generated(
[ERROR]                  ^
[ERROR]   symbol:   class Generated
[ERROR]   location: package javax.annotation
[ERROR] /hadoop/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-csi/target/generated-sources/protobuf/grpc-java/csi/v0/NodeGrpc.java:20: error: cannot find symbol
[ERROR] @javax.annotation.Generated(
[ERROR]                  ^
[ERROR]   symbol:   class Generated
[ERROR]   location: package javax.annotation
[ERROR]
[ERROR] Command line was: /usr/java/jdk-9.0.4/bin/javadoc @options @packages
{noformat}",tasanuma0829
HADOOP-15894,getFileChecksum() needs to adopt S3Guard,"Encountered a 404 failure in {{ITestS3AMiscOperations.testNonEmptyFileChecksumsUnencrypted}}; newly created file wasn't seen. Even with S3guard enabled, that method isn't doing anything to query the store for it existing.",jack-lee
HADOOP-15891,Provide Regex Based Mount Point In Inode Tree,"This jira is created to support regex based mount point in Inode Tree. We noticed that mount point only support fixed target path. However, we might have user cases when target needs to refer some fields from source. e.g. We might want a mapping of /cluster1/user1 => /cluster1-dc1/user-nn-user1, we want to refer `cluster` and `user` field in source to construct target. It's impossible to archive this with current link type. Though we could set one-to-one mapping, the mount table would become bloated if we have thousands of users. Besides, a regex mapping would empower us more flexibility. So we are going to build a regex based mount point which target could refer groups from src regex mapping. ",wzzdreamer
HADOOP-15890,Some S3A committer tests don't match ITest* pattern; don't run in maven,"some of the s3A committer tests don't have the right prefix for the maven IT test runs to pick up

{code}
ITMagicCommitMRJob.java
ITStagingCommitMRJobBad
ITDirectoryCommitMRJob
ITStagingCommitMRJob
{code}

They all work when run by name or in the IDE (which is where I developed them), but they don't run in maven builds.

Fix: rename. There are some new tests in branch-3.2 from HADOOP-15107 which aren't in 3.1; need patches for both.",stevel@apache.org
HADOOP-15889,Add hadoop.token configuration parameter to load tokens,"Currently, Hadoop allows passing files containing tokens.
WebHDFS provides base64 delegation tokens that can be used directly.
This JIRA adds the option to pass base64 tokens directly without using files.",elgoiri
HADOOP-15887,Add an option to avoid writing data locally in Distcp,"When copying large amount of data from one cluster to another via Distcp, and the Distcp jobs run in the target cluster, the datanode local usage would be imbalanced. Because the default placement policy chooses the local node to store the first replication.

In https://issues.apache.org/jira/browse/HDFS-3702 we add a flag in DFSClient to avoid replicating to the local datanode.  We can make use of this flag in Distcp.",tao jie
HADOOP-15886,Fix findbugs warnings in RegistryDNS.java,"{noformat}
  FindBugs :
       module:hadoop-common-project/hadoop-registry
       Exceptional return value of java.util.concurrent.ExecutorService.submit(Callable) ignored in org.apache.hadoop.registry.server.dns.RegistryDNS.addNIOTCP(InetAddress, int) At RegistryDNS.java:ignored in org.apache.hadoop.registry.server.dns.RegistryDNS.addNIOTCP(InetAddress, int) At RegistryDNS.java:[line 900]
       Exceptional return value of java.util.concurrent.ExecutorService.submit(Callable) ignored in org.apache.hadoop.registry.server.dns.RegistryDNS.addNIOUDP(InetAddress, int) At RegistryDNS.java:ignored in org.apache.hadoop.registry.server.dns.RegistryDNS.addNIOUDP(InetAddress, int) At RegistryDNS.java:[line 926]
       Exceptional return value of java.util.concurrent.ExecutorService.submit(Callable) ignored in org.apache.hadoop.registry.server.dns.RegistryDNS.serveNIOTCP(ServerSocketChannel, InetAddress, int) At RegistryDNS.java:ignored in org.apache.hadoop.registry.server.dns.RegistryDNS.serveNIOTCP(ServerSocketChannel, InetAddress, int) At RegistryDNS.java:[line 850]
{noformat}",ajisakaa
HADOOP-15885,Add base64 (urlString) support to DTUtil,"HADOOP-12563 added a utility to manage Delegation Token files. Currently, it supports Java and Protobuf formats. However, When interacting with WebHDFS, we use base64. In addition, when printing a token, we also print the base64 value. We should be able to import base64 tokens in the utility.",elgoiri
HADOOP-15883,Fix WebHdfsFileSystemContract test,"HADOOP-15864 fix bug about Job/Task execute failure when server (NameNode, KMS, Timeline) domain name can not resolve. meanwhile it change semantic of http status code about webhdfsfilesystem, this ticket will trace to fix TestWebHdfsFileSystemContract#testResponseCode.",hexiaoqiao
HADOOP-15882,Upgrade maven-shade-plugin from 2.4.3 to 3.2.0,"While working on HADOOP-15815, we have faced a shaded-client error. Please see [~bharatviswa]'s comment [here|https://issues.apache.org/jira/browse/HADOOP-15815?focusedCommentId=16662718&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-16662718].

MSHADE-242 and MSHADE-258 are needed to fix it. Let's upgrade maven-shade-plugin to 3.1.0 or later.
 ",tasanuma0829
HADOOP-15879,Upgrade eclipse jetty version to 9.3.25.v20180904,,bharatviswa
HADOOP-15878,website should have a list of CVEs w/impacted versions and guidance,"Our website should have a page with publicly disclosed CVEs listed. They should include the community's understanding of impacted and fixed versions.

For a simple example, see what kafka does:

https://kafka.apache.org/cve-list",busbey
HADOOP-15877,Upgrade ZooKeeper version to 3.5.4-beta and Curator version to 4.0.1,A long-term option to fix YARN-8937.,ajisakaa
HADOOP-15876,Use keySet().removeAll() to remove multiple keys from Map in AzureBlobFileSystemStore,"Looking at hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java , {{removeDefaultAcl}} in particular:
{code}
    for (Map.Entry<String, String> defaultAclEntry : defaultAclEntries.entrySet()) {
      aclEntries.remove(defaultAclEntry.getKey());
    }
{code}
The above operation can be written this way:
{code}
    aclEntries.keySet().removeAll(defaultAclEntries.keySet());
{code}",danielzhou
HADOOP-15873,Add JavaBeans Activation Framework API to LICENSE.txt,"When I compile HBase master trunk against Hadoop trunk
{code}
$ mvn package -Dhadoop.profile=3.0 -Dhadoop-three.version=3.3.0-SNAPSHOT -DskipTests=true
{code}
, it fails to compile:
{quote}
[INFO] --- maven-enforcer-plugin:3.0.0-M1:enforce (check-aggregate-license) @ hbase-shaded-client ---
[WARNING] Rule 0: org.apache.maven.plugins.enforcer.EvaluateBeanshell failed with message:
License errors detected, for more detail find ERROR in
                    /Users/weichiu/sandbox/hbase/hbase-shaded/hbase-shaded-client/target/maven-shared-archive-resources/META-INF/LICENSE
{quote}
Checking the error in the file,  I saw

{quote}
--
This product includes JavaBeans Activation Framework API jar licensed under the CDDL/GPLv2+CE.

CDDL or GPL version 2 plus the Classpath Exception
ERROR: Please check ^^^^^^^^^^^^ this License for acceptability here:

https://www.apache.org/legal/resolved

If it is okay, then update the list named 'non_aggregate_fine' in the LICENSE.vm file.
If it isn't okay, then revert the change that added the dependency.

More info on the dependency:

<groupId>javax.activation</groupId>
<artifactId>javax.activation-api</artifactId>
<version>1.2.0</version>
{quote}

It looks like the dependency was added in HADOOP-15775. [~ajisakaa] [~tasanuma0829] could you check? We'll need to understand the license of this library

https://github.com/javaee/activation/blob/master/LICENSE.txt",ajisakaa
HADOOP-15872,ABFS: Update to target 2018-11-09 REST version for ADLS Gen 2,"This update to the latest REST version (2018-11-09) will make the following changes to the ABFS driver:

1) The ABFS implementation of getFileStatus currently requires read permission.  According to HDFS permissions guide, it should only require execute on the parent folders (traversal access).  A new REST API has been introduced in REST version ""2018-11-09"" of ADLS Gen 2 to fix this problem.

2) The new ""2018-11-09"" REST version introduces support to i) automatically translate UPNs to OIDs when setting the owner, owning group, or ACL and ii) optionally translate OIDs to UPNs in the responses when getting the owner, owning group, or ACL.  Configuration will be introduced to optionally translate OIDs to UPNs in the responses.  Since translation has a performance impact, the default will be to perform no translation and return the OIDs.",thanatosjug
HADOOP-15870,S3AInputStream.remainingInFile should use nextReadPos,Otherwise `remainingInFile` will not change after `seek`.,jack-lee
HADOOP-15869,BlockDecompressorStream#decompress should not return -1 in case of IOException.,"BlockDecompressorStream#decompress() return -1 in case of BlockMissingException. Application which is using BlockDecompressorStream may think file is empty and proceed further. But actually read operation should fail.
{code:java}
// Get original data size
try {
   originalBlockSize = rawReadInt();
} catch (IOException ioe) {
   return -1;
}{code}",surendrasingh
HADOOP-15868,"AliyunOSS: update document for properties of multiple part download, multiple part upload and directory copy","Update document for properties of multiple part download, multiple part upload and directory copy.",wujinhu
HADOOP-15866,Renamed HADOOP_SECURITY_GROUP_SHELL_COMMAND_TIMEOUT keys break compatibility,"Our internal tool found HADOOP-15523 breaks public API compatibility:

class CommonConfigurationKeysPublic
|| ||Change||Effect||
|1|Field HADOOP_SECURITY_GROUP_SHELL_COMMAND_TIMEOUT_SECS has been renamed to HADOOP_SECURITY_GROUP_SHELL_COMMAND_TIMEOUT_KEY.|Recompilation of a client program may be terminated with the message: cannot find variable HADOOP_SECURITY_GROUP_SHELL_COMMAND_TIMEOUT_SECS in CommonConfigurationKeysPublic.|
|2|Field HADOOP_SECURITY_GROUP_SHELL_COMMAND_TIMEOUT_SECS_DEFAULT has been renamed to HADOOP_SECURITY_GROUP_SHELL_COMMAND_TIMEOUT_DEFAULT.|Recompilation of a client program may be terminated with the message: cannot find variable HADOOP_SECURITY_GROUP_SHELL_COMMAND_TIMEOUT_SECS_DEFAULT in CommonConfigurationKeysPublic.|

HADOOP_SECURITY_GROUP_SHELL_COMMAND_TIMEOUT_SECS_DEFAULT is used to instantiate a variable in ShellBasedGroupsMapping objects, and since almost all applications requires groups mapping, this can cause runtime error if application loads multiple versions of Hadoop library.

IMO this is a blocker for 3.2.0",jojochuang
HADOOP-15865,ConcurrentModificationException in Configuration.overlay() method,"Configuration.overlay() is not thread-safe and can be the cause of ConcurrentModificationException since we use iteration over Properties object. 
{code}
private void overlay(Properties to, Properties from) {
 for (Entry<Object, Object> entry: from.entrySet()) {
 to.put(entry.getKey(), entry.getValue());
 }
 }
{code}
Properties class is thread-safe but iterator is not. We should manually synchronize on the returned set of entries which we use for iteration.

We faced with ResourceManger fails during recovery caused by ConcurrentModificationException:
{noformat}
2018-10-12 08:00:56,968 INFO org.apache.hadoop.service.AbstractService: Service ResourceManager failed in state STARTED; cause: java.util.ConcurrentModificationException
java.util.ConcurrentModificationException
 at java.util.Hashtable$Enumerator.next(Hashtable.java:1383)
 at org.apache.hadoop.conf.Configuration.overlay(Configuration.java:2801)
 at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2696)
 at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2632)
 at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2528)
 at org.apache.hadoop.conf.Configuration.get(Configuration.java:1062)
 at org.apache.hadoop.conf.Configuration.getStringCollection(Configuration.java:1914)
 at org.apache.hadoop.security.alias.CredentialProviderFactory.getProviders(CredentialProviderFactory.java:53)
 at org.apache.hadoop.conf.Configuration.getPasswordFromCredentialProviders(Configuration.java:2043)
 at org.apache.hadoop.conf.Configuration.getPassword(Configuration.java:2023)
 at org.apache.hadoop.yarn.webapp.util.WebAppUtils.getPassword(WebAppUtils.java:452)
 at org.apache.hadoop.yarn.webapp.util.WebAppUtils.loadSslConfiguration(WebAppUtils.java:428)
 at org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:293)
 at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startWepApp(ResourceManager.java:1017)
 at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:1117)
 at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
 at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1251)
2018-10-12 08:00:56,968 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing RMDelegation token with sequence number: 3489914
2018-10-12 08:00:56,968 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDelegationToken and SequenceNumber
2018-10-12 08:00:56,968 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore: Removing RMDelegationToken_3489914
2018-10-12 08:00:56,969 INFO org.apache.hadoop.ipc.Server: Stopping server on 8032
{noformat}",oshevchenko
HADOOP-15864,Job submitter / executor fail when SBN domain name can not resolved,"Job submit failure and Task executes failure if Standby NameNode domain name can not resolved on HDFS HA with DelegationToken feature.

This issue is triggered when create {{ConfiguredFailoverProxyProvider}} instance which invoke {{HAUtil.cloneDelegationTokenForLogicalUri}} in HA mode with Security. Since in HDFS HA mode UGI need include separate token for each NameNode in order to dealing with Active-Standby switch, the double tokens' content is same of course. 
However when #setTokenService in {{HAUtil.cloneDelegationTokenForLogicalUri}} it checks whether the address of NameNode has been resolved or not, if Not, throw #IllegalArgumentException upon, then job submitter/ task executor fail.

HDFS-8068 and HADOOP-12125 try to fix it, but I don't think the two tickets resolve completely.
Another questions many guys consider is why NameNode domain name can not resolve? I think there are many scenarios, for instance node replace when meet fault, and refresh DNS sometimes. Anyway, Standby NameNode failure should not impact Hadoop cluster stability in my opinion.

a. code ref: org.apache.hadoop.security.SecurityUtil line373-386
{code:java}
  public static Text buildTokenService(InetSocketAddress addr) {
    String host = null;
    if (useIpForTokenService) {
      if (addr.isUnresolved()) { // host has no ip address
        throw new IllegalArgumentException(
            new UnknownHostException(addr.getHostName())
        );
      }
      host = addr.getAddress().getHostAddress();
    } else {
      host = StringUtils.toLowerCase(addr.getHostName());
    }
    return new Text(host + "":"" + addr.getPort());
  }
{code}

b.exception log ref:
{code:xml}
at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Couldn't create proxy provider class org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider
at org.apache.hadoop.hdfs.NameNodeProxies.createFailoverProxyProvider(NameNodeProxies.java:515)
at org.apache.hadoop.hdfs.NameNodeProxies.createProxy(NameNodeProxies.java:170)
at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:761)
at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:691)
at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:150)
at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2713)
at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:93)
at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2747)
at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2729)
at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:385)
at org.apache.hadoop.fs.viewfs.ChRootedFileSystem.<init>(ChRootedFileSystem.java:106)
at org.apache.hadoop.fs.viewfs.ViewFileSystem$1.getTargetFileSystem(ViewFileSystem.java:178)
at org.apache.hadoop.fs.viewfs.ViewFileSystem$1.getTargetFileSystem(ViewFileSystem.java:172)
at org.apache.hadoop.fs.viewfs.InodeTree.createLink(InodeTree.java:303)
at org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:377)
at org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:172)
at org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:172)
at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2713)
at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:93)
at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2747)
at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2729)
at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:385)
at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:176)
at org.apache.hadoop.mapred.JobConf.getWorkingDirectory(JobConf.java:665)
... 35 more
Caused by: java.lang.reflect.InvocationTargetException
at sun.reflect.GeneratedConstructorAccessor14.newInstance(Unknown Source)
at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
at org.apache.hadoop.hdfs.NameNodeProxies.createFailoverProxyProvider(NameNodeProxies.java:498)
... 58 more
Caused by: java.lang.IllegalArgumentException: java.net.UnknownHostException: standbynamenode
at org.apache.hadoop.security.SecurityUtil.buildTokenService(SecurityUtil.java:390)
at org.apache.hadoop.security.SecurityUtil.setTokenService(SecurityUtil.java:369)
at org.apache.hadoop.hdfs.HAUtil.cloneDelegationTokenForLogicalUri(HAUtil.java:317)
at org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider.<init>(ConfiguredFailoverProxyProvider.java:132)
at org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider.<init>(ConfiguredFailoverProxyProvider.java:84)
... 62 more
Caused by: java.net.UnknownHostException: standbynamenode
... 67 more
{code}",hexiaoqiao
HADOOP-15861,Move DelegationTokenIssuer to the right path,"The addendum path of HADOOP-14445 updated the package name of DelegationTokenIssuer, but it didn't update the patch.
It is currently under 
hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/org/apache/hadoop/security/token/DelegationTokenIssuer.java

We should move it under
hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/DelegationTokenIssuer.java

File this as a separate jira instead of muddling through HADOOP-14445.",jojochuang
HADOOP-15859,ZStandardDecompressor.c mistakes a class for an instance,"As a follow up to HADOOP-15820, I was doing more testing on ZSTD compression and still encountered segfaults in the JVM in HBase after that fix. 

I took a deeper look and realized there is still another bug, which looks like it's that we are actually [calling setInt()|https://github.com/apache/hadoop/blob/f13e231025333ebf80b30bbdce1296cef554943b/hadoop-common-project/hadoop-common/src/main/native/src/org/apache/hadoop/io/compress/zstd/ZStandardDecompressor.c#L148] on the ""remaining"" variable on the ZStandardDecompressor class itself (instead of an instance of that class) because the Java stub for the native C init() function [is marked static|https://github.com/apache/hadoop/blob/a0a276162147e843a5a4e028abdca5b66f5118da/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/zstd/ZStandardDecompressor.java#L253], leading to memory corruption and a crash during GC later.

Initially I thought we would fix this by changing the Java init() method to be non-static, but it looks like the ""remaining"" setInt() call is actually unnecessary anyway, because in ZStandardDecompressor.java's reset() we [set ""remaining"" to 0 right after calling the JNI init() call|https://github.com/apache/hadoop/blob/a0a276162147e843a5a4e028abdca5b66f5118da/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/zstd/ZStandardDecompressor.java#L216]. So ZStandardDecompressor.java init() doesn't have to be changed to an instance method, we can leave it as static, but remove the JNI init() call's ""remaining"" setInt() call altogether.

Furthermore we should probably clean up the class/instance distinction in the C file because that's what led to this confusion. There are some other methods where the distinction is incorrect or ambiguous, we should fix them to prevent this from happening again.

I talked to [~jlowe] who further pointed out the ZStandardCompressor also has similar problems and needs to be fixed too.",jlowe
HADOOP-15857,Remove ozonefs class name definition from core-default.xml,"Ozone file system is under renaming in HDDS-651 from o3:// to o3fs://. But branch-3.2 still contains a reference with o3://.

The easiest way to fix it just remove the fs.o3.imp definition from core-default.xml from branch-3.2 as since HDDS-654 the file system could be registered with Service Provider Interface (META-INF/services...)",elek
HADOOP-15856,Trunk build fails to compile native on Windows,"After removal of {{javah}} dependency in HADOOP-15767
Trunk build fails with unable to find JNI headers.

HADOOP-15767 fixed javah isssue with JDK10 only for linux builds.",vinayrpet
HADOOP-15855,"Review hadoop credential doc, including object store details","I've got some changes to make to the hadoop credentials API doc; some minor editing and examples of credential paths in object stores with some extra details (i.e how you can't refer to a store from the same store URI)

these examples need to come with unit tests to verify that the examples are correct, obviously",stevel@apache.org
HADOOP-15854,AuthToken Use StringBuilder instead of StringBuffer,Use {{StringBuilder}} instead of {{StringBuffer}} because {{StringBuilder}} is not synchronized.,belugabehr
HADOOP-15853,"TestConfigurationDeprecation leaves behind a temp file, resulting in a license issue","HADOOP-15708 made some changes to {{TestConfigurationDeprecation}}.  One of them was adding
{code:java}
  final static String CONFIG4 = new File(""./test-config4"" +
      ""-TestConfigurationDeprecation.xml"").getAbsolutePath();
{code}
which we never clean up in the {{tearDown}} method:
{code:java}
  @After
  public void tearDown() throws Exception {
    new File(CONFIG).delete();
    new File(CONFIG2).delete();
    new File(CONFIG3).delete();
  }
{code}

This results in that file being left behind, and causing a license warning in test runs.",ayushtkn
HADOOP-15852,Refactor QuotaUsage,"My new mission is to remove instances of {{StringBuffer}} in favor of {{StringBuilder}}.

* Simplify Code
* Use Eclipse to generate hashcode/equals
* User StringBuilder instead of StringBuffer",belugabehr
HADOOP-15851,Disable wildfly logs to the console,"On loading OpenSSL library successfully, Wildfly logging messages like below
{code:java}
Oct 15, 2018 6:47:24 AM org.wildfly.openssl.SSL init
INFO: WFOPENSSL0002 OpenSSL Version OpenSSL 1.1.0g 2 Nov 2017
{code}
These messages may fiddle with existing scripts which parses logs with a predefined schema.",vishwajeet.dusane
HADOOP-15850,CopyCommitter#concatFileChunks should check that the blocks per chunk is not 0,"I was investigating test failure of TestIncrementalBackupWithBulkLoad from hbase against hadoop 3.1.1

hbase MapReduceBackupCopyJob$BackupDistCp would create listing file:
{code}
        LOG.debug(""creating input listing "" + listing + "" , totalRecords="" + totalRecords);
        cfg.set(DistCpConstants.CONF_LABEL_LISTING_FILE_PATH, listing);
        cfg.setLong(DistCpConstants.CONF_LABEL_TOTAL_NUMBER_OF_RECORDS, totalRecords);
{code}
For the test case, two bulk loaded hfiles are in the listing:
{code}
2018-10-13 14:09:24,123 DEBUG [Time-limited test] mapreduce.MapReduceBackupCopyJob$BackupDistCp(195): BackupDistCp : hdfs://localhost:42796/user/hbase/test-data/160aeab5-6bca-9f87-465e-2517a0c43119/data/default/test-1539439707496/96b5a3613d52f4df1ba87a1cef20684c/f/394e6d39a9b94b148b9089c4fb967aad_SeqId_205_
2018-10-13 14:09:24,125 DEBUG [Time-limited test] mapreduce.MapReduceBackupCopyJob$BackupDistCp(195): BackupDistCp : hdfs://localhost:42796/user/hbase/test-data/160aeab5-6bca-9f87-465e-2517a0c43119/data/default/test-1539439707496/96b5a3613d52f4df1ba87a1cef20684c/f/a7599081e835440eb7bf0dd3ef4fd7a5_SeqId_205_
2018-10-13 14:09:24,125 DEBUG [Time-limited test] mapreduce.MapReduceBackupCopyJob$BackupDistCp(197): BackupDistCp execute for 2 files of 10242
{code}
Later on, CopyCommitter#concatFileChunks would throw the following exception:
{code}
2018-10-13 14:09:25,351 WARN  [Thread-936] mapred.LocalJobRunner$Job(590): job_local1795473782_0004
java.io.IOException: Inconsistent sequence file: current chunk file org.apache.hadoop.tools.CopyListingFileStatus@bb8826ee{hdfs://localhost:42796/user/hbase/test-data/       160aeab5-6bca-9f87-465e-2517a0c43119/data/default/test-1539439707496/96b5a3613d52f4df1ba87a1cef20684c/f/a7599081e835440eb7bf0dd3ef4fd7a5_SeqId_205_ length = 5100 aclEntries  = null, xAttrs = null} doesnt match prior entry org.apache.hadoop.tools.CopyListingFileStatus@243d544d{hdfs://localhost:42796/user/hbase/test-data/160aeab5-6bca-9f87-465e-   2517a0c43119/data/default/test-1539439707496/96b5a3613d52f4df1ba87a1cef20684c/f/394e6d39a9b94b148b9089c4fb967aad_SeqId_205_ length = 5142 aclEntries = null, xAttrs = null}
  at org.apache.hadoop.tools.mapred.CopyCommitter.concatFileChunks(CopyCommitter.java:276)
  at org.apache.hadoop.tools.mapred.CopyCommitter.commitJob(CopyCommitter.java:100)
  at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:567)
{code}
The above warning shouldn't happen - the two bulk loaded hfiles are independent.

From the contents of the two CopyListingFileStatus instances, we can see that their isSplit() return false. Otherwise the following from toString should be logged:
{code}
    if (isSplit()) {
      sb.append("", chunkOffset = "").append(this.getChunkOffset());
      sb.append("", chunkLength = "").append(this.getChunkLength());
    }
{code}
From hbase side, we can specify one bulk loaded hfile per job but that defeats the purpose of using DistCp.

",yuzhihong@gmail.com
HADOOP-15849,Upgrade netty version to 3.10.6 ,We're currently at 3.10.5. It'd be good to upgrade to the latest 3.10.6 release.,xiaochen
HADOOP-15848,ITestS3AContractMultipartUploader#testMultipartUploadEmptyPart test error,"To reproduce failure: {{mvn verify -Dscale -Dit.test=org.apache.hadoop.fs.contract.s3a.ITestS3AContractMultipartUploader -Dtest=skip}} against {{eu-west-1}}.

Test output:
{noformat}
[INFO] Running org.apache.hadoop.fs.contract.s3a.ITestS3AContractMultipartUploader
[ERROR] Tests run: 11, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 59.301 s <<< FAILURE! - in org.apache.hadoop.fs.contract.s3a.ITestS3AContractMultipartUploader
[ERROR] testMultipartUploadEmptyPart(org.apache.hadoop.fs.contract.s3a.ITestS3AContractMultipartUploader)  Time elapsed: 0.75 s  <<< ERROR!
java.lang.IllegalArgumentException: partNumber must be between 1 and 10000 inclusive, but is 0
{noformat}


",ehiggs
HADOOP-15847,S3Guard testConcurrentTableCreations to set r & w capacity == 1,"I just found a {{testConcurrentTableCreations}} DDB table lurking in a region, presumably from an interrupted test. Luckily test/resources/core-site.xml forces the r/w capacity to be 10, but it could still run up bills.

Recommend
* explicitly set capacity = 1 for the test
* and add comments in the testing docs about keeping cost down.

I think we may also want to make this a scale-only test, so it's run less often",jack-lee
HADOOP-15846,"ABFS: fix mask related bugs in setAcl, modifyAclEntries and removeAclEntries.","# setAcl, modifyAclEntries and removeAclEntries should not re-calculate default mask if not touched.
 # Duplicate Acl Entries are not allowed.",thanatosjug
HADOOP-15845,s3guard init and destroy command will create/destroy tables if ddb.table & region are set,"If you have s3guard set up with a table name and a region, then s3guard init will automatically create the table, without you specifying a bucket or URI.

I had expected the command just to print out its arguments, but it actually did the init with the default bucket values

Even worse, `hadoop s3guard destroy` will destroy the table. 

This is too dangerous to allow. The command must require either the name of a bucket or an an explicit ddb table URI",gabor.bota
HADOOP-15843,s3guard bucket-info command to not print a stack trace on bucket-not-found,"when you go {{hadoop s3guard bucket-info s3a://bucket-which-doesnt-exist}} you get a full stack trace on the failure. This is overkill: all the caller needs to know is the bucket isn't there.

Proposed: catch FNFE and treat as special, have return code of ""44"", ""not found"".",adam.antal
HADOOP-15842,add fs.azure.account.oauth2.client.secret to hadoop.security.sensitive-config-keys,"in HADOOP-15839 I left out ""fs.azure.account.oauth2.client.secret"". Fix by adding it",stevel@apache.org
HADOOP-15841,ABFS: change createRemoteFileSystemDuringInitialization default to true,"I haven't seen a way to create a working container (at least for the dfs endpoint) except for setting fs.azure.createRemoteFileSystemDuringInitialization=true. I personally don't see that much of a downside to having it default to true, and it's a mild inconvenience to remember to set it to true for some action to create a container. I vaguely recall [~tmarquardt] considering changing this default too.

I propose we do it?",mackrorysd
HADOOP-15839,Review + update cloud store sensitive keys in hadoop.security.sensitive-config-keys,"Make sure that {{hadoop.security.sensitive-config-keys}} is up to date with all cloud store options, including

h3. s3a:
* s3a per-bucket secrets
* s3a session tokens

h3: abfs
* {{fs.azure.account.oauth2.client.secret}}

h3. adls
fs.adl.oauth2.credential
fs.adl.oauth2.refresh.token
",stevel@apache.org
HADOOP-15837,DynamoDB table Update can fail S3A FS init,"When DDB autoscales a table, it goes into an UPDATING state. The waitForTableActive operation in the AWS SDK doesn't seem to wait long enough for this to recover. We need to catch & retry",stevel@apache.org
HADOOP-15836,Review of AccessControlList,"* Improve unit tests (expected / actual were backwards)
* Unit test expected elements to be in order but the class's return Collections were unordered
* Formatting cleanup
* Removed superfluous white space
* Remove use of LinkedList
* Removed superfluous code
* Use {{unmodifiable}} Collections where JavaDoc states that caller must not manipulate the data structure",belugabehr
HADOOP-15835,Reuse Object Mapper in KMSJSONWriter,"In lieu of HADOOP-15550 in branch-3.0, branch-2.9, branch-2.8. This patch will provide some benefit of MapperObject reuse though not as complete as the JsonSerialization util lazy loading fix.",jeagles
HADOOP-15833,Intermittent failures of some S3A tests with S3Guard in parallel test runs,intermittent failure of a pair of {{ITestS3GuardToolDynamoDB}} tests in parallel runs. They don't seem to fail in sequential mode.,stevel@apache.org
HADOOP-15832,Upgrade BouncyCastle to 1.60,"As part of my work on YARN-6586, I noticed that we're using a very old version of BouncyCastle:
{code:xml}
<dependency>
   <groupId>org.bouncycastle</groupId>
   <artifactId>bcprov-jdk16</artifactId>
   <version>1.46</version>
   <scope>test</scope>
</dependency>
{code}
The *-jdk16 artifacts have been discontinued and are not recommended (see [http://bouncy-castle.1462172.n4.nabble.com/Bouncycaslte-bcprov-jdk15-vs-bcprov-jdk16-td4656252.html]). 
 In particular, the newest release, 1.46, is from {color:#FF0000}2011{color}! 
 [https://mvnrepository.com/artifact/org.bouncycastle/bcprov-jdk16]

The currently maintained and recommended artifacts are *-jdk15on:
 [https://www.bouncycastle.org/latest_releases.html]
 They're currently on version 1.60, released only a few months ago.

We should update BouncyCastle to the *-jdk15on artifacts and the 1.60 release. It's currently a test-only artifact, so there should be no backwards-compatibility issues with updating this. It's also needed for YARN-6586, where we'll actually be shipping it.",rkanter
HADOOP-15831,Include modificationTime in the toString method of CopyListingFileStatus,"I was looking at a DistCp error observed in hbase backup test:
{code}
2018-10-08 18:12:03,067 WARN  [Thread-933] mapred.LocalJobRunner$Job(590): job_local1175594345_0004
java.io.IOException: Inconsistent sequence file: current chunk file org.apache.hadoop.tools.CopyListingFileStatus@7ac56817{hdfs://localhost:41712/user/hbase/test-data/       c0f6352c-cf39-bbd1-7d10-57a9c01e7ce9/data/default/test-1539022262249/be1bf5445faddb63e45726410a07917a/f/f565f49046b04eecbf8d129eac7a7b88_SeqId_205_ length = 5100 aclEntries  = null, xAttrs = null} doesnt match prior entry org.apache.hadoop.tools.CopyListingFileStatus@7aa4deb2{hdfs://localhost:41712/user/hbase/test-data/c0f6352c-cf39-bbd1-7d10-   57a9c01e7ce9/data/default/test-1539022262249/be1bf5445faddb63e45726410a07917a/f/41b6cb64bae64cbcac47d1fd9aae59f4_SeqId_205_ length = 5142 aclEntries = null, xAttrs = null}
  at org.apache.hadoop.tools.mapred.CopyCommitter.concatFileChunks(CopyCommitter.java:276)
  at org.apache.hadoop.tools.mapred.CopyCommitter.commitJob(CopyCommitter.java:100)
  at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:567)
2018-10-08 18:12:03,150 INFO  [Time-limited test] mapreduce.MapReduceBackupCopyJob$BackupDistCp(226): Progress: 100.0% subTask: 1.0 mapProgress: 1.0
{code}
I noticed that modificationTime was not included in the toString of CopyListingFileStatus.

I propose including modificationTime so that it is easier to tell when the respective files last change.",yuzhihong@gmail.com
HADOOP-15830,Server.java Prefer ArrayList,"*  Prefer ArrayDeque over LinkedList (faster, less memory overhead)
* Address this code:

{code}
    //
    // Remove calls that have been pending in the responseQueue 
    // for a long time.
    //
    private void doPurge(RpcCall call, long now) {
      LinkedList<RpcCall> responseQueue = call.connection.responseQueue;
      synchronized (responseQueue) {
        Iterator<RpcCall> iter = responseQueue.listIterator(0);
        while (iter.hasNext()) {
          call = iter.next();
          if (now > call.timestamp + PURGE_INTERVAL) {
            closeConnection(call.connection);
            break;
          }
        }
      }
    }
{code}

It says ""Remove calls"" (plural) but only one call will be removed because of the 'break' statement.",belugabehr
HADOOP-15829,Review of NetgroupCache,* Simplify code and performance by using Guava Multimap,belugabehr
HADOOP-15828,Review of MachineList class,"Clean up and simplify class {{MachineList}}.  Primarily, remove LinkedList implementation and use empty collections instead of 'null' values, add logging.",belugabehr
HADOOP-15827,NPE in DynamoDBMetadataStore.lambda$listChildren for root + auth S3Guard,"NPE in a test run of {{-Dparallel-tests -DtestsThreadCount=6  -Ds3guard -Ddynamodb -Dauthoritative}}

{code}
[ERROR] testLSRootDir(org.apache.hadoop.fs.s3a.ITestS3AFileSystemContract)  Time elapsed: 42.822 s  <<< ERROR!
java.lang.NullPointerException
{code}",gabor.bota
HADOOP-15826,@Retries annotation of putObject() call & uses wrong,"The retry annotations of the S3AFilesystem putObject call and its writeOperationsHelper use aren't in sync with what the code does.

Fix",stevel@apache.org
HADOOP-15825,ABFS: Enable some tests for namespace not enabled account using OAuth,"When testing namespace not enabled account using Oauth, some tests were skipped. So need to update the tests.",danielzhou
HADOOP-15823,ABFS: Stop requiring client ID and tenant ID for MSI,"ABFS requires the user to configure the tenant ID and client ID. From my understanding of MSI, that shouldn't be necessary and is an added requirement compared to MSI in ADLS. Can that be dropped?",danielzhou
HADOOP-15822,zstd compressor can fail with a small output buffer,"TestZStandardCompressorDecompressor fails a couple of tests on my machine with the latest zstd library (1.3.5).  Compression can fail to successfully finalize the stream when a small output buffer is used resulting in a failed to init error, and decompression with a direct buffer can fail with an invalid src size error.",jlowe
HADOOP-15821,Move Hadoop YARN Registry to Hadoop Registry,"Currently, Hadoop YARN Registry is in YARN. However, this can be used by other parts of the project (e.g., HDFS). In addition, it does not have any real dependency to YARN.

We should move it into commons and make it Hadoop Registry.",elgoiri
HADOOP-15820,ZStandardDecompressor native code sets an integer field as a long,"Java_org_apache_hadoop_io_compress_zstd_ZStandardDecompressor_init in ZStandardDecompressor.c sets the {{remaining}} field as a long when it actually is an integer.

Kudos to Ben Lau from our HBase team for discovering this issue.",jlowe
HADOOP-15819,S3A integration test failures: FileSystem is closed! - without parallel test run,"Running the integration tests for hadoop-aws {{mvn -Dscale verify}} against Amazon AWS S3 (eu-west-1, us-west-1, with no s3guard) we see a lot of these failures:

{noformat}
[ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 4.408 s <<< FAILURE! - in org.apache.hadoop.fs.s3a.commit.staging.integration.ITDirectoryCommitMRJob
[ERROR] testMRJob(org.apache.hadoop.fs.s3a.commit.staging.integration.ITDirectoryCommitMRJob)  Time elapsed: 0.027 s  <<< ERROR!
java.io.IOException: s3a://cloudera-dev-gabor-ireland: FileSystem is closed!

[ERROR] Tests run: 2, Failures: 0, Errors: 2, Skipped: 0, Time elapsed: 4.345 s <<< FAILURE! - in org.apache.hadoop.fs.s3a.commit.staging.integration.ITStagingCommitMRJob
[ERROR] testStagingDirectory(org.apache.hadoop.fs.s3a.commit.staging.integration.ITStagingCommitMRJob)  Time elapsed: 0.021 s  <<< ERROR!
java.io.IOException: s3a://cloudera-dev-gabor-ireland: FileSystem is closed!

[ERROR] testMRJob(org.apache.hadoop.fs.s3a.commit.staging.integration.ITStagingCommitMRJob)  Time elapsed: 0.022 s  <<< ERROR!
java.io.IOException: s3a://cloudera-dev-gabor-ireland: FileSystem is closed!

[ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 4.489 s <<< FAILURE! - in org.apache.hadoop.fs.s3a.commit.staging.integration.ITStagingCommitMRJobBadDest
[ERROR] testMRJob(org.apache.hadoop.fs.s3a.commit.staging.integration.ITStagingCommitMRJobBadDest)  Time elapsed: 0.023 s  <<< ERROR!
java.io.IOException: s3a://cloudera-dev-gabor-ireland: FileSystem is closed!

[ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 4.695 s <<< FAILURE! - in org.apache.hadoop.fs.s3a.commit.magic.ITMagicCommitMRJob
[ERROR] testMRJob(org.apache.hadoop.fs.s3a.commit.magic.ITMagicCommitMRJob)  Time elapsed: 0.039 s  <<< ERROR!
java.io.IOException: s3a://cloudera-dev-gabor-ireland: FileSystem is closed!

[ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0.015 s <<< FAILURE! - in org.apache.hadoop.fs.s3a.commit.ITestS3ACommitterFactory
[ERROR] testEverything(org.apache.hadoop.fs.s3a.commit.ITestS3ACommitterFactory)  Time elapsed: 0.014 s  <<< ERROR!
java.io.IOException: s3a://cloudera-dev-gabor-ireland: FileSystem is closed!
{noformat}

The big issue is that the tests are running in a serial manner - no test is running on top of the other - so we should not see that the tests are failing like this. The issue could be in how we handle org.apache.hadoop.fs.FileSystem#CACHE - the tests should use the same S3AFileSystem so if A test uses a FileSystem and closes it in teardown then B test will get the same FileSystem object from the cache and try to use it, but it is closed.

We see this a lot in our downstream testing too. It's not possible to tell that the failed regression test result is an implementation issue in the runtime code or a test implementation problem. 
I've checked when and what closes the S3AFileSystem with a sightly modified version of S3AFileSystem which logs the closers of the fs if an error should occur. I'll attach this modified java file for reference. See the next example of the result when it's running:

{noformat}
2018-10-04 00:52:25,596 [Thread-4201] ERROR s3a.S3ACloseEnforcedFileSystem (S3ACloseEnforcedFileSystem.java:checkIfClosed(74)) - Use after close(): 
java.lang.RuntimeException: Using closed FS!.
	at org.apache.hadoop.fs.s3a.S3ACloseEnforcedFileSystem.checkIfClosed(S3ACloseEnforcedFileSystem.java:73)
	at org.apache.hadoop.fs.s3a.S3ACloseEnforcedFileSystem.mkdirs(S3ACloseEnforcedFileSystem.java:474)
	at org.apache.hadoop.fs.contract.AbstractFSContractTestBase.mkdirs(AbstractFSContractTestBase.java:338)
	at org.apache.hadoop.fs.contract.AbstractFSContractTestBase.setup(AbstractFSContractTestBase.java:193)
	at org.apache.hadoop.fs.s3a.ITestS3AClosedFS.setup(ITestS3AClosedFS.java:40)
	at sun.reflect.GeneratedMethodAccessor22.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
2018-10-04 00:52:25,596 [Thread-4201] WARN  s3a.S3ACloseEnforcedFileSystem (S3ACloseEnforcedFileSystem.java:logClosers(61)) - FS was closed 6 times by:
2018-10-04 00:52:25,596 [Thread-4201] WARN  s3a.S3ACloseEnforcedFileSystem (S3ACloseEnforcedFileSystem.java:logClosers(64)) - ----------------- close() #1 ---------------
2018-10-04 00:52:25,596 [Thread-4201] WARN  s3a.S3ACloseEnforcedFileSystem (S3ACloseEnforcedFileSystem.java:logClosers(65)) - java.lang.RuntimeException: close() called here
	at org.apache.hadoop.fs.s3a.S3ACloseEnforcedFileSystem.close(S3ACloseEnforcedFileSystem.java:83)
	at org.apache.hadoop.io.IOUtils.cleanupWithLogger(IOUtils.java:280)
	at org.apache.hadoop.io.IOUtils.closeStream(IOUtils.java:298)
	at org.apache.hadoop.fs.s3a.AbstractS3ATestBase.teardown(AbstractS3ATestBase.java:55)
	at org.apache.hadoop.fs.s3a.commit.AbstractCommitITest.teardown(AbstractCommitITest.java:206)
	at org.apache.hadoop.fs.s3a.auth.ITestAssumedRoleCommitOperations.teardown(ITestAssumedRoleCommitOperations.java:90)
	at sun.reflect.GeneratedMethodAccessor23.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)

2018-10-04 00:52:25,596 [Thread-4201] WARN  s3a.S3ACloseEnforcedFileSystem (S3ACloseEnforcedFileSystem.java:logClosers(64)) - ----------------- close() #2 ---------------
2018-10-04 00:52:25,596 [Thread-4201] WARN  s3a.S3ACloseEnforcedFileSystem (S3ACloseEnforcedFileSystem.java:logClosers(65)) - java.lang.RuntimeException: close() called here
	at org.apache.hadoop.fs.s3a.S3ACloseEnforcedFileSystem.close(S3ACloseEnforcedFileSystem.java:83)
	at org.apache.hadoop.io.IOUtils.cleanupWithLogger(IOUtils.java:280)
	at org.apache.hadoop.io.IOUtils.closeStream(IOUtils.java:298)
	at org.apache.hadoop.fs.s3a.AbstractS3ATestBase.teardown(AbstractS3ATestBase.java:55)
	at sun.reflect.GeneratedMethodAccessor23.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)

2018-10-04 00:52:25,597 [Thread-4201] WARN  s3a.S3ACloseEnforcedFileSystem (S3ACloseEnforcedFileSystem.java:logClosers(64)) - ----------------- close() #3 ---------------
2018-10-04 00:52:25,598 [Thread-4201] WARN  s3a.S3ACloseEnforcedFileSystem (S3ACloseEnforcedFileSystem.java:logClosers(65)) - java.lang.RuntimeException: close() called here
	at org.apache.hadoop.fs.s3a.S3ACloseEnforcedFileSystem.close(S3ACloseEnforcedFileSystem.java:83)
	at org.apache.hadoop.io.IOUtils.cleanupWithLogger(IOUtils.java:280)
	at org.apache.hadoop.io.IOUtils.closeStream(IOUtils.java:298)
	at org.apache.hadoop.fs.s3a.AbstractS3ATestBase.teardown(AbstractS3ATestBase.java:55)
	at sun.reflect.GeneratedMethodAccessor23.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
(...)
{noformat}

From the logs it seems that the closers are the teardown methods of the abstract test classes, so the same FileSystem object gets reused between tests.


To run a test with the modified fs the following should be in the config:
{noformat}
<property>
  <name>fs.s3a.impl</name>
  <value>org.apache.hadoop.fs.s3a.S3ACloseEnforcedFileSystem</value>
  <description>The implementation class of the S3A Filesystem</description>
</property>
{noformat}

I file this jira to solve this issue, and start a conversation if needed.",gabor.bota
HADOOP-15818,Fix deprecated maven-surefire-plugin configuration in hadoop-kms module,"{noformat}
[INFO] --- maven-surefire-plugin:2.21.0:test (default-test) @ hadoop-kms ---
[WARNING] The parameter forkMode is deprecated since version 2.14. Use forkCount and reuseForks instead.
{noformat}",vbmudalige
HADOOP-15817,Reuse Object Mapper in KMSJSONReader,"Paying an expensive cost to construct object mapper deserializer cache.
 
{code:title=KMS Server Stack Trace}
""qtp1926764753-117"" #117 prio=5 os_prio=0 tid=0x000000000321c000 nid=0x1f0bd runnable [0x00002b4caabf7000]
   java.lang.Thread.State: RUNNABLE
        at java.lang.reflect.Executable.sharedGetParameterAnnotations(Executable.java:553)
        at java.lang.reflect.Constructor.getParameterAnnotations(Constructor.java:523)
        at org.codehaus.jackson.map.introspect.AnnotatedClass._constructConstructor(AnnotatedClass.java:784)
        at org.codehaus.jackson.map.introspect.AnnotatedClass.resolveCreators(AnnotatedClass.java:327)
        at org.codehaus.jackson.map.introspect.BasicClassIntrospector.classWithCreators(BasicClassIntrospector.java:187)
        at org.codehaus.jackson.map.introspect.BasicClassIntrospector.collectProperties(BasicClassIntrospector.java:157)
        at org.codehaus.jackson.map.introspect.BasicClassIntrospector.forCreation(BasicClassIntrospector.java:119)
        at org.codehaus.jackson.map.introspect.BasicClassIntrospector.forCreation(BasicClassIntrospector.java:16)
        at org.codehaus.jackson.map.DeserializationConfig.introspectForCreation(DeserializationConfig.java:877)
        at org.codehaus.jackson.map.deser.BasicDeserializerFactory.createMapDeserializer(BasicDeserializerFactory.java:430)
        at org.codehaus.jackson.map.deser.StdDeserializerProvider._createDeserializer(StdDeserializerProvider.java:380)
        at org.codehaus.jackson.map.deser.StdDeserializerProvider._createAndCache2(StdDeserializerProvider.java:310)
        at org.codehaus.jackson.map.deser.StdDeserializerProvider._createAndCacheValueDeserializer(StdDeserializerProvider.java:290)
        - locked <0x0000000752fbbc28> (a java.util.HashMap)
        at org.codehaus.jackson.map.deser.StdDeserializerProvider.findValueDeserializer(StdDeserializerProvider.java:159)
        at org.codehaus.jackson.map.deser.StdDeserializerProvider.findTypedValueDeserializer(StdDeserializerProvider.java:180)
        at org.codehaus.jackson.map.ObjectMapper._findRootDeserializer(ObjectMapper.java:2829)
        at org.codehaus.jackson.map.ObjectMapper._readMapAndClose(ObjectMapper.java:2728)
        at org.codehaus.jackson.map.ObjectMapper.readValue(ObjectMapper.java:1909)
        at org.apache.hadoop.crypto.key.kms.server.KMSJSONReader.readFrom(KMSJSONReader.java:52)
        at org.apache.hadoop.crypto.key.kms.server.KMSJSONReader.readFrom(KMSJSONReader.java:35)
        at com.sun.jersey.spi.container.ContainerRequest.getEntity(ContainerRequest.java:474)
        at com.sun.jersey.server.impl.model.method.dispatch.EntityParamDispatchProvider$EntityInjectable.getValue(EntityParamDispatchProvider.java:123)
        at com.sun.jersey.server.impl.inject.InjectableValuesProvider.getInjectableValues(InjectableValuesProvider.java:46)
        at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$EntityParamInInvoker.getParams(AbstractResourceMethodDispatchProvider.java:153)
        at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:203)
        at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)
        at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:288)
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
        at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
        at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)
        at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1469)
        at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1400)
        at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1349)
        at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1339)
        at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:416)
        at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:537)
        at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:699)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
        at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:848)
        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1772)
        at org.eclipse.jetty.websocket.server.WebSocketUpgradeFilter.doFilter(WebSocketUpgradeFilter.java:205)
        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)
        at org.apache.hadoop.crypto.key.kms.server.KMSMDCFilter.doFilter(KMSMDCFilter.java:84)
{code}",jeagles
HADOOP-15816,Upgrade Apache Zookeeper version due to security concerns,"* [CVE-2018-8012|https://web.nvd.nist.gov/view/vuln/detail?vulnId=CVE-2018-8012]
 * [CVE-2017-5637|https://web.nvd.nist.gov/view/vuln/detail?vulnId=CVE-2017-5637]

We should upgrade the dependency to version 3.4.11 or the latest, if possible.",ajisakaa
HADOOP-15815,Upgrade Eclipse Jetty version to 9.3.24,"* [CVE-2017-7657|https://web.nvd.nist.gov/view/vuln/detail?vulnId=CVE-2017-7657]
 * [CVE-2017-7658|https://web.nvd.nist.gov/view/vuln/detail?vulnId=CVE-2017-7658]
 * [CVE-2017-7656|https://web.nvd.nist.gov/view/vuln/detail?vulnId=CVE-2017-7656]
 * [CVE-2018-12536|https://web.nvd.nist.gov/view/vuln/detail?vulnId=CVE-2018-12536]

We should upgrade the dependency to version 9.3.24 or the latest, if possible.",borisvu
HADOOP-15814,Maven 3.3.3 unable to parse pom file,"Found via HDFS-13952.

Reproducible on Maven 3.3.3, but not a problem for Maven 3.5.0 and above.
I had to make the following change in order to compile. Looks like a problem after the ABFS merge.

{code}
diff --git a/hadoop-project/pom.xml b/hadoop-project/pom.xml
index cd38376..4c2c267 100644
--- a/hadoop-project/pom.xml
+++ b/hadoop-project/pom.xml
@@ -1656,7 +1656,9 @@
           <artifactId>maven-javadoc-plugin</artifactId>
           <version>${maven-javadoc-plugin.version}</version>
           <configuration>
-            <additionalOptions>-Xmaxwarns 10000</additionalOptions>
+            <additionalOptions>
+              <additionalOption>-Xmaxwarns 10000</additionalOption>
+            </additionalOptions>
           </configuration>
         </plugin>
         <plugin>
{code}
Otherwise it gives me this error:
{quote}
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-javadoc-plugin:3.0.1:jar (module-javadocs) on project hadoop-project: Unable to parse configuration of mojo org.apache.maven.plugins:maven-javadoc-plugin:3.0.1:jar for parameter additionalOptions: Cannot assign configuration entry 'additionalOptions' with value '-Xmaxwarns 10000' of type java.lang.String to property of type java.lang.String[] -> [Help 1]
{quote}",jojochuang
HADOOP-15813,Enable more reliable SSL connection reuse,"The java keep-alive cache relies on instance equivalence of the SSL socket factory.  In many java versions, SSLContext#getSocketFactory always returns a new instance which completely breaks the cache.  Clients flooding a service with lingering per-request connections that can lead to port exhaustion.  The hadoop SSLFactory should cache the socket factory associated with the context.",daryn
HADOOP-15812,ABFS: Improve AbfsRestOperationException format to ensure full msg can be displayed on console,"AbfsRestOperationException msg contains multiple ""\n"" for displaying nicely, 
however, hadoop cmd class  split the error msg by ""\n"" and display the first string in console, which lead to the incomplete error msg shown in console.
Hence we need to improve this format.
",danielzhou
HADOOP-15809,ABFS: better exception handling when making getAccessToken call,"Currently in *getTokenSingleCall()*: if it get a HTTP failure response, it tries to consume inputStream in httpUrlConnection, which will *always* lead to an *IOException* and this exception never get checked in *AzureADAuthenticator*. 
 As a result the httpStatus code is never checked in the retry policy of AzureADAuthenticator. Tthat IOException will be caught by AbfsRestOperation, which will keep on retrying.",danielzhou
HADOOP-15808,Harden Token service loader use,"The Hadoop token service loading (identifiers, renewers...) works provided there's no problems loading any registered implementation. If there's a classloading or classcasting problem, the exception raised will stop all token support working; possibly the application not starting.

This matters for S3A/HADOOP-14556 as things may not load if aws-sdk isn't on the classpath. It probably lurks in the wasb/abfs support too, but things have worked there because the installations with DT support there have always had correctly set up classpaths.

Fix: do what we did for the FS service loader. Catch failures to instantiate a service provider impl and skip it",stevel@apache.org
HADOOP-15805,Hadoop logo not showed correctly in old site,"Hadoop logo not showed correctly in old site.  In old site pages, we use the address {{[http://hadoop.apache.org/images/hadoop-logo.jpg]}} to show the hadoop logo. Actually, this address is outdated. The right address now is [http://hadoop.apache.org/hadoop-logo.jpg].",sandeep nemuri
HADOOP-15804,upgrade to commons-compress 1.18,"[https://github.com/apache/commons-compress/blob/master/RELEASE-NOTES.txt]

Some CVEs have been fixed in recent releases ([https://commons.apache.org/proper/commons-compress/security-reports.html])

[https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-common/3.1.1] depends on commons-compress 1.4.1",ajisakaa
HADOOP-15802,start-build-env.sh creates an invalid /etc/sudoers.d/hadoop-build-${USER_ID} file entry,"In my Ubuntu 18.04 dev VM, I cloned the hadoop repo and ran the start-build-env.sh script.  Once the docker build was completed and the container running, I tried to sudo and it failed.  Upon investigation, I discovered that it was creating an entry in /etc/sudoers.d/hadoop-build-${USER_ID} that contained the characters '\t' rather than a tab.",jonboone
HADOOP-15801,ABFS: Fixing skipUserGroupMetadata in AzureBlobFileSystemStore,"AzureBlobFileSystem check property ""skipUserGroupMetadata""  to skip retrieving the user group information and provide a dummy one during fs initialization, so AzureBlobFileSystemStore should do the same thing or should just use the user&group info fetched from AzureBlobFileSystem.",danielzhou
HADOOP-15799,ITestS3AEmptyDirectory#testDirectoryBecomesEmpty fails when running with dynamo,"I've seen a new failure when running verify for HADOOP-15621. First I thought it was my new patch, but it happens on trunk. This is a major issue, it could be because of implementation issue in dynamo.
{noformat}
[ERROR] testDirectoryBecomesEmpty(org.apache.hadoop.fs.s3a.ITestS3AEmptyDirectory) Time elapsed: 1.864 s <<< FAILURE!
java.lang.AssertionError: dir is empty expected:<TRUE> but was:<FALSE>
at org.junit.Assert.fail(Assert.java:88)
at org.junit.Assert.failNotEquals(Assert.java:743)
at org.junit.Assert.assertEquals(Assert.java:118)
at org.apache.hadoop.fs.s3a.ITestS3AEmptyDirectory.assertEmptyDirectory(ITestS3AEmptyDirectory.java:56)
at org.apache.hadoop.fs.s3a.ITestS3AEmptyDirectory.testDirectoryBecomesEmpty(ITestS3AEmptyDirectory.java:48)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74){noformat}",gabor.bota
HADOOP-15798,LocalMetadataStore put() does not retain isDeleted in parent listing,"To reproduce: mvn verify -Dauth -Ds3guard -Dlocal -Dtest=none -Dit.test=ITestS3GuardListConsistency

ITestS3GuardListConsistency#testConsistentListAfterDelete test fails constantly when running with LocalMetadataStore.

{noformat}
java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertFalse(Assert.java:64)
	at org.junit.Assert.assertFalse(Assert.java:74)
	at org.apache.hadoop.fs.s3a.ITestS3GuardListConsistency.testConsistentListAfterDelete(ITestS3GuardListConsistency.java:205)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
{noformat}",gabor.bota
HADOOP-15796,ClassCastException: S3AFileSystem cannot be cast to MockS3AFileSystem when fs caching is disabled,"Sometimes we get {{java.io.IOException: s3a://TEST-BUCKET: FileSystem is closed!}} when running tests, because of filesystem caching: an fs instance is closed from a test teardown while another test is still using it. (That should be fixed in another issue.)

To avoid test errors introduced by closed fs I've disabled fs caching while testing. This caused the following issue in lots of test:

{noformat}
[ERROR] testPartitionsResolution(org.apache.hadoop.fs.s3a.commit.staging.TestStagingPartitionedFileListing)  Time elapsed: 1.994 s  <<< ERROR!
java.lang.ClassCastException: org.apache.hadoop.fs.s3a.S3AFileSystem cannot be cast to org.apache.hadoop.fs.s3a.MockS3AFileSystem
	at org.apache.hadoop.fs.s3a.commit.staging.StagingTestBase.lookupWrapperFS(StagingTestBase.java:145)
	at org.apache.hadoop.fs.s3a.commit.staging.StagingTestBase$JobCommitterTest.setupJob(StagingTestBase.java:294)
{noformat}

We should fix the casting during these tests.

To switch of fs caching during a test (mvn verify) run, add the following to the config:

{code:java}
  <property>
    <name>fs.s3a.impl.disable.cache</name>
    <value>true</value>
  </property>
{code}
",gabor.bota
HADOOP-15795,Make HTTPS the default protocol for ABFS," HTTPS should be used as default in ABFS, but also  we provide a configuration key for user to disable it in non-secure mode.",danielzhou
HADOOP-15793,ABFS: Skip unsupported test cases when non namespace enabled in ITestAzureBlobFileSystemAuthorization,,kowon2008
HADOOP-15792,typo in AzureBlobFileSystem.getIsNamespaceEnabeld,"There's a typo in the visible-for-test method {{AzureBlobFileSystem.getIsNamespaceEnabeld}}

Trivial to fix, just postponing until after the 3.2.x branch",abmodi
HADOOP-15791,Remove Ozone related sources from the 3.2 branch,"As it is discussed at HDDS-341 and written in the original proposal of Ozone merge, we can remove all the ozone/hdds projects from the 3.2 release branch.

{quote}
 * On trunk (as opposed to release branches) HDSL will be a separate module in Hadoop's source tree. This will enable the HDSL to work on their trunk and the Hadoop trunk without making releases for every change.
  * Hadoop's trunk will only build HDSL if a non-default profile is enabled.
  * When Hadoop creates a release branch, the RM will delete the HDSL module from the branch.
{quote}
",elek
HADOOP-15788,Improve Distcp for long-haul/cloud deployments,"There are a number of outstanding distcp options related to: extensibility, failure reporting/cleanup, long-haul options, cloud performance.

Hadoop 3.1 added some speedups; follow this up with others. ",stevel@apache.org
HADOOP-15785,[JDK10] Javadoc build fails on JDK 10 in hadoop-common,"{noformat}
$ mvn javadoc:javadoc --projects hadoop-common-project/hadoop-common
...
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 02:22 min
[INFO] Finished at: 2018-09-25T02:23:06Z
[INFO] Final Memory: 119M/467M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-javadoc-plugin:3.0.1:jar (module-javadocs) on project hadoop-common: MavenReportException: Error while generating Javadoc: 
[ERROR] Exit code: 1 - javadoc: warning - You have not specified the version of HTML to use.
[ERROR] The default is currently HTML 4.01, but this will change to HTML5
[ERROR] in a future release. To suppress this warning, please specify the
[ERROR] version of HTML used in your documentation comments and to be
[ERROR] generated by this doclet, using the -html4 or -html5 options.
[ERROR] /hadoop/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Client.java:1578: error: malformed HTML
[ERROR] * to servers are uniquely identified by <remoteAddress, protocol, ticket>
...
{noformat}",dineshchitlangia
HADOOP-15783,[JDK10] TestSFTPFileSystem.testGetModifyTime fails,"{noformat}
[ERROR] Tests run: 11, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 5.039 s <<< FAILURE! - in org.apache.hadoop.fs.sftp.TestSFTPFileSystem
[ERROR] testGetModifyTime(org.apache.hadoop.fs.sftp.TestSFTPFileSystem)  Time elapsed: 0.095 s  <<< FAILURE!
java.lang.AssertionError: expected:<1537836464942> but was:<1537836464000>
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.failNotEquals(Assert.java:743)
	at org.junit.Assert.assertEquals(Assert.java:118)
	at org.junit.Assert.assertEquals(Assert.java:555)
	at org.junit.Assert.assertEquals(Assert.java:542)
	at org.apache.hadoop.fs.sftp.TestSFTPFileSystem.testGetModifyTime(TestSFTPFileSystem.java:329)
{noformat}",ajisakaa
HADOOP-15781,S3A assumed role tests failing due to changed error text in AWS exceptions,"

This is caused by HADOOP-15642 but I'd missed it because I'd been playing with assumed roles locally (restricting their rights) and mistook the failures for ""steve's misconfigured the test role"", not ""the SDK",stevel@apache.org
HADOOP-15780,S3Guard: document how to deal with non-S3Guard processes writing data to S3Guarded buckets,"Our general policy for S3Guard is this: All modifiers of a bucket that is configured for use with S3Guard, must use S3Guard. Otherwise, the MetadataStore will not be properly updated as the S3 bucket changes and problems will arise.

There are limited circumstances in which may be safe to have an external (non-s3guard) process writing data.  There are also scenarios where it definitely breaks things.

I think we should start by documenting the cases that this works / does not work for. After we've enumerated that, we can suggest enhancements as needed to make this sort of configuration easier to use.

To get the ball rolling, some things that do not work:
- Deleting a path *p* with S3Guard, then writing a new file at path *p* without S3guard (will still have delete marker in S3Guard, making the file appear to be deleted but still visible in S3 due to false ""eventual consistency"") (as [~stevel@apache.org] and I have discussed)
- When fs.s3a.metadatastore.authoritative is true, adding files to directories without S3Guard, then listing with S3Guard may exclude externally-written files from listings.

(Note, there are also S3A interop issues with other non-S3A clients even without S3Guard, due to the unique way S3A interprets empty directory markers).",gabor.bota
HADOOP-15779,S3guard: add inconsistency detection metrics,"S3Guard uses a trailing log of metadata changes made to an S3 bucket to add consistency to the eventually-consistent AWS S3 service. We should add some metrics that are incremented when we detect inconsistency (eventual consistency) in S3.

I'm thinking at least two counters: (1) getFileStatus() (HEAD) inconsistency detected, and (2) listing inconsistency detected. We may want to further separate into categories (present / not present etc.)

This is related to Auth. Mode and TTL work that is ongoing, so let me outline how I think this should all evolve:

This should happen after HADOOP-15621 (TTL for dynamo MetadataStore), since that will change *when* we query both S3 and the MetadataStore (e.g. Dynamo) for metadata. There I suggest that:
 # Prune time is different than TTL. Prune time: ""how long until inconsistency is no longer a problem"" . TTL time ""how long a MetadataStore entry is considered authoritative before refresh""
 # Prune expired: delete entries (when hadoop CLI prune command is run). TTL Expired: entries become non-authoritative.
 #  Prune implemented in each MetadataStore, but TTL filtering happens in S3A.

Once we have this, S3A will be consulting both S3 and MetadataStore depending on configuration and/or age of the entry in the MetadataStore. Today HEAD/getFileStatus() is always short-circuit (skips S3 if MetadataStore returns results). I think S3A should consult both when TTL is stale, and invoke a callback on inconsistency that increments the new metrics. For listing, we already are comparing both sources of truth (except when S3A auth mode is on and a directory is marked authoritative in MS), so it would be pretty simple to invoke a callback on inconsistency and bump some metrics.

Comments / suggestions / questions welcomed.",gabor.bota
HADOOP-15778,ABFS: Fix client side throttling for read,"1. The content length for ReadFile in updateMetrics of AbfsClientThrottlingIntercept is incorrect for cases when the request fails.
It is currently equal to the number of bytes that are read whereas it should be equal to the number of bytes requested.



2. sendingRequest of AbfsClientThrottlingIntercept at AbfsRestOperation should be called irrespective of  whether the request has body.",snehavarma
HADOOP-15775,[JDK9] Add missing javax.activation-api dependency,Many unit tests fail due to missing java.activation module. This failure can be fixed by adding javax.activation-api as third-party dependency.,ajisakaa
HADOOP-15774,Leverage Hadoop Service Registry to discover Hadoop services,"Currently, Hadoop relies on configuration files to specify the servers.
This requires maintaining these configuration files and propagating the changes.
Hadoop should have a framework to provide discovery.
For example, in HDFS, we could define the Namenodes in a shared location and the DNs would use the framework to find the Namenodes.",elgoiri
HADOOP-15773,ABFS: Fix issues raised by Yetus,"I aggregated the HADOOP-15407 branch into a single patch and posted it on HADOOP-15770 just to get an aggregate report of all current issues raised by Yetus. There was a javac deprecation warning, a number of checkstyle issues, some whitespace issues, and there are a couple of valid javadoc errors I see locally. Let's fix them before we merge.

I see a number of wildcard imports, all for the contents of large classes of configuration constants, and I think those should stay the way they are. There are a number of existing checkstyle issues in WASB, too that are irrelevant for the merge, and there are some field visibility issues in tests that are required that way for the tests to work as designed.",mackrorysd
HADOOP-15772,Remove the 'Path ... should be specified as a URI' warnings on startup,"The following warnings are logged on service startup and are noise. It is perfectly valid to list local paths without using the URI syntax.
{code:java}
2018-09-16 23:16:11,393 WARN  common.Util (Util.java:stringAsURI(99)) - Path /hadoop/hdfs/namenode should be specified as a URI in configuration files. Please update hdfs configuration.{code}
 ",ayushtkn
HADOOP-15771,Update the link to HowToContribute page,"HowToContribute page is moved to https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute.

However, the new Apache Hadoop web site still links to the old page.
{noformat:title=config.toml}
[[menu.main]]
  name = ""How to Contribute""
  url = ""https://wiki.apache.org/hadoop/HowToContribute""
  parent = ""development""
{noformat}",tasanuma0829
HADOOP-15770,Merge HADOOP-15407 to trunk,I started a [VOTE] thread last night. Will be posting a patch of the current branch state to get pre-commit checks.,mackrorysd
HADOOP-15769,ABFS: distcp tests are always skipped,"the distcp contract tests for ABFS exist, but they aren't quite wired up completely, so are always being skipped.",stevel@apache.org
HADOOP-15768,Do not use Time#now to calculate the rpc process time duration,"For the rpc process time calculation, we are using a not-recommended way: Using {{Time#now}} to calculate for this.
{code}
        // Invoke the protocol method
        long startTime = Time.now();
        int qTime = (int) (startTime-receivedTime);
....
          int processingTime = (int) (Time.now() - startTime);
...
server.updateMetrics(detailedMetricsName, qTime, processingTime, false);
{code} 

Actually  we should use {{Time#monotonicNow()}} instead. This JIRA will fix these across RpcEngine impl classes.",jianliang.wu
HADOOP-15767,[JDK10] Building native package on JDK10 fails due to missing javah,"This is the error log.
{noformat}
[ERROR] Failed to execute goal org.codehaus.mojo:native-maven-plugin:1.0-alpha-8:javah (default) on project hadoop-common: Error running javah command: Error executing command line. Exit code:127 -> [Help 1]
{noformat}

See also: https://github.com/mojohaus/maven-native/issues/17",tasanuma0829
HADOOP-15764,[JDK10] Migrate from sun.net.dns.ResolverConfiguration to the replacement,"In JDK10, sun.net.dns.ResolverConfiguration is encapsulated and not accessible from unnamed modules. This issue is to remove the usage of ResolverConfiguration.",ajisakaa
HADOOP-15762,"AbstractContractAppendTest to add more tests, implementations to comply","There are some extra append tests from HADOOP-14507; put them in {{AbstractContractAppendTest}} and make sure that the filesystems are all compliant

specifically
# can you append over a directory
# can you append to a file  which you have just deleted 

how directory appends are rejected seems to vary",stevel@apache.org
HADOOP-15759,AliyunOSS: update oss-sdk version to 3.0.0,"OSS Java SDK 3.0.0 adds 7 CredentialsProviders. And Assumes Role Credentials function bases on [STSAssumeRoleSessionCredentialsProvider|https://github.com/aliyun/aliyun-oss-java-sdk/blob/master/src/main/java/com/aliyun/oss/common/auth/STSAssumeRoleSessionCredentialsProvider.java] that sdk offers, so we update its version.",wujinhu
HADOOP-15758,"Filesystem.get(URI, Configuration, user) API not working with proxy users","A user reported that the Filesystem.get API is not working as expected when they use the 'FileSystem.get(URI, Configuration, user)' method signature - but 'FileSystem.get(URI, Configuration)' works fine. The user is trying to use this method signature to mimic proxy user functionality e.g. provide ticket cache based kerberos credentials (using KRB5CCNAME env variable) for the proxy user and then in the java program pass name of the user to be impersonated. The alternative, to use [proxy users functionality|https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/Superusers.html] in Hadoop works as expected.

 

Since FileSystem.get(URI, Configuration, user) is a public API and it does not restrict its usage in this fashion, we should ideally make it work or add docs to discourage its usage to implement proxy users.

 ",hgadre
HADOOP-15757,ABFS: remove dependency on common-codec Base64,"Currently ABFS relies on common-codec Base64, because different versions of common-codec are widely used and some are missing the methods needed by ABFS, it cause lots of ""no such method"" exception in customer's env, hence we decide to add util for Base64 to avoid such issues in future.",danielzhou
HADOOP-15756,[JDK10] Migrate from sun.net.util.IPAddressUtil to the replacement,"In JDK10, sun.net.util.IPAddressUtil is encapsulated and not accessible from unnamed modules. This issue is to remove the usage of IPAddressUtil.",ajisakaa
HADOOP-15755,StringUtils#createStartupShutdownMessage throws NPE when args is null,"StringUtils#createStartupShutdownMessage uses 
{code:java}
Arrays.asList(args)
{code}
which throws NPE when args is null.",dineshchitlangia
HADOOP-15754,s3guard: testDynamoTableTagging should clear existing config,"I recently committed HADOOP-14734 which adds support for tagging Dynamo DB tables for S3Guard when they are created.

 

Later, when testing another patch, I hit a test failure because I still had a tag option set in my test configuration (auth-keys.xml) that was adding my own table tag.
{noformat}
[ERROR] testDynamoTableTagging(org.apache.hadoop.fs.s3a.s3guard.ITestS3GuardToolDynamoDB)  Time elapsed: 13.384 s  <<< FAILURE!

java.lang.AssertionError: expected:<2> but was:<3>

        at org.junit.Assert.fail(Assert.java:88)

        at org.junit.Assert.failNotEquals(Assert.java:743)

        at org.junit.Assert.assertEquals(Assert.java:118)

        at org.junit.Assert.assertEquals(Assert.java:555)

        at org.junit.Assert.assertEquals(Assert.java:542)

        at org.apache.hadoop.fs.s3a.s3guard.ITestS3GuardToolDynamoDB.testDynamoTableTagging(ITestS3GuardToolDynamoDB.java:129)

        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

        at java.lang.reflect.Method.invoke(Method.java:498)

        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)

        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)

        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)

        at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)

        at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)

        at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)

        at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)

        at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74){noformat}
I think the solution is just to clear any tag.* options set in the configuration at the beginning of the test.",gabor.bota
HADOOP-15753,"ABFS: support path ""abfs://mycluster/file/path""","WASB support path format: ""wasb://mycluster/file/path"", but ABFS doesn't, which caused some issues for customer. I will add support for this path format.",danielzhou
HADOOP-15750,remove obsolete S3A test ITestS3ACredentialsInURL,"now that we've cut user:pass, need to remove the test ITestS3ACredentialsInURL, which fails.

I'd missed this because I've got s3guard enabled in test runs, and it automatically skips.  oops.",stevel@apache.org
HADOOP-15748,S3 listing inconsistency can raise NPE in globber,"FileSystem Globber does a listStatus(path) and then, if only one element is returned, {{getFileStatus(path).isDirectory()}} to see if it is a dir. The way getFileStatus() is wrapped, IOEs are downgraded to null

On S3, if the path has had entries deleted, the listing may include files which are no longer there, so the getFileStatus(path),isDirectory triggers an NPE

While its wrong to glob against S3 when its being inconsistent, we should at least fail gracefully here.

Proposed
# log all IOEs raised in Globber.getFileStatus @ debug
# catch FNFEs and downgrade to warn
# continue

The alternative would be fail fast on FNFE, but that's more traumatic
",stevel@apache.org
HADOOP-15747,warning about user:pass in URI to explicitly call out Hadoop 3.2 as removal,"Now that HADOOP-14833 has removed user:secret from URIs, change the warning printed when people to that to explicitly declare Hadoop 3.3 as when it will happen. Do the same in the docs.",stevel@apache.org
HADOOP-15745,Add ABFS configuration to ConfigRedactor,Sensitive information like credentials should be detected by ConfigRedactor so they never appear in logs or other channels. ABFS credentials are not all currently detected correctly so we should amend the default list of config patterns.,mackrorysd
HADOOP-15744,AbstractContractAppendTest fails against HDFS on HADOOP-15407 branch,"{code:java}
mvn test -Dtest=TestHDFSContractAppend#testAppendDirectory,TestRouterWebHDFSContractAppend#testAppendDirectory{code}
In case of TestHDFSContractAppend the test excepts FileAlreadyExistsException but HDFS sends the exception wrapped into a RemoteException.
In case of TestRouterWebHDFSContractAppend the append does not even throw exception.

[~stevel@apache.org], [~tmarquardt], any thoughts?",stevel@apache.org
HADOOP-15742,Log if ipc backoff is enabled in CallQueueManager,"Currently we don't log the info of ipc backoff. It will look good to print this as well so that makes users know if we enable this.
{code:java}
  public CallQueueManager(Class<? extends BlockingQueue<E>> backingClass,
                          Class<? extends RpcScheduler> schedulerClass,
      boolean clientBackOffEnabled, int maxQueueSize, String namespace,
      Configuration conf) {
    int priorityLevels = parseNumLevels(namespace, conf);
    this.scheduler = createScheduler(schedulerClass, priorityLevels,
        namespace, conf);
    BlockingQueue<E> bq = createCallQueueInstance(backingClass,
        priorityLevels, maxQueueSize, namespace, conf);
    this.clientBackOffEnabled = clientBackOffEnabled;
    this.putRef = new AtomicReference<BlockingQueue<E>>(bq);
    this.takeRef = new AtomicReference<BlockingQueue<E>>(bq);
    LOG.info(""Using callQueue: "" + backingClass + "" queueCapacity: "" +
        maxQueueSize + "" scheduler: "" + schedulerClass);
  }
{code}",jianliang.wu
HADOOP-15741,[JDK10] Upgrade Maven Javadoc Plugin from 3.0.0-M1 to 3.0.1,MJAVADOC-517 is fixed in 3.0.1. Let's upgrade the plugin to 3.0.1 or upper to support Java 10.,tasanuma0829
HADOOP-15740,ABFS: Check variable names during initialization of AbfsClientThrottlingIntercept ,"In the initializeSingleton function of the AbfsClientThrottlingIntercept the local variable name is same as global variable isAutoThrottlingEnabled because of which the global isAutoThrottlingEnabled is not being set to true.

 ",snehavarma
HADOOP-15739,ABFS: remove unused maven dependencies and add used undeclared dependencies,,danielzhou
HADOOP-15736,Trash : Negative Value For Deletion Interval Leads To Abnormal Behaviour.,"If deletion interval ( {{fs.trash.interval}} ) is somehow configured negative.
 The trash gets enabled since the value is not zero.
 It even changes the emptier interval to that negative value since the value needs to be less than or equal to deletion interval.
 But the emptier due to negative value doesn't get up throws IllegalArgumentException().
 Ultimately the trash gets piled up since no emptier work is being done.",ayushtkn
HADOOP-15735,backport HADOOP-11687 for hadoop-aws 2.7.x,"HADOOP-12970 Intermitent SignatureDoesNotMatch Exception is fixed by HADOOP-11687. 

And from hadoop-aws >= 2.8, HADOOP-11687 is applied. but Spark 2.3.1 is still using hadoop-aws 2.7.7. 

I was stumpled upon Intermitent SignatureDoesNotMatch Exception with s3 with spark 2.3.1. So I guess backporting might benefit some spark users. In my situation, SignatureDoesNotMatch was reliably reproducible (4 out of 8 attemp). And after using patched version symptom was not seen (0 out of 10)

",ruseel
HADOOP-15733,Correct the log when Invalid emptier Interval configured,"{{Scenario:-}}

 Configure Deletion interval as 10 and  Emptier interval as -1
 Restart the namenode 
 Run the rm command  hdfs dfs -rm /file

{{Actualoutput:-}}

In log it's printing the -1 value for emptierinverval but as per the code it's should be deletion interval value

{noformat}
2018-09-07 14:59:41,721 INFO org.apache.hadoop.fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 10 minutes, Emptier interval = -1 minutes.
{noformat}

{{Exceptedoutput:-}}

Emptier interval value should be 10",ayushtkn
HADOOP-15731,TestDistributedShell fails on Windows,"[ERROR] testDSShellWithMultipleArgs(org.apache.hadoop.yarn.applications.distributedshell.TestDistributedShell) Time elapsed: 25.68 s <<< FAILURE!
java.lang.AssertionError
 at org.junit.Assert.fail(Assert.java:86)
 at org.junit.Assert.assertTrue(Assert.java:41)
 at org.junit.Assert.assertTrue(Assert.java:52)
 at org.apache.hadoop.yarn.applications.distributedshell.TestDistributedShell.verifyContainerLog(TestDistributedShell.java:1296)

[ERROR] testDSShellWithoutDomainV2CustomizedFlow(org.apache.hadoop.yarn.applications.distributedshell.TestDistributedShell) Time elapsed: 90.021 s <<< ERROR!
java.lang.Exception: test timed out after 90000 milliseconds
 at java.lang.Thread.sleep(Native Method)
 at org.apache.hadoop.yarn.applications.distributedshell.TestDistributedShell.testDSShell(TestDistributedShell.java:398)
 at org.apache.hadoop.yarn.applications.distributedshell.TestDistributedShell.testDSShellWithoutDomainV2CustomizedFlow(TestDistributedShell.java:313)

 

 ",botong
HADOOP-15729,[s3a] stop treat fs.s3a.max.threads as the long-term minimum,"A while ago the s3a connector started experiencing deadlocks because the AWS SDK requires an unbounded threadpool. It places monitoring tasks on the work queue before the tasks they wait on, so it's possible (has even happened with larger-than-default threadpools) for the executor to become permanently saturated and deadlock.

So we started giving an unbounded threadpool executor to the SDK, and using a bounded, blocking threadpool service for everything else S3A needs (although currently that's only in the S3ABlockOutputStream). fs.s3a.max.threads then only limits this threadpool, however we also specified fs.s3a.max.threads as the number of core threads in the unbounded threadpool, which in hindsight is pretty terrible.

Currently those core threads do not timeout, so this is actually setting a sort of minimum. Once that many tasks have been submitted, the threadpool will be locked at that number until it bursts beyond that, but it will only spin down that far. If fs.s3a.max.threads is set reasonably high and someone uses a bunch of S3 buckets, they could easily have thousands of idle threads constantly.

We should either not use fs.s3a.max.threads for the corepool size and introduce a new configuration, or we should simply allow core threads to timeout. I'm reading the OpenJDK source now to see what subtle differences there are between core threads and other threads if core threads can timeout.",mackrorysd
HADOOP-15728,ABFS: Add backward compatibility to handle Unsupported Operation for storage account with no namespace feature,"For non-namespaceenbaled account, UnsupportedOperationException is thrown when setOwner() or setPermission() is called, which cause problems for others.
They should behave the same as before, so should make them  no-op if the storage account is not namespace enabled.",danielzhou
HADOOP-15726,Create utility to limit frequency of log statements,"There is a common pattern of logging a behavior that is normally extraneous. Under some circumstances, such a behavior becomes common, flooding the logs and making it difficult to see what else is going on in the system. Under such situations it is beneficial to limit how frequently the extraneous behavior is logged, while capturing some summary information about the suppressed log statements.

This is currently implemented in {{FSNamesystemLock}} (in HDFS-10713). We have additional use cases for this in HDFS-13791, so this is a good time to create a common utility for different sites to share this logic.",xkrogen
HADOOP-15723,ABFS: Ranger Support,Add support for Ranger,kowon2008
HADOOP-15721,Disable checkstyle javadoc warnings for test classes,"The current checkstyle rules with throw a warning if the javadoc is missing for a test class which is of minimal value. We should consider disabling this check and allowing contributors to comment test classes as they see fit.

Here is an example:
{code:java}
./hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/MockLinuxContainerRuntime.java:27:public class MockLinuxContainerRuntime implements LinuxContainerRuntime {: Missing a Javadoc comment. [JavadocType]{code}",dineshchitlangia
HADOOP-15719,Fail-fast when using OAuth over http,"If you configure OAuth and then use abfs:// instead of abfss:// it will fail, but it takes a very long time, and still isn't very clear why. Good place to put a good human-readable exception message and fail fast.",danielzhou
HADOOP-15717,TGT renewal thread does not log IOException,"I came across a case where tgt.getEndTime() was returned null and it resulted in an NPE, this observation was popped out of a test suite execution on a cluster. The reason for logging the {{IOException}} is that it helps to troubleshoot what caused the exception, as it can come from two different calls from the try-catch.

I can see that [~gabor.bota] handled this with HADOOP-15593, but apart from logging the fact that the ticket's {{endDate}} was null, we have not logged the exception at all.
With the current code, the exception is swallowed and the thread terminates in case the ticket's {{endDate}} is null. 
As this can happen with OpenJDK for example, it is required to print the exception (stack trace, message) to the log.
The code should be updated here: https://github.com/apache/hadoop/blob/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/UserGroupInformation.java#L918
",snemeth
HADOOP-15715,ITestAzureBlobFileSystemE2E timing out with non-scale timeout of 10 min,"{{ITestAzureBlobFileSystemRandomRead}} is timing out on remote parallel test runs, because its working with multi MB files, which need bandwidth which the other tests deny.

Either it needs to be tagged as a scale test, or, as minimum, timeout changed to match that value",danielzhou
HADOOP-15714,tune abfs/wasb parallel the sequential test execution,"after HADOOP-15664, sequential tests don't run unless you ask for them with the sequential tests profile, as there are now separate profiles for parallel-tests, parallel-tests-abfs and parallel-tests-wasb, so the sequential-tests profile can't just run if !parallel-tests

Proposed: revert back to only having one parallel-tests property, but allow it to take the values ""abfs"" and ""wasb"" to explicitly enable on those suites.

This would add the set of options

* -Dparallel-tests : all tests in parallel   (== true at runtime)
* -Dparallel-tests=abfs   abfs tests only
* -Dparallel-tests=wasb wasb tests only
* """" : nothing defined, sequential

the activations of the profiles would then look at the values in their activation; sequential-tests would be triggered if it was unset.

",danielzhou
HADOOP-15710,ABFS checkException to map 403 to AccessDeniedException,"when you can't auth to ABFS, you get a 403 exception back. This should be translated into an access denied exception for better clarity/handling",stevel@apache.org
HADOOP-15709,Move S3Guard LocalMetadataStore constants to org.apache.hadoop.fs.s3a.Constants,"Move the following constants from {{org.apache.hadoop.fs.s3a.s3guard.LocalMetadataStore}} to  {{org.apache.hadoop.fs.s3a.Constants}} (where they should be):
* DEFAULT_MAX_RECORDS
* DEFAULT_CACHE_ENTRY_TTL_MSEC
* CONF_MAX_RECORDS
* CONF_CACHE_ENTRY_TTL",gabor.bota
HADOOP-15708,Reading values from Configuration before adding deprecations make it impossible to read value with deprecated key,"Hadoop Common contains a widely used Configuration class.
 This class can handle deprecations of properties, e.g. if property 'A' gets deprecated with an alternative property key 'B', users can access property values with keys 'A' and 'B'.
 Unfortunately, this does not work in one case.
 When a config file is specified (for instance, XML) and a property is read with the config.get() method, the config is loaded from the file at this time. 
 If the deprecation mapping is not yet specified by the time any config value is retrieved and the XML config refers to a deprecated key, then the deprecation mapping specified, the config value cannot be retrieved neither with the deprecated nor with the new key.
 The attached patch contains a testcase that reproduces this wrong behavior.

Here are the steps outlined what the testcase does:
 1. Creates an XML config file with a deprecated property
 2. Adds the config to the Configuration object
 3. Retrieves the config with its deprecated key (it does not really matter which property the user gets, could be any)
 4. Specifies the deprecation rules including the one defined in the config
 5. Prints and asserts the property retrieved from the config with both the deprecated and the new property keys.

For reference, here is the log of one execution that actually shows what the issue is:
{noformat}
Loaded items: 1
Looked up property value with name hadoop.zk.address: null
Looked up property value with name yarn.resourcemanager.zk-address: dummyZkAddress
Contents of config file: [<?xml version=""1.0""?>, <configuration>, <property><name>yarn.resourcemanager.zk-address</name><value>dummyZkAddress</value></property>, </configuration>]
Looked up property value with name hadoop.zk.address: null
2018-08-31 10:10:06,484 INFO  Configuration.deprecation (Configuration.java:logDeprecation(1397)) - yarn.resourcemanager.zk-address is deprecated. Instead, use hadoop.zk.address
Looked up property value with name hadoop.zk.address: null
Looked up property value with name hadoop.zk.address: null

java.lang.AssertionError: 
Expected :dummyZkAddress
Actual   :null
{noformat}
*As it's visible from the output and the code, the issue is really that if the config is retrieved either with the deprecated or the new value, Configuration both wants to serve the value with the new key.*
 *If the mapping is not specified before any retrieval happened, the value is only stored under the deprecated key but not the new key.*",zsiegl
HADOOP-15707,Add IsActiveServlet to be used for Load Balancers,"Hadoop has a few services with HA setups and it is common to set them behind Load Balancers.
We should add a way for the Load Balancers to understand what should be the UI to show.
For example, the standby RM just redirects the requests to the active RM.
However, if both RMs are behind a Load Balancer the IP might not be reachable.

Most Load balancers have probes to check if a server reports HTTP code 200:
* [Azure|https://docs.microsoft.com/en-us/azure/load-balancer/load-balancer-custom-probe-overview]
* [AWS|https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-healthchecks.html]

Components in Hadoop (e.g., NN, RM, Router,...) should have a unified way to report if they are active.",lukmajercak
HADOOP-15706,Typo in compatibility doc: SHOUD -> SHOULD,"{quote}% grep SHOUD ./hadoop-common-project/hadoop-common/src/site/markdown/Compatibility.md
Apache Hadoop revisions SHOUD retain binary compatability such that end-user{quote}",laszlok
HADOOP-15705,"Typo in the definition of ""stable"" in the interface classification","""Compatible changes allowed: maintenance (x.Y.0)""

should be 

""Compatible changes allowed: maintenance (x.y.Z)""",templedf
HADOOP-15704,Mark ABFS extension package and interfaces as LimitedPrivate/Unstable,"Refer to Steve's comments in HADOOP-15692.  Passing the FS or FS URI to the CustomDelegationTokenManager would allow it to provide per-filesystem tokens.  We currently have implementations of CustomDelegationTokenManager, and need to do a little leg work here, but it may be possible to update before GA.",stevel@apache.org
HADOOP-15703,ABFS - Implement client-side throttling ,"Big data workloads frequently exceed the AzureBlobFS max ingress and egress limits (https://docs.microsoft.com/en-us/azure/storage/common/storage-scalability-targets). For example, the max ingress limit for a GRS account in the United States is currently 10 Gbps. When the limit is exceeded, the AzureBlobFS service fails a percentage of incoming requests, and this causes the client to initiate the retry policy. The retry policy delays requests by sleeping, but the sleep duration is independent of the client throughput and account limit. This results in low throughput, due to the high number of failed requests and thrashing causes by the retry policy.

To fix this, we introduce a client-side throttle which minimizes failed requests and maximizes throughput. ",tmarquardt
HADOOP-15702,ABFS: Increase timeout of ITestAbfsReadWriteAndSeek,"ITestAbfsReadWriteAndSeek.testReadAndWriteWithDifferentBufferSizesAndSeek fails for me all the time. Let's increase the timout limit.

It also seems to get executed twice...",danielzhou
HADOOP-15700,ABFS: Failure in OpenSSLProvider should fall back to JSSE,Failure to {{OpenSSLProvider.register()}} should fall back to default JSSE initialization. This is needed to support Java 7 in case the HADOOP-15669 is back-ported to support Java7.,vishwajeet.dusane
HADOOP-15699,Fix some of testContainerManager failures in Windows,"TestContainerManager.testAuxPathHandler: The filename, directory name, or volume label syntax is incorrect
TestContainerManager.testForcefulShutdownSignal: ProcessStartFile doesn't exist!
TestContainerManager.testGracefulShutdownSignal: ProcessStartFile doesn't exist!
TestContainerManager.testOutputThreadDumpSignal: ProcessStartFile doesn't exist!",botong
HADOOP-15698,KMS log4j is not initialized properly at startup,"During KMs startup, log4j logs don't show up resulting in important logs getting omitted. This happens because log4 initialisation only happens in KMSWebApp#contextInitialized and logs written before that don't show up.

For example the following log never shows up:

[https://github.com/apache/hadoop/blob/a55d6bba71c81c1c4e9d8cd11f55c78f10a548b0/hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/util/ZKSignerSecretProvider.java#L197-L199]

Another example is that the KMS startup message never shows up in the kms logs.

Note that this works in the unit tests, because MiniKMS sets the log4j system property.",knanasi
HADOOP-15696,KMS performance regression due to too many open file descriptors after Jetty migration,"We recently found KMS performance regressed in Hadoop 3.0, possibly linking to the migration from Tomcat to Jetty in HADOOP-13597.

Symptoms:

# Hadoop 3.x KMS open file descriptors quickly rises to more than 10 thousand under stress, sometimes even exceeds 32K, which is the system limit, causing failures for any access to encryption zones. Our internal testing shows the openfd number was in the range of a few hundred in Hadoop 2.x, and it increases by almost 100x in Hadoop 3.
# Hadoop 3.x KMS as much as twice the heap size than in Hadoop 2.x. The same heap size can go OOM in Hadoop 3.x. Jxray analysis suggests most of them are temporary byte arrays associated with open SSL connections.
# Due to the heap usage, Hadoop 3.x KMS has more frequent GC activities, and we observed up to 20% performance reduction due to GC.

A possible solution is to reduce the idle timeout setting in HttpServer2. It is currently hard-coded 10 seconds. By setting it to 1 second, open fds dropped from 20 thousand down to 3 thousand in my experiment.

File this jira to invite open discussion for a solution.

Credit: [~misha@cloudera.com] for the proposed Jetty idle timeout remedy; [~xiaochen] for digging into this problem.

Screenshots:

CDH5 (Hadoop 2) KMS CPU utilization, resident memory and file descriptor chart.
 !Screen Shot 2018-08-22 at 4.30.39 PM.png! 
CDH6 (Hadoop 3) KMS CPU utilization, resident memory and file descriptor chart.
 !Screen Shot 2018-08-22 at 4.30.32 PM.png! 

CDH5 (Hadoop 2) GC activities on the KMS process
 !Screen Shot 2018-08-22 at 4.26.51 PM.png! 
CDH6 (Hadoop 3) GC activities on the KMS process
 !Screen Shot 2018-08-22 at 4.27.02 PM.png! 

JXray report
 !Screen Shot 2018-08-22 at 11.36.16 AM.png! 

open fd drops from 20 k down to 3k after the proposed change.
 !Screen Shot 2018-08-24 at 7.08.16 PM.png! 
",jojochuang
HADOOP-15694,ABFS: Allow OAuth credentials to not be tied to accounts,"Now that there's OAuth support, it's possible to have a notion of identity that's distinct from the account itself. If a cluster is configured via OAuth with it's own identity, it's likely operators will want to use that identity regardless of which storage account a job uses.

So OAuth configs right now (and probably others) are looked up with <config_key>.<account>. I propose that we add a function for looking up these configs that returns an account-specific value if it exists, but in the event it does not will also try to return <config_key>, if that exists.

I can work on a patch for this if nobody has any objections.",mackrorysd
HADOOP-15693,Typo in Secure Mode documentation,,tlevine
HADOOP-15692,ABFS: extensible support for custom oauth,ABFS supports oauth in various forms and needs to export interfaces for customization of FileSystem.getDelegationToken and getAccessToken.,danielzhou
HADOOP-15691,Add PathCapabilities to FS and FC to complement StreamCapabilities,"Add a {{PathCapabilities}} interface to both FileSystem and FileContext to declare the capabilities under the path of an FS

This is needed for 
* HADOOP-14707: declare that a dest FS supports permissions
* object stores to declare that they offer PUT-in-place alongside (slow-rename)
* Anything else where the implementation semantics of an FS is so different caller apps would benefit from probing for the underlying semantics

I know, we want all filesystem to work *exactly* the same. But it doesn't hold, especially for object stores —and to efficiently use them, callers need to be able to ask for specific features.",stevel@apache.org
HADOOP-15690,Hadoop docs' current should point to the latest release,"In [http://hadoop.apache.org/docs/,] the current folder points to Hadoop 2.9.1.

It should point to the latest release - Hadoop 3.1.1",hanishakoneru
HADOOP-15689,"Add ""*.patch"" into .gitignore file of branch-2",,demongaorui
HADOOP-15688,ABFS: InputStream wrapped in FSDataInputStream twice,"I can't read Parquet files from ABFS. It has 2 different implementations to read seekable streams, and it'll use the one that uses ByteBuffer reads if it can. It currently decides to use the ByteBuffer read implementation because the FSDataInputStream it gets back wraps another FSDataInputStream, which implements ByteBufferReadable.

That's not the most robust way to check that ByteBufferReads are supported by the ultimately underlying InputStream, but it's unnecessary and probably a mistake to double-wrap the InputStream, so let's not.",mackrorysd
HADOOP-15687,Credentials class should allow access to aliases,"The credentials class can read token files from disk which are keyed by an alias. It also allows to retrieve tokens by alias and it also allows to list all tokens.

It does not - however - allow to get the full map of all tokens including the aliases (or at least a list of all aliases).",lars_francke
HADOOP-15684,"triggerActiveLogRoll stuck on dead name node, when ConnectTimeoutException happens. ","When name node call triggerActiveLogRoll, and the cachedActiveProxy is a dead name node, it will throws a ConnectTimeoutException, expected behavior is to try next NN, but current logic doesn't do so, instead, it keeps trying the dead, mistakenly take it as active.

 

2018-08-17 10:02:12,001 WARN [Edit log tailer] org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Unable to trigger a roll of the active NN

org.apache.hadoop.net.ConnectTimeoutException: Call From SourceMachine001/SourceIP to001 TargetMachine001.ap.gbl:8020 failed on socket timeout exception: org.apache.hadoop.net.ConnectTimeoutException: 20000 millis timeout org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:298)

 

C:\Users\rotang>ping TargetMachine001

Pinging TargetMachine001[TargetIP001] with 32 bytes of data:
 Request timed out.
 Request timed out.
 Request timed out.
 Request timed out.

 Attachment is a log file saying how it repeatedly retries a dead name node, and a fix patch.

 I replaced the actual machine name/ip as SourceMachine001/SourceIP001 and TargetMachine001/TargetIP001.

 

How to Repro:

In a good running NNs, take down the active NN (don't let it come back during test), and then the stand by NNs will keep trying dead (old active) NN, because it is the cached one.",trjianjianjiao
HADOOP-15683,Client.setupConnection should not block Client.stop() calls,"If the IPC Client is still setting up connections when Client.stop() is called, the stop() call will not succeed until setupConnection finishes (successfully or with failure). 

This can cause very long delay (maxFailures * timeout can be more than 10minutes depending on configuration) in stopping the client. 
Client.stop() is for example used in NN switch from Standby to Active, and can therefore have very bad consequences and cause downtime.",lukmajercak
HADOOP-15682,ABFS: Add support for StreamCapabilities. Fix javadoc and checkstyle,"Add support for the new StreamCapabilities interface.  This work is similar to what was done for WASB [HADOOP-15677|https://jira.apache.org/jira/browse/HADOOP-15677].

Also fix javadoc and checkstyle errors.",tmarquardt
HADOOP-15680,ITestNativeAzureFileSystemConcurrencyLive times out,"When I am running tests locally ITestNativeAzureFileSystemConcurrencyLive sometimes times out.

I would like to increase the timeout to avoid unnecessary noise.",boky01
HADOOP-15679,ShutdownHookManager shutdown time needs to be configurable & extended,"HADOOP-12950 added a timeout on shutdowns to avoid problems with hanging shutdowns. But the timeout is too short for applications where a large flush of data is needed on shutdown.

A key example of this is Spark apps which save their history to object stores, where the file close() call triggers an upload of the final local cached block of data (could be 32+MB), and then execute the final mutipart commit.

Proposed

# make the default sleep time 30s, not 10s
# make it configurable with a time duration property (with minimum time of 1s.?)",stevel@apache.org
HADOOP-15677,WASB: Add support for StreamCapabilities,"StreamCapabilities is a new interface in branch-3, and was partially added to WASB.  Let's complete the implementation and add test coverage for block blobs, block blobs with compaction, and page blobs.",tmarquardt
HADOOP-15676,Cleanup TestSSLHttpServer,"This issue will fix: 
* Several typos in this class
* Code is not very well readable in some of the places.",snemeth
HADOOP-15674,Test failure TestSSLHttpServer.testExcludedCiphers with TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256 cipher suite,"Running {{hadoop/hadoop-common-project/hadoop-common# mvn test -Dtest=""TestSSLHttpServer#testExcludedCiphers"" -Dhttps.protocols=TLSv1.2 -Dhttps.cipherSuites=TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256}} fails with:

{noformat}
Error Message
No Ciphers in common, SSLHandshake must fail.
Stacktrace
	java.lang.AssertionError: No Ciphers in common, SSLHandshake must fail.
	at org.junit.Assert.fail(Assert.java:88)
	at org.apache.hadoop.http.TestSSLHttpServer.testExcludedCiphers(TestSSLHttpServer.java:178)
{noformat}
",snemeth
HADOOP-15673,Hadoop:3 image is missing from dockerhub,Currently the apache/hadoop:3 image is missing from the dockerhub as the Dockerfile in docker-hadoop-3 branch contains the outdated 3.0.0 download url. It should be updated to the latest 3.1.1 url.,bharatviswa
HADOOP-15671,AliyunOSS: Support Assume Roles in AliyunOSS,"We will add assume role function in Aliyun OSS.

For details about assume role and sts token, click the link below:

[https://www.alibabacloud.com/help/doc-detail/31935.html?spm=a2c5t.11065259.1996646101.searchclickresult.1fad155aKOUvJZ]

 

Major Changes:
 # Stabilise the constructor of CredentialsProvider so that other developers can have their own implementations.
 #  add assumed role functions for hadoop aliyun module",wujinhu
HADOOP-15670,UserGroupInformation TGT renewer thread doesn't use monotonically increasing time for calculating interval to sleep,"As per the [documentation of Time#now() method|https://github.com/apache/hadoop/blob/74411ce0ce7336c0f7bb5793939fdd64a5dcdef6/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/Time.java#L49-L57], it should not be used for calculating duration or interval to sleep. But the TGT renewer thread in UserGroupInformation object doesn't follow this recommendation,

[https://github.com/apache/hadoop/blob/74411ce0ce7336c0f7bb5793939fdd64a5dcdef6/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/UserGroupInformation.java#L892-L899]

This should be fixed to use Time.monotonicNow() API instead.

 ",hgadre
HADOOP-15669,ABFS: Improve HTTPS Performance,We see approximately 50% worse throughput for ABFS over HTTPs vs HTTP.  Lets perform a detailed measurement and see what can be done to improve throughput.,vishwajeet.dusane
HADOOP-15668,mvn test goal fails on HADOOP-15407 branch,"It's very easy to reproduce:
{code}cd hadoop-common-project/hadoop-common/
mvn test -Dtest=Whatever{code}

The error is due to
{code}     [exec] Running bats -t hadoop_stop_daemon.bats
     [exec] 1..2
     [exec] ok 1 hadoop_stop_daemon_changing_pid
     [exec] not ok 2 hadoop_stop_daemon_force_kill
     [exec] # (in test file hadoop_stop_daemon.bats, line 43)
     [exec] #   `[ -f ${TMP}/pidfile ]' failed
     [exec] # bindir: /Users/abokor/work/hadoop/hadoop-common-project/hadoop-common/src/test/scripts
     [exec] # sh: /Users/abokor/work/hadoop/hadoop-common-project/hadoop-common/src/test/scripts/process_with_sigterm_trap.sh: No such file or directory{code}


This happens because actually 3 commits belong to HADOOP-15527 but HADOOP-15407 branch contains only one of them so the test won't find process_with_sigterm_trap.sh.

I am not sure what is the best practice to solve this kind of issues. Is patch required or can somebody just cherry-pick the missing commits?",boky01
HADOOP-15667,FileSystemMultipartUploader should verify that UploadHandle has non-0 length,"The S3AMultipartUploader has a good check on the length of the UploadHandle. This should be moved to MultipartUploader, made protected, and called in the various implementations.",ehiggs
HADOOP-15664,ABFS: Reduce test run time via parallelization and grouping,"1) Let's reduce the total test runtime by improving parallelization of the tests.

2) Let's make it possible to select WASB tests, ABFS tests, or both so developers can run only the tests appropriate for the change they've made.

3) Update the testing-azure.md accordingly",danielzhou
HADOOP-15663,ABFS: Simplify configuration,"Configuration for WASB and ABFS is too complex.  The current approach is to use four files for test configuration. 

Both WASB and ABFS have basic test configuration which is committed to the repo (azure-test.xml and azure-bfs-test.xml).  Currently these contain the fs.AbstractFileSystem.[scheme].impl configuration, but otherwise are empty except for an include reference to a file containing the endpoint credentials. 

Both WASB and ABFS have endpoint credential configuration files (azure-auth-keys.xml and azure-bfs-auth-keys.xml).  These have been added to .gitignore to prevent them from accidentally being submitted in a patch, which would leak the developers storage account credentials.  These files contain account names, storage account keys, and service endpoints.

There is some overlap of the configuration for WASB and ABFS, where they use the same property name but use different values.  

1) Let's reduce the number of test configuration files to one, if possible.

2) Let's simplify the account name, key, and endpoint configuration for WASB and ABFS if possible, but still support the legacy way of doing it, which is very error prone.

3) Let's improve error handling, so that typos or misconfiguration are not so difficult to troubleshoot.",danielzhou
HADOOP-15662,ABFS: Better exception handling of DNS errors,"DNS errors are common during testing due to typos or misconfiguration.  They can also occur in production, as some transient DNS issues occur from time to time. 

1) Let's investigate if we can distinguish between the two and fail fast for the test issues, but continue to have retry logic for the transient DNS issues in production.

2) Let's improve the error handling of DNS failures, so the user has an actionable error message.",danielzhou
HADOOP-15661,ABFS: Add support for ACL,- Add support for ACL,danielzhou
HADOOP-15660,ABFS: Add support for OAuth,- Add support for OAuth,danielzhou
HADOOP-15659,ABFS: Code changes for bug fix and new tests,"- add bug fixes.
- remove unnecessary dependencies.
- add new tests for code changes.",danielzhou
HADOOP-15657,Registering MutableQuantiles via Metric annotation,"Currently when creating new metrics we use @Metric annotation for registering the MutableMetric i.e
{code:java}
@Metric
private MutableInt foobarMetricCount
{code}
However,  there's no support for registering MutableQuantiles via Metric annotation, hence creating this Jira to register MutableQuantiles via Metric annotation. 

Example:

 
{code:java}
@Metric(about = ""async PUT entities latency"", valueName = ""latency"", interval = 10)
private MutableQuantiles foobarAsyncLatency;
 
{code}
 ",sushil-k-s
HADOOP-15656,Support byteman in hadoop-runner baseimage,"[Byteman|http://byteman.jboss.org/] is an easy to use tool to instrument a java process with agent string.

For example [this script|https://gist.githubusercontent.com/elek/0589a91b4d55afb228279f6c4f04a525/raw/8bb4e03de7397c8a9d9bb74a5ec80028b42575c4/hadoop.btm] defines a rule to print out all the hadoop rpc traffic to the standard output (which is extremely useful for testing development).

This patch adds the byteman.jar to the baseimage and defines a simple logic to add agent instrumentation string to the HADOOP_OPTS (optional it also could download the byteman script from an url)

",elek
HADOOP-15655,Enhance KMS client retry behavior,"KMS doesn't retry upon SocketTimeoutException (the ssl connection was established, but the handshake timed out).
{noformat}
6:08:55.315 PM	WARN	KMSClientProvider	
Failed to connect to example.com:16000
6:08:55.317 PM	WARN	LoadBalancingKMSClientProvider	
KMS provider at [https://example.com:16000/kms/v1/] threw an IOException: 
java.net.SocketTimeoutException: Read timed out
	at java.net.SocketInputStream.socketRead0(Native Method)
	at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
	at java.net.SocketInputStream.read(SocketInputStream.java:171)
	at java.net.SocketInputStream.read(SocketInputStream.java:141)
	at sun.security.ssl.InputRecord.readFully(InputRecord.java:465)
	at sun.security.ssl.InputRecord.read(InputRecord.java:503)
	at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:983)
	at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1385)
	at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1413)
	at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1397)
	at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559)
	at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185)
	at sun.net.www.protocol.https.HttpsURLConnectionImpl.connect(HttpsURLConnectionImpl.java:153)
	at org.apache.hadoop.security.authentication.client.KerberosAuthenticator.authenticate(KerberosAuthenticator.java:186)
	at org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator.authenticate(DelegationTokenAuthenticator.java:140)
	at org.apache.hadoop.security.authentication.client.AuthenticatedURL.openConnection(AuthenticatedURL.java:348)
	at org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL.openConnection(DelegationTokenAuthenticatedURL.java:333)
	at org.apache.hadoop.crypto.key.kms.KMSClientProvider$1.run(KMSClientProvider.java:478)
	at org.apache.hadoop.crypto.key.kms.KMSClientProvider$1.run(KMSClientProvider.java:473)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1685)
	at org.apache.hadoop.crypto.key.kms.KMSClientProvider.createConnection(KMSClientProvider.java:472)
	at org.apache.hadoop.crypto.key.kms.KMSClientProvider.decryptEncryptedKey(KMSClientProvider.java:788)
	at org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider$5.call(LoadBalancingKMSClientProvider.java:288)
	at org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider$5.call(LoadBalancingKMSClientProvider.java:284)
	at org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider.doOp(LoadBalancingKMSClientProvider.java:124)
	at org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider.decryptEncryptedKey(LoadBalancingKMSClientProvider.java:284)
	at org.apache.hadoop.crypto.key.KeyProviderCryptoExtension.decryptEncryptedKey(KeyProviderCryptoExtension.java:532)
	at org.apache.hadoop.hdfs.DFSClient.decryptEncryptedDataEncryptionKey(DFSClient.java:927)
	at org.apache.hadoop.hdfs.DFSClient.createWrappedInputStream(DFSClient.java:946)
	at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:316)
	at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:311)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:323)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:949)
	at org.apache.hadoop.hbase.util.FSUtils.getVersion(FSUtils.java:338)
	at org.apache.hadoop.hbase.util.FSUtils.checkVersion(FSUtils.java:423)
	at org.apache.hadoop.hbase.master.MasterFileSystem.checkRootDir(MasterFileSystem.java:260)
	at org.apache.hadoop.hbase.master.MasterFileSystem.createInitialFileSystemLayout(MasterFileSystem.java:151)
	at org.apache.hadoop.hbase.master.MasterFileSystem.<init>(MasterFileSystem.java:122)
	at org.apache.hadoop.hbase.master.HMaster.finishActiveMasterInitialization(HMaster.java:795)
	at org.apache.hadoop.hbase.master.HMaster.startActiveMasterManager(HMaster.java:2036)
	at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:553)
	at java.lang.Thread.run(Thread.java:748)
6:08:55.346 PM	WARN	LoadBalancingKMSClientProvider	
Aborting since the Request has failed with all KMS providers(depending on hadoop.security.kms.client.failover.max.retries=1 setting and numProviders=1) in the group OR the exception is not recoverable
{noformat}",knanasi
HADOOP-15645,ITestS3GuardToolLocal.testDiffCommand fails if bucket has per-bucket binding to DDB,"If your bucket has a per-bucket setting to use S3Guard, then the ITestS3GuardToolLocal.testDiffCommand test can fail as fs-only data can creep into the metastore. 

The rawfs should use clearBucketOption to clear the metastore binding, so guarantee a real raw fs",stevel@apache.org
HADOOP-15644,Hadoop Docker Image Pip Install Fails on branch-2,"HADOOP-15610 fixes the pip install failures on branch 3.x releases, but it is still failing on branch-2, though with a slightly different error message
{code:java}
Downloading/unpacking pylint
  Running setup.py (path:/tmp/pip_build_root/pylint/setup.py) egg_info for package pylint
    /usr/lib/python2.7/distutils/dist.py:267: UserWarning: Unknown distribution option: 'python_requires'
      warnings.warn(msg)
    Traceback (most recent call last):
      File ""<string>"", line 17, in <module>
      File ""/tmp/pip_build_root/pylint/setup.py"", line 177, in <module>
        install()
      File ""/tmp/pip_build_root/pylint/setup.py"", line 174, in install
        **kwargs)
      File ""/usr/lib/python2.7/distutils/core.py"", line 111, in setup
        _setup_distribution = dist = klass(attrs)
      File ""/usr/lib/python2.7/dist-packages/setuptools/dist.py"", line 239, in __init__
        self.fetch_build_eggs(attrs.pop('setup_requires'))
      File ""/usr/lib/python2.7/dist-packages/setuptools/dist.py"", line 264, in fetch_build_eggs
        replace_conflicting=True
      File ""/usr/lib/python2.7/dist-packages/pkg_resources.py"", line 620, in resolve
        dist = best[req.key] = env.best_match(req, ws, installer)
      File ""/usr/lib/python2.7/dist-packages/pkg_resources.py"", line 858, in best_match
        return self.obtain(req, installer) # try and download/install
      File ""/usr/lib/python2.7/dist-packages/pkg_resources.py"", line 870, in obtain
        return installer(requirement)
      File ""/usr/lib/python2.7/dist-packages/setuptools/dist.py"", line 314, in fetch_build_egg
        return cmd.easy_install(req)
      File ""/usr/lib/python2.7/dist-packages/setuptools/command/easy_install.py"", line 616, in easy_install
        return self.install_item(spec, dist.location, tmpdir, deps)
      File ""/usr/lib/python2.7/dist-packages/setuptools/command/easy_install.py"", line 646, in install_item
        dists = self.install_eggs(spec, download, tmpdir)
      File ""/usr/lib/python2.7/dist-packages/setuptools/command/easy_install.py"", line 834, in install_eggs
        return self.build_and_install(setup_script, setup_base)
      File ""/usr/lib/python2.7/dist-packages/setuptools/command/easy_install.py"", line 1040, in build_and_install
        self.run_setup(setup_script, setup_base, args)
      File ""/usr/lib/python2.7/dist-packages/setuptools/command/easy_install.py"", line 1025, in run_setup
        run_setup(setup_script, args)
      File ""/usr/lib/python2.7/dist-packages/setuptools/sandbox.py"", line 50, in run_setup
        lambda: execfile(
      File ""/usr/lib/python2.7/dist-packages/setuptools/sandbox.py"", line 100, in run
        return func()
      File ""/usr/lib/python2.7/dist-packages/setuptools/sandbox.py"", line 52, in <lambda>
        {'__file__':setup_script, '__name__':'__main__'}
      File ""setup.py"", line 76, in <module>

      File ""/usr/lib/python2.7/distutils/core.py"", line 111, in setup
        _setup_distribution = dist = klass(attrs)
      File ""/usr/lib/python2.7/dist-packages/setuptools/dist.py"", line 243, in __init__
        _Distribution.__init__(self,attrs)
      File ""/usr/lib/python2.7/distutils/dist.py"", line 287, in __init__
        self.finalize_options()
      File ""/usr/lib/python2.7/dist-packages/setuptools/dist.py"", line 277, in finalize_options
        ep.load()(self, ep.name, value)
      File ""build/bdist.linux-x86_64/egg/setuptools_scm/integration.py"", line 10, in version_keyword
      File ""build/bdist.linux-x86_64/egg/setuptools_scm/version.py"", line 66, in _warn_if_setuptools_outdated
    setuptools_scm.version.SetuptoolsOutdatedWarning: your setuptools is too old (<12)
    Complete output from command python setup.py egg_info:
    /usr/lib/python2.7/distutils/dist.py:267: UserWarning: Unknown distribution option: 'python_requires'

  warnings.warn(msg)

Traceback (most recent call last):

  File ""<string>"", line 17, in <module>

  File ""/tmp/pip_build_root/pylint/setup.py"", line 177, in <module>

    install()

  File ""/tmp/pip_build_root/pylint/setup.py"", line 174, in install

    **kwargs)

  File ""/usr/lib/python2.7/distutils/core.py"", line 111, in setup

    _setup_distribution = dist = klass(attrs)

  File ""/usr/lib/python2.7/dist-packages/setuptools/dist.py"", line 239, in __init__

    self.fetch_build_eggs(attrs.pop('setup_requires'))

  File ""/usr/lib/python2.7/dist-packages/setuptools/dist.py"", line 264, in fetch_build_eggs

    replace_conflicting=True
  File ""/usr/lib/python2.7/dist-packages/pkg_resources.py"", line 620, in resolve

    dist = best[req.key] = env.best_match(req, ws, installer)

  File ""/usr/lib/python2.7/dist-packages/pkg_resources.py"", line 858, in best_match

    return self.obtain(req, installer) # try and download/install

  File ""/usr/lib/python2.7/dist-packages/pkg_resources.py"", line 870, in obtain

    return installer(requirement)

  File ""/usr/lib/python2.7/dist-packages/setuptools/dist.py"", line 314, in fetch_build_egg

    return cmd.easy_install(req)

  File ""/usr/lib/python2.7/dist-packages/setuptools/command/easy_install.py"", line 616, in easy_install

    return self.install_item(spec, dist.location, tmpdir, deps)

  File ""/usr/lib/python2.7/dist-packages/setuptools/command/easy_install.py"", line 646, in install_item

    dists = self.install_eggs(spec, download, tmpdir)

  File ""/usr/lib/python2.7/dist-packages/setuptools/command/easy_install.py"", line 834, in install_eggs

    return self.build_and_install(setup_script, setup_base)

  File ""/usr/lib/python2.7/dist-packages/setuptools/command/easy_install.py"", line 1040, in build_and_install

    self.run_setup(setup_script, setup_base, args)

  File ""/usr/lib/python2.7/dist-packages/setuptools/command/easy_install.py"", line 1025, in run_setup

    run_setup(setup_script, args)

  File ""/usr/lib/python2.7/dist-packages/setuptools/sandbox.py"", line 50, in run_setup

    lambda: execfile(

  File ""/usr/lib/python2.7/dist-packages/setuptools/sandbox.py"", line 100, in run

    return func()

  File ""/usr/lib/python2.7/dist-packages/setuptools/sandbox.py"", line 52, in <lambda>

    {'__file__':setup_script, '__name__':'__main__'}

  File ""setup.py"", line 76, in <module>



  File ""/usr/lib/python2.7/distutils/core.py"", line 111, in setup

    _setup_distribution = dist = klass(attrs)

  File ""/usr/lib/python2.7/dist-packages/setuptools/dist.py"", line 243, in __init__

    _Distribution.__init__(self,attrs)

  File ""/usr/lib/python2.7/distutils/dist.py"", line 287, in __init__

    self.finalize_options()

  File ""/usr/lib/python2.7/dist-packages/setuptools/dist.py"", line 277, in finalize_options

    ep.load()(self, ep.name, value)

  File ""build/bdist.linux-x86_64/egg/setuptools_scm/integration.py"", line 10, in version_keyword

  File ""build/bdist.linux-x86_64/egg/setuptools_scm/version.py"", line 66, in _warn_if_setuptools_outdated

setuptools_scm.version.SetuptoolsOutdatedWarning: your setuptools is too old (<12)

----------------------------------------
Cleaning up...
Command python setup.py egg_info failed with error code 1 in /tmp/pip_build_root/pylint
Storing debug log for failure in /root/.pip/pip.log
The command '/bin/sh -c pip install pylint' returned a non-zero code: 1l{code}",haibochen
HADOOP-15642,Update aws-sdk version to 1.11.375,"Move to a later version of the AWS SDK library for a different set of features and issues.

proposed version: 1.11.375

One thing which doesn't work on the one we ship with is the ability to create assumed role sessions >1h, as there's a check in the client lib for role-duration <= 3600 seconds. I'll assume more recent SDKs delegate duration checks to the far end.

see: [https://aws.amazon.com/about-aws/whats-new/2018/03/longer-role-sessions/]

* assuming later versions will extend assumed role life, docs will need changing, 
* Adding a test in HADOOP-15583 which expects an error message if you ask for a duration of 3h; this should act as the test to see what happens.
* think this time would be good to explicitly write down the SDK update process",stevel@apache.org
HADOOP-15641,Fix ozone docker-compose illegal character in hostname,"This generated warnings  in GRPC/Ratis.

{code}
scm_1           | Jul 30, 2018 7:08:47 PM org.apache.ratis.shaded.io.grpc.internal.ProxyDetectorImpl detectProxy
scm_1           | WARNING: Failed to construct URI for proxy lookup, proceeding without proxy
scm_1           | java.net.URISyntaxException: Illegal character in hostname at index 13: https://ozone_datanode_1.ozone_default:9858
scm_1           | 	at java.net.URI$Parser.fail(URI.java:2848)
scm_1           | 	at java.net.URI$Parser.parseHostname(URI.java:3387)
scm_1           | 	at java.net.URI$Parser.parseServer(URI.java:3236)
scm_1           | 	at java.net.URI$Parser.parseAuthority(URI.java:3155)
scm_1           | 	at java.net.URI$Parser.parseHierarchical(URI.java:3097)
scm_1           | 	at java.net.URI$Parser.parse(URI.java:3053)
scm_1           | 	at java.net.URI.<init>(URI.java:673)
scm_1           | 	at org.apache.ratis.shaded.io.grpc.internal.ProxyDetectorImpl.detectProxy(ProxyDetectorImpl.java:128)
scm_1           | 	at org.apache.ratis.shaded.io.grpc.internal.ProxyDetectorImpl.proxyFor(ProxyDetectorImpl.java:118)
scm_1           | 	at org.apache.ratis.shaded.io.grpc.internal.InternalSubchannel.startNewTransport(InternalSubchannel.java:207)
scm_1           | 	at org.apache.ratis.shaded.io.grpc.internal.InternalSubchannel.obtainActiveTransport(InternalSubchannel.java:188)
scm_1           | 	at org.apache.ratis.shaded.io.grpc.internal.ManagedChannelImpl$SubchannelImpl.requestConnection(ManagedChannelImpl.java:1130)
scm_1           | 	at org.apache.ratis.shaded.io.grpc.PickFirstBalancerFactory$PickFirstBalancer.handleResolvedAddressGroups(PickFirstBalancerFactory.java:79)
scm_1           | 	at org.apache.ratis.shaded.io.grpc.internal.ManagedChannelImpl$NameResolverListenerImpl$1NamesResolved.run(ManagedChannelImpl.java:1032)
scm_1           | 	at org.apache.ratis.shaded.io.grpc.internal.ChannelExecutor.drain(ChannelExecutor.java:73)
scm_1           | 	at org.apache.ratis.shaded.io.grpc.internal.ManagedChannelImpl$LbHelperImpl.runSerialized(ManagedChannelImpl.java:1000)
scm_1           | 	at org.apache.ratis.shaded.io.grpc.internal.ManagedChannelImpl$NameResolverListenerImpl.onAddresses(ManagedChannelImpl.java:1044)
scm_1           | 	at org.apache.ratis.shaded.io.grpc.internal.DnsNameResolver$1.run(DnsNameResolver.java:201)
scm_1           | 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
scm_1           | 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
scm_1           | 	at java.lang.Thread.run(Thread.java:748)
scm_1           | 

{code}
",xyao
HADOOP-15638,KMS Accept Queue Size default changed from 500 to 128 in Hadoop 3.x,"HADOOP-13597 migrated KMS from Tomcat to Jetty.
In Hadoop 2.x, the Accept Queue Size default value was set by environment variable KMS_ACCEPT_QUEUE with default value 500.

But after the migration, Hadoop 3.x now uses the default HttpServer2 configuration to set accept queue size (hadoop.http.socket.backlog.size), which is 128.

Should we restore the default accept queue size in kms-default.xml? Should we document it as a known issue? After all, Hadoop 2 --> 3 allows breaking changing.",jojochuang
HADOOP-15637,LocalFs#listLocatedStatus does not filter out hidden .crc files,"After HADOOP-7165, {{LocalFs#listLocatedStatus}} incorrectly returns the hidden {{.crc}} files used to store checksum information. This is because HADOOP-7165 implemented {{listLocatedStatus}} on {{FilterFs}}, so the default implementation is no longer used, and {{FilterFs}} directly calls the raw FS since {{listLocatedStatus}} is not defined in {{ChecksumFs}}.",xkrogen
HADOOP-15636,Add ITestDynamoDBMetadataStore,"I committed HADOOP-14918 but I forgot to 'git add' the renamed test file. I would just add it and commit and reference the JIRA, but testTableProvision is now timing out, so we should look into that.",gabor.bota
HADOOP-15635,s3guard set-capacity command to fail fast if bucket is unguarded,"If you try to do {{hadoop s3guard set-capacity s3a://landsat-pds}}, or any other bucket which exists but doesn't have s3guard enabled, you get a stack trace reporting that the ddb table doesn't exist.

the command should check for the bucket being guarded and fail on that",gabor.bota
HADOOP-15633,fs.TrashPolicyDefault: Can't create trash directory,"Reproduce it as follow

{code:java}
hadoop fs -mkdir /user/hadoop/aaa
hadoop fs -touchz /user/hadoop/aaa/bbb
hadoop fs -rm /user/hadoop/aaa/bbb
hadoop fs -mkdir /user/hadoop/aaa/bbb
hadoop fs -touchz /user/hadoop/aaa/bbb/ccc
hadoop fs -rm /user/hadoop/aaa/bbb/ccc
{code}

Then we get errors 

{code:java}
18/07/26 17:55:24 WARN fs.TrashPolicyDefault: Can't create trash directory: hdfs://xxx/user/hadoop/.Trash/Current/user/hadoop/aaa/bbb
org.apache.hadoop.fs.FileAlreadyExistsException: Path is not a directory: /user/hadoop/.Trash/Current/user/hadoop/aaa/bbb
        at org.apache.hadoop.hdfs.server.namenode.FSDirMkdirOp.mkdirs(FSDirMkdirOp.java:65)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3961)
        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:984)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:622)
        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:982)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2115)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2111)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1867)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2111)

        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
        at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)
        at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:73)
        at org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:3002)
        at org.apache.hadoop.hdfs.DFSClient.mkdirs(DFSClient.java:2970)
        at org.apache.hadoop.hdfs.DistributedFileSystem$21.doCall(DistributedFileSystem.java:1047)
        at org.apache.hadoop.hdfs.DistributedFileSystem$21.doCall(DistributedFileSystem.java:1043)
        at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
        at org.apache.hadoop.hdfs.DistributedFileSystem.mkdirsInternal(DistributedFileSystem.java:1061)
        at org.apache.hadoop.hdfs.DistributedFileSystem.mkdirs(DistributedFileSystem.java:1036)
        at org.apache.hadoop.fs.TrashPolicyDefault.moveToTrash(TrashPolicyDefault.java:136)
        at org.apache.hadoop.fs.Trash.moveToTrash(Trash.java:114)
        at org.apache.hadoop.fs.Trash.moveToAppropriateTrash(Trash.java:95)
        at org.apache.hadoop.fs.shell.Delete$Rm.moveToTrash(Delete.java:118)
        at org.apache.hadoop.fs.shell.Delete$Rm.processPath(Delete.java:105)
        at org.apache.hadoop.fs.shell.Command.processPaths(Command.java:317)
        at org.apache.hadoop.fs.shell.Command.processPathArgument(Command.java:289)
        at org.apache.hadoop.fs.shell.Command.processArgument(Command.java:271)
        at org.apache.hadoop.fs.shell.Command.processArguments(Command.java:255)
        at org.apache.hadoop.fs.shell.Command.processRawArguments(Command.java:201)
        at org.apache.hadoop.fs.shell.Command.run(Command.java:165)
        at org.apache.hadoop.fs.FsShell.run(FsShell.java:287)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:84)
        at org.apache.hadoop.fs.FsShell.main(FsShell.java:340)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.fs.FileAlreadyExistsException): Path is not a directory: /user/hadoop/.Trash/Current/user/hadoop/aaa/bbb
        at org.apache.hadoop.hdfs.server.namenode.FSDirMkdirOp.mkdirs(FSDirMkdirOp.java:65)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3961)
        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:984)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:622)
        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:982)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2115)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2111)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1867)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2111)

        at org.apache.hadoop.ipc.Client.call(Client.java:1474)
        at org.apache.hadoop.ipc.Client.call(Client.java:1412)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
        at com.sun.proxy.$Proxy10.mkdirs(Unknown Source)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.mkdirs(ClientNamenodeProtocolTranslatorPB.java:558)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:253)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:101)
        at com.sun.proxy.$Proxy11.mkdirs(Unknown Source)
        at org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:3000)
        ... 21 more
{code}

",ferhui
HADOOP-15629,Missing trimming in readlink in case of protocol,"When extending the unit tests for the links, we surfaced errors in readLink.",giovanni.fumarola
HADOOP-15627,S3A ITestS3GuardWriteBack failing if bucket explicitly set to s3guard+DDB,"Repeatable failure in {{ITestS3GuardWriteBack.testListStatusWriteBack}}

Possible causes could include
* test not setting up the three fs instances
* (disabled) caching not isolating properly
* something more serious",stevel@apache.org
HADOOP-15626,FileContextMainOperationsBaseTest.testBuilderCreateAppendExistingFile fails on filesystems without append.,After HADOOP-14396. one of the new tests fails on S3A because append() isn't supported there,stevel@apache.org
HADOOP-15625,S3A input stream to use etags to detect changed source files,"S3A input stream doesn't handle changing source files any better than the other cloud store connectors. Specifically: it doesn't noticed it has changed, caches the length from startup, and whenever a seek triggers a new GET, you may get one of: old data, new data, and even perhaps go from new data to old data due to eventual consistency.

We can't do anything to stop this, but we could detect changes by

# caching the etag of the first HEAD/GET (we don't get that HEAD on open with S3Guard, BTW)
# on future GET requests, verify the etag of the response
# raise an IOE if the remote file changed during the read.

It's a more dramatic failure, but it stops changes silently corrupting things.",brahmareddy
HADOOP-15621,S3Guard: Implement time-based (TTL) expiry for Authoritative Directory Listing,"Similar to HADOOP-13649, I think we should add a TTL (time to live) feature to the Dynamo metadata store (MS) for S3Guard.

This is a similar concept to an ""online algorithm"" version of the CLI prune() function, which is the ""offline algorithm"".

Why: 
 1. Self healing (soft state): since we do not implement transactions around modification of the two systems (s3 and metadata store), certain failures can lead to inconsistency between S3 and the metadata store (MS) state. Having a time to live (TTL) on each entry in S3Guard means that any inconsistencies will be time bound. Thus ""wait and restart your job"" becomes a valid, if ugly, way to get around any issues with FS client failure leaving things in a bad state.
 2. We could make manual invocation of `hadoop s3guard prune ...` unnecessary, depending on the implementation.
 3. Makes it possible to fix the problem that dynamo MS prune() doesn't prune directories due to the lack of true modification time.

How:
 I think we need a new column in the dynamo table ""entry last written time"". This is updated each time the entry is written to dynamo.
 After that we can either
 1. Have the client simply ignore / elide any entries that are older than the configured TTL.
 2. Have the client delete entries older than the TTL.

The issue with #2 is it will increase latency if done inline in the context of an FS operation. We could mitigate this some by using an async helper thread, or probabilistically doing it ""some times"" to amortize the expense of deleting stale entries (allowing some batching as well).

Caveats:
 - Clock synchronization as usual is a concern. Many clusters already keep clocks close enough via NTP. We should at least document the requirement along with the configuration knob that enables the feature.",gabor.bota
HADOOP-15620,Über-jira: S3A phase VI: Hadoop 3.3 features,,stevel@apache.org
HADOOP-15619,Über-JIRA: S3Guard Phase IV: Hadoop 3.3 features,Features for S3Guard for Hadoop 3.3. Goal: take the experimental tag off,stevel@apache.org
HADOOP-15618,Fix debug log for property IPC_CLIENT_BIND_WILDCARD_ADDR_KEY,"Fix log message in Client class.
Currently:{code}LOG.debug(""{} set to true. Will bind client sockets to wildcard ""
            + ""address."",
        CommonConfigurationKeys.IPC_CLIENT_BIND_WILDCARD_ADDR_KEY);{code}
to
{code}LOG.debug(""{} set to {}"", CommonConfigurationKeys.IPC_CLIENT_BIND_WILDCARD_ADDR_KEY, bindToWildCardAddress);{code}
",ajayydv
HADOOP-15616,Incorporate Tencent Cloud COS File System Implementation,"Tencent cloud is top 2 cloud vendors in China market and the object store COS （[https://intl.cloud.tencent.com/product/cos]） is widely used among China’s cloud users but now it is hard for hadoop user to access data laid on COS storage as no native support for COS in Hadoop.

This work aims to integrate Tencent cloud COS with Hadoop/Spark/Hive, just like what we do before for S3, ADL, OSS, etc. With simple configuration, Hadoop applications can read/write data from COS without any code change.",yuyang733
HADOOP-15614,TestGroupsCaching.testExceptionOnBackgroundRefreshHandled reliably fails,"When {{testExceptionOnBackgroundRefreshHandled}} is run individually, it reliably fails. It seems like a fundamental bug in the test or groups caching.

A similar issue was dealt with in HADOOP-13375. [~cheersyang], do you have any insight into this?

This test case was added in HADOOP-13263.",cheersyang
HADOOP-15613,KerberosAuthenticator should resolve the hostname to get the service principal,"When in rest URL, IP is used as a address then ""KerberosAuthenticator.this.url.getHost()"" not able to resolve the hostname. This hostname will be used to construct server HTTP principal.",surendrasingh
HADOOP-15612,Improve exception when tfile fails to load LzoCodec ,"When hadoop-lzo is not on classpath you get
{code:java}
java.io.IOException: LZO codec class not specified. Did you forget to set property io.compression.codec.lzo.class?{code}
which is probably rarely the real cause given the default class name. The real root cause is not attached to the exception thrown.",jira.shegalov
HADOOP-15611,Log more details for FairCallQueue,"In the usage of the FairCallQueue, we find there missing some Key log. Only a few logs are printed, it makes us hard to learn and debug this feature.

At least, following places can print more logs.
* DecayRpcScheduler#decayCurrentCounts
* WeightedRoundRobinMultiplexer#moveToNextQueue",jianliang.wu
HADOOP-15610,Hadoop Docker Image Pip Install Fails,"The Hadoop Docker image on trunk does not build. The pip package on the Ubuntu Xenial repo is out of date and fails by throwing the following error when attempting to install pylint:

""You are using pip version 8.1.1, however version 10.0.1 is available""

The following patch fixes this issue.",jackbearden
HADOOP-15609,Retry KMS calls when SSLHandshakeException occurs,"KMS call should retry when javax.net.ssl.SSLHandshakeException occurs and FailoverOnNetworkExceptionRetry policy is used.

For example in the following stack trace, we can see that the KMS Provider's connection is lost, an SSLHandshakeException is thrown and the operation is not retried:
{code}
W0711 18:19:50.213472  1508 LoadBalancingKMSClientProvider.java:132] KMS provider at [https://example.com:16000/kms/v1/] threw an IOException:
Java exception follows:
javax.net.ssl.SSLHandshakeException: Remote host closed connection during handshake
        at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:1002)
        at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1385)
        at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1413)
        at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1397)
        at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559)
        at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185)
        at sun.net.www.protocol.http.HttpURLConnection.getOutputStream0(HttpURLConnection.java:1316)
        at sun.net.www.protocol.http.HttpURLConnection.getOutputStream(HttpURLConnection.java:1291)
        at sun.net.www.protocol.https.HttpsURLConnectionImpl.getOutputStream(HttpsURLConnectionImpl.java:250)
        at org.apache.hadoop.crypto.key.kms.KMSClientProvider.call(KMSClientProvider.java:512)
        at org.apache.hadoop.crypto.key.kms.KMSClientProvider.call(KMSClientProvider.java:502)
        at org.apache.hadoop.crypto.key.kms.KMSClientProvider.decryptEncryptedKey(KMSClientProvider.java:791)
        at org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider$5.call(LoadBalancingKMSClientProvider.java:288)
        at org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider$5.call(LoadBalancingKMSClientProvider.java:284)
        at org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider.doOp(LoadBalancingKMSClientProvider.java:124)
        at org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider.decryptEncryptedKey(LoadBalancingKMSClientProvider.java:284)
        at org.apache.hadoop.crypto.key.KeyProviderCryptoExtension.decryptEncryptedKey(KeyProviderCryptoExtension.java:532)
        at org.apache.hadoop.hdfs.DFSClient.decryptEncryptedDataEncryptionKey(DFSClient.java:927)
        at org.apache.hadoop.hdfs.DFSClient.createWrappedInputStream(DFSClient.java:946)
        at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:316)
        at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:311)
        at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
        at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:323)
Caused by: java.io.EOFException: SSL peer shut down incorrectly
        at sun.security.ssl.InputRecord.read(InputRecord.java:505)
        at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:983)
        ... 22 more
W0711 18:19:50.239328  1508 LoadBalancingKMSClientProvider.java:149] Aborting since the Request has failed with all KMS providers(depending on hadoop.security.kms.client.failover.max.retries=1 setting and numProviders=1) in the group OR the exception is not recoverable
{code}",knanasi
HADOOP-15608,File expansion should be disable in hadoop-functions.sh when iterate over HADOOP_CLASSPATH," When the HADOOP_CLASSPATH set from outside and it use ""wildcard characters"" then forloop in hadoop-funtions.sh expand it. This will case ""Argument list too long"" exception for JVM.

Example:

Suppose HADOOP_CLASSPATH value is ""/lib/*:/opt/example.jar""

Now this code 
{code}
    for idx in $(echo ""${HADOOP_CLASSPATH}"" | tr : '\n'); do
      array[${c}]=${idx}
      ((c=c+1))
    done
{code}

it will expand /lib/* directory and add all the jars inside /lib in classpath. This should not append.

CC : [~aw]",surendrasingh
HADOOP-15607,AliyunOSS: fix duplicated partNumber issue in AliyunOSSBlockOutputStream ,"When I generated data with hive-tpcds tool, I got exception below:

2018-07-16 14:50:43,680 INFO mapreduce.Job: Task Id : attempt_1531723399698_0001_m_000052_0, Status : FAILED
 Error: com.aliyun.oss.OSSException: The list of parts was not in ascending order. Parts list must specified in order by part number.
 [ErrorCode]: InvalidPartOrder
 [RequestId]: 5B4C40425FCC208D79D1EAF5
 [HostId]: 100.103.0.137
 [ResponseError]:
 <?xml version=""1.0"" encoding=""UTF-8""?>
 <Error>
 <Code>InvalidPartOrder</Code>
 <Message>The list of parts was not in ascending order. Parts list must specified in order by part number.</Message>
 <RequestId>5B4C40425FCC208D79D1EAF5</RequestId>
 <HostId>xx.xx.xx.xx</HostId>
 <ErrorDetail>current PartNumber 3, you given part number 3is not in ascending order</ErrorDetail>
 </Error>

at com.aliyun.oss.common.utils.ExceptionFactory.createOSSException(ExceptionFactory.java:99)
 at com.aliyun.oss.internal.OSSErrorResponseHandler.handle(OSSErrorResponseHandler.java:69)
 at com.aliyun.oss.common.comm.ServiceClient.handleResponse(ServiceClient.java:248)
 at com.aliyun.oss.common.comm.ServiceClient.sendRequestImpl(ServiceClient.java:130)
 at com.aliyun.oss.common.comm.ServiceClient.sendRequest(ServiceClient.java:68)
 at com.aliyun.oss.internal.OSSOperation.send(OSSOperation.java:94)
 at com.aliyun.oss.internal.OSSOperation.doOperation(OSSOperation.java:149)
 at com.aliyun.oss.internal.OSSOperation.doOperation(OSSOperation.java:113)
 at com.aliyun.oss.internal.OSSMultipartOperation.completeMultipartUpload(OSSMultipartOperation.java:185)
 at com.aliyun.oss.OSSClient.completeMultipartUpload(OSSClient.java:790)
 at org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore.completeMultipartUpload(AliyunOSSFileSystemStore.java:643)
 at org.apache.hadoop.fs.aliyun.oss.AliyunOSSBlockOutputStream.close(AliyunOSSBlockOutputStream.java:120)
 at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
 at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:101)
 at org.apache.hadoop.mapreduce.lib.output.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:106)
 at org.apache.hadoop.mapreduce.lib.output.MultipleOutputs.close(MultipleOutputs.java:574)
 at org.notmysock.tpcds.GenTable$DSDGen.cleanup(GenTable.java:169)
 at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:149)
 at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799)
 at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347)
 at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)
 at java.security.AccessController.doPrivileged(Native Method)
 at javax.security.auth.Subject.doAs(Subject.java:422)
 at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1686)

 

I reviewed code below, 
{code:java}
blockId {code}
has thread synchronization problem
{code:java}
// code placeholder
private void uploadCurrentPart() throws IOException {
  blockFiles.add(blockFile);
  blockStream.flush();
  blockStream.close();
  if (blockId == 0) {
    uploadId = store.getUploadId(key);
  }
  ListenableFuture<PartETag> partETagFuture =
      executorService.submit(() -> {
        PartETag partETag = store.uploadPart(blockFile, key, uploadId,
            blockId + 1);
        return partETag;
      });
  partETagsFutures.add(partETagFuture);
  blockFile = newBlockFile();
  blockId++;
  blockStream = new BufferedOutputStream(new FileOutputStream(blockFile));
}
{code}",wujinhu
HADOOP-15604,Bulk commits of S3A MPUs place needless excessive load on S3 & S3Guard,"When there are ~50 files being committed; each in their own thread from the commit pool; probably the DDB repo is being overloaded just from one single process doing task commit. We should be backing off more, especially given that failing on a write could potentially leave the store inconsistent with the FS (renames, etc)

It would be nice to have some tests to prove that the I/O thresholds are the reason for unprocessed items in DynamoDB metadata store
",gabor.bota
HADOOP-15602,Support SASL Rpc request handling in separate Handlers ,"Right now, during RPC Connection establishment, all SASL requests are considered as OutOfBand requests and handled within the same Reader thread.

SASL handling involves authentication with Kerberos and SecretManagers(for Token validation). During this time, Reader thread would be blocked, hence blocking all the incoming RPC requests on other established connections. Some secretManager impls require to communicate to external systems (ex: ZK) for verification.

SASL RPC handling in separate dedicated handlers, would enable Reader threads to read RPC requests from established connections without blocking.",vinayrpet
HADOOP-15599,Improve TrashPolicyDefault error message,"Found the following warning message:
{noformat}
2018-07-09 06:00:00,486 WARN org.apache.hadoop.fs.TrashPolicyDefault: Unexpected item in trash: /user/hue/.Trash/hue. Ignoring.{noformat}
However, it's not clear what file in the trash directory that was unexpected. 

Relevant code:
{code:java|title=TrashPolicyDefault#deleteCheckpoint}
try {
  time = getTimeFromCheckpoint(name);
} catch (ParseException e) {
  LOG.warn(""Unexpected item in trash: ""+dir+"". Ignoring."");
  continue;
}
{code}
File this Jira to get the file name recorded. ",amihalyi
HADOOP-15598,DataChecksum calculate checksum is contented on hashtable synchronization,"When profiling a multi-threaded hive streaming ingest, observed lock contention on java.util.Properties getProperty() to check if os.arch is ""sparc"". java.util.Properties internally uses HashTable. HashTable.get() is synchronized method. In the test application, on a 30s profile with 64 threads ~40% CPU time is spent on getProperty() contention. See attached snapshot.",prasanth_j
HADOOP-15596,Stack trace should not be printed out when running hadoop key commands,"Stack trace is printed out if any exception occurs while executing hadoop key commands. The whole stack trace should not be printed out.

For example when the kms is down, we get this error message for the hadoop key list command:
{code:java}
 -bash-4.1$ hadoop key list
 Cannot list keys for KeyProvider: KMSClientProvider[http://example.com:16000/kms/v1/]: Connection refusedjava.net.ConnectException: Connection refused
 at java.net.PlainSocketImpl.socketConnect(Native Method)
 at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339)
 at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200)
 at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)
 at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
 at java.net.Socket.connect(Socket.java:579)
 at sun.net.NetworkClient.doConnect(NetworkClient.java:175)
 at sun.net.www.http.HttpClient.openServer(HttpClient.java:432)
 at sun.net.www.http.HttpClient.openServer(HttpClient.java:527)
 at sun.net.www.http.HttpClient.<init>(HttpClient.java:211)
 at sun.net.www.http.HttpClient.New(HttpClient.java:308)
 at sun.net.www.http.HttpClient.New(HttpClient.java:326)
 at sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:996)
 at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:932)
 at sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:850)
 at org.apache.hadoop.security.authentication.client.KerberosAuthenticator.authenticate(KerberosAuthenticator.java:186)
 at org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator.authenticate(DelegationTokenAuthenticator.java:125)
 at org.apache.hadoop.security.authentication.client.AuthenticatedURL.openConnection(AuthenticatedURL.java:216)
 at org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL.openConnection(DelegationTokenAuthenticatedURL.java:312)
 at org.apache.hadoop.crypto.key.kms.KMSClientProvider$1.run(KMSClientProvider.java:397)
 at org.apache.hadoop.crypto.key.kms.KMSClientProvider$1.run(KMSClientProvider.java:392)
 at java.security.AccessController.doPrivileged(Native Method)
 at javax.security.auth.Subject.doAs(Subject.java:415)
 at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1614)
 at org.apache.hadoop.crypto.key.kms.KMSClientProvider.createConnection(KMSClientProvider.java:392)
 at org.apache.hadoop.crypto.key.kms.KMSClientProvider.getKeys(KMSClientProvider.java:479)
 at org.apache.hadoop.crypto.key.KeyShell$ListCommand.execute(KeyShell.java:286)
 at org.apache.hadoop.crypto.key.KeyShell.run(KeyShell.java:79)
 at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
 at org.apache.hadoop.crypto.key.KeyShell.main(KeyShell.java:513)
{code}",knanasi
HADOOP-15595,TestSSLHttpServer fails if the build uses custom -Dhttps.cipherSuites,"When running the build with {{-Dhttps.protocols=TLSv1.2 -Dhttps.cipherSuites=TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256}}, the error is:
{code:java}
[INFO] Running org.apache.hadoop.http.TestSSLHttpServer
[ERROR] Tests run: 5, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 2.122 s <<< FAILURE! - in org.apache.hadoop.http.TestSSLHttpServer
[ERROR] testExcludedCiphers(org.apache.hadoop.http.TestSSLHttpServer)  Time elapsed: 0.01 s  <<< FAILURE!
java.lang.AssertionError: No Ciphers in common, SSLHandshake must fail.
	at org.apache.hadoop.http.TestSSLHttpServer.testExcludedCiphers(TestSSLHttpServer.java:181)

[INFO] 
[INFO] Results:
[INFO] 
[ERROR] Failures: 
[ERROR]   TestSSLHttpServer.testExcludedCiphers:181->Assert.fail:88 No Ciphers in common, SSLHandshake must fail.
{code}",grepas
HADOOP-15594,Exclude commons-lang3 from hadoop-client-minicluster,,tasanuma0829
HADOOP-15593,UserGroupInformation TGT renewer throws NPE,"Found the following NPE thrown in UGI tgt renewer. The NPE was thrown within an exception handler so the original exception was hidden, though it's likely caused by expired tgt.
{noformat}
18/07/02 10:30:57 ERROR util.SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[TGT Renewer for foo@EXAMPLE.COM,5,main]
java.lang.NullPointerException
        at javax.security.auth.kerberos.KerberosTicket.getEndTime(KerberosTicket.java:482)
        at org.apache.hadoop.security.UserGroupInformation$1.run(UserGroupInformation.java:894)
        at java.lang.Thread.run(Thread.java:748){noformat}
Suspect it's related to [https://bugs.openjdk.java.net/browse/JDK-8154889].

The relevant code was added in HADOOP-13590. File this jira to handle the exception better.",gabor.bota
HADOOP-15592,AssumedRoleCredentialProvider to propagate connection settings of S3A FS,"The Assumed Role stuff of HADOOP-15141 doesn't pass down the various timeout options to the STS connection it builds up. That's OK For testing, but not in production, not if things play up (or you want to set a proxy).

Proposed: we will have to use that painful builder API so as to be able to set the aws client config up",stevel@apache.org
HADOOP-15591,KMSClientProvider should log KMS DT acquisition at INFO level,"We can see HDFS and Hive delegation token (DT) creation as INFO messages in Spark application logs but not for KMS DTs:

18/06/07 10:02:35 INFO hdfs.DFSClient: Created token for admin: HDFS_DELEGATION_TOKEN owner=admin@EXAMPLE.NET, renewer=yarn, realUser=, issueDate=1528390955760, maxDate=1528995755760, sequenceNumber=125659, masterKeyId=795 on ha-hdfs:dev
18/06/07 10:02:37 INFO hive.metastore: Trying to connect to metastore with URI thrift://hostnam.example.net:9083
18/06/07 10:02:37 INFO hive.metastore: Opened a connection to metastore, current connections: 1
18/06/07 10:02:37 INFO hive.metastore: Connected to metastore.
18/06/07 10:02:37 INFO security.HiveCredentialProvider: Get Token from hive metastore: Kind: HIVE_DELEGATION_TOKEN, Service: , Ident: 00 1b 61 6e 69 73 68 2d 61 64 6d 69 6e 40 43 4f 52 50 2e 49 4e 54 55 49 54 2e 4e 45 54 04 68 69 76 65 00 8a 01 63 db 33 3a 83 8a 01 63 ff 3f be 83 8e 17 8d 8e 06 96

Please implement KMS DT acquisition events at INFO level as it will help supportability of encrypted HDSF filesystems.",knanasi
HADOOP-15590,Use gpg2 and add set GPG_AGENT_INFO for hadoop release,"When doing 3.0.3 release, by running command

dev-support/bin/create-release --asfrelease --docker --dockercache

documented in

https://wiki.apache.org/hadoop/HowToRelease

I hit the following problems:

1. 
{quote}
starting gpg agent ERROR: Unable to launch or acquire gpg-agent. Disable signing.
{quote}
The script expect GPG_AGENT_INFO env being set with needed info by the gpg-agent. However, it was not. This is because of changes made in gpg-agent. I found the workaround is to add the following line to dev-support/bin/create-release script right after starting gpg-agent:
{quote}
export GPG_AGENT_INFO=""~/.gnupg/S.gpg-agent:$(pgrep gpg-agent):1""
{quote}

2.
{quote}
gpg: can't connect to `~/.gnupg/S.gpg-agent': invalid value
{quote}
I found that this is caused by unmatching gpg-agent and gpg versions installed via Docker. I modified dev-support/docker/Dockerfile to install gnupg2 instead of gnupg. This made gpg and gpg-agent both 2.1.11 instead of one on 2.1.11 the other on 1.14. And this solved the above problem. 



",yzhangal
HADOOP-15586,Fix wrong log statement in AbstractService,"There are some wrong logging statements in AbstractService, here is one example: 

{code:java}
LOG.debug(""noteFailure {}"" + exception);
{code}
",snemeth
HADOOP-15584,move httpcomponents version in pom.xml,"Move httpcomponents version to their own config.

By moving httpcomponent versions in pom.xml to their own variables this will allow for easy overriding",bdscheller
HADOOP-15583,Stabilize S3A Assumed Role support,"started off just on sharing credentials across S3A and S3Guard, but in the process it has grown to becoming one of stabilising the assumed role support so it can be used for more than just testing.

Was: ""S3Guard to get AWS Credential chain from S3AFS; credentials closed() on shutdown""


h3. Issue: lack of auth chain sharing causes ddb and s3 to get out of sync

S3Guard builds its DDB auth chain itself, which stops it having to worry about being created standalone vs part of an S3AFS, but it means its authenticators are in a separate chain.

When you are using short-lived assumed roles or other session credentials updated in the S3A FS authentication chain, you need that same set of credentials picked up by DDB. Otherwise, at best you are doubling load, at worse: the DDB connector may not get refreshed credentials.

Proposed: {{DynamoDBClientFactory.createDynamoDBClient()}} to take an optional ref to aws credentials. If set: don't create a new set. 

There's one little complication here: our {{AWSCredentialProviderList}} list is autocloseable; it's close() will go through all children and close them. Apparently the AWS S3 client (And hopefully the DDB client) will close this when they are closed themselves. If DDB  has the same set of credentials as the FS, then there could be trouble if they are closed in one place when the other still wants to use them.

Solution; have a use count the uses of the credentials list, starting at one: every close() call decrements, and when this hits zero the cleanup is kicked off

h3. Issue: {{AssumedRoleCredentialProvider}} connector to STS not picking up the s3a connection settings, including proxy.

h3. issue: we're not using getPassword() to get user/password for proxy binding for STS. Fix: use that and pass down the bucket ref for per-bucket secrets in a JCEKS file.

h3. Issue; hard to debug what's going wrong :)

h3. Issue: docs about KMS permissions for SSE-KMS are wrong, and the ITestAssumedRole* tests don't request KMS permissions, so fail in a bucket when the base s3 FS is using SSE-KMS. KMS permissions need to be included in generated profiles",stevel@apache.org
HADOOP-15582,Document ABFS,"Add documentation for abfs under {{hadoop-tools/hadoop-azure/src/site/markdown}}

Possible topics include
* intro to scheme
* why abfs (link to MSDN, etc)
* config options
* switching from wasb/interop
* troubleshooting

testing.md should add a section on testing this stuff too.",tmarquardt
HADOOP-15581,Set default jetty log level to INFO in KMS,"During debugging KMS, jetty is printing lots of things at DEBUG/TRACE level. These isn't helpful usually unless someone is debugging the web server part, so we should consider putting an explicit INFO log for them, similar to https://issues.apache.org/jira/browse/HADOOP-14515.",knanasi
HADOOP-15579,ABFS: TestAbfsConfigurationFieldsValidation breaks if FS is configured in core-site,"{{TestAbfsConfigurationFieldsValidation.testConfigServiceImplAnnotatedFieldsInitialized}}

Will fail if you have configured any of
the properties in your abfs defaults/core-defaults imports. It is therefore
(a) not a unit test and (b) brittle",stevel@apache.org
HADOOP-15578,GridmixTestUtils uses the wrong staging directory in windows,{{GridmixTestUtils#createHomeAndStagingDirectory}} gets the staging area from the configuration key {{mapreduce.jobtracker.staging.root.dir}}. This variable depends on {{hadoop.tmp.dir}} which in Windows is set to a local Windows folder. When the test tries to create the path in HDFS it gets an error because the path is not compliant.,elgoiri
HADOOP-15576,S3A  Multipart Uploader to work with S3Guard and encryption,"The new Multipart Uploader API of HDFS-13186 needs to work with S3Guard, with the tests to demonstrate this

# move from low-level calls of S3A client to calls of WriteOperationHelper; adding any new methods are needed there.
# Tests. the tests of HDFS-13713. 
# test execution, with -DS3Guard, -DAuth

There isn't an S3A version of {{AbstractSystemMultipartUploaderTest}}, and even if there was, it might not show that S3Guard was bypassed, because there's no checks that listFiles/listStatus shows the newly committed files.

Similarly, because MPU requests are initiated in S3AMultipartUploader, encryption settings are't picked up. Files being uploaded this way *are not being encrypted*",ehiggs
HADOOP-15574,Suppress build error if there are no docs after excluding private annotations,"Seen in hadoop-ozone when building with the Maven hdds profile enabled.

{noformat}
$ mvn clean install -DskipTests -DskipShade -Phdds -Pdist --projects hadoop-ozone/ozonefs
...
[INFO] --- maven-javadoc-plugin:3.0.0-M1:jar (module-javadocs) @ hadoop-ozone-filesystem ---
[INFO]
ExcludePrivateAnnotationsStandardDoclet
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 13.223 s
[INFO] Finished at: 2018-06-28T19:46:49+09:00
[INFO] Final Memory: 122M/1196M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-javadoc-plugin:3.0.0-M1:jar (module-javadocs) on project hadoop-ozone-filesystem: MavenReportException: Error while generating Javadoc:
[ERROR] Exit code: 1 - Picked up _JAVA_OPTIONS: -Duser.language=en
[ERROR] java.lang.ArrayIndexOutOfBoundsException: 0
[ERROR] 	at com.sun.tools.doclets.formats.html.ConfigurationImpl.setTopFile(ConfigurationImpl.java:537)
[ERROR] 	at com.sun.tools.doclets.formats.html.ConfigurationImpl.setSpecificDocletOptions(ConfigurationImpl.java:309)
[ERROR] 	at com.sun.tools.doclets.internal.toolkit.Configuration.setOptions(Configuration.java:560)
[ERROR] 	at com.sun.tools.doclets.internal.toolkit.AbstractDoclet.startGeneration(AbstractDoclet.java:134)
[ERROR] 	at com.sun.tools.doclets.internal.toolkit.AbstractDoclet.start(AbstractDoclet.java:82)
[ERROR] 	at com.sun.tools.doclets.formats.html.HtmlDoclet.start(HtmlDoclet.java:80)
[ERROR] 	at com.sun.tools.doclets.standard.Standard.start(Standard.java:39)
[ERROR] 	at org.apache.hadoop.classification.tools.ExcludePrivateAnnotationsStandardDoclet.start(ExcludePrivateAnnotationsStandardDoclet.java:41)
[ERROR] 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[ERROR] 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[ERROR] 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[ERROR] 	at java.lang.reflect.Method.invoke(Method.java:498)
[ERROR] 	at com.sun.tools.javadoc.DocletInvoker.invoke(DocletInvoker.java:310)
[ERROR] 	at com.sun.tools.javadoc.DocletInvoker.start(DocletInvoker.java:189)
[ERROR] 	at com.sun.tools.javadoc.Start.parseAndExecute(Start.java:366)
[ERROR] 	at com.sun.tools.javadoc.Start.begin(Start.java:219)
[ERROR] 	at com.sun.tools.javadoc.Start.begin(Start.java:205)
[ERROR] 	at com.sun.tools.javadoc.Main.execute(Main.java:64)
[ERROR] 	at com.sun.tools.javadoc.Main.main(Main.java:54)
{noformat}",tasanuma0829
HADOOP-15572,Test S3Guard ops with assumed roles & verify required permissions,"We haven't documented permissions for S3Guard (WiP of mine); when I try to test using the AssumedRoleCredentialProvider & a role nominally restricted to R/W of S3guard *but not create/delete*, I can still create and destroy buckets

Either I've got my list wrong, or how S3Guard sets up its auth isn't right & somehow falling back to the full role",stevel@apache.org
HADOOP-15571,Multiple FileContexts created with the same configuration object should be allowed to have different umask,"Ran into a super hard-to-debug issue due to this. [Edit: Turns out the same issue as YARN-5749 that [~Tao Yang] ran into]

h4. Issue

Configuration conf = new Configuration();
 fc1 = FileContext.getFileContext(uri1, conf);
 fc2 = FileContext.getFileContext(uri2, conf);
 fc.setUMask(umask_for_fc1); // Screws up umask for fc2 also!

This was not the case before HADOOP-13440.
h4. Symptoms:
h5. Scenario I ran into

When trying to localize a HDFS directory (hdfs:///my/dir/1.txt), NodeManager tries to replicate the directory structure on the local file-system ($yarn-local-dirs/filecache/my/dir/1.txt).

Now depending on whether NM has ever done a log-aggregation (completely unrelated code that sets umask to be 137 for its own files on HDFS), the directories /my and /my/dir on local-fs may have different permissions. In the specific case where NM did log-aggregation, /my/dir was created with 137 umask and so localization of 1.txt completely failed due to absent directory executable permissions!
h5. Previous scenarios:

We ran into this before in test-cases and instead of fixing the root-cause, we just fixed the test-cases: YARN-5679 / YARN-5749",vinodkv
HADOOP-15569,Expand S3A Assumed Role docs,"The  S3A assumed role doc is now where we document the permissions needed to work with buckets

# detail the permissions you need for s3guard user and admin ops
# and what you need for SSE-KMS


This involves me working them out, so presumably get some new stack traces

also: fix any errors noted in the doc",stevel@apache.org
HADOOP-15568,fix some typos in the .sh comments,Fix a few typos in the comments of the shell scripts,stevel@apache.org
HADOOP-15567,Support expiry time in AdlFileSystem,"ADLS supports setting an expiration time for a file.
We can leverage Xattr in FileSystem to set the expiration time.
This could use the same xattr as HDFS-6382 and the interface from HDFS-6525.",huanbang1993
HADOOP-15563,S3guard init and set-capacity to support DDB autoscaling,"To keep costs down on DDB, autoscaling is a key feature: you set the max values and when idle, you don't get billed, *at the cost of delayed scale time and risk of not getting the max value when AWS is busy*

It can be done from the AWS web UI, but not in the s3guard init and set-capacity calls

It can be done [through the API|https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/AutoScaling.HowTo.SDK.html]

Usual issues then: wiring up, CLI params, testing. It'll be hard to test.",gabor.bota
HADOOP-15560,ABFS: removed dependency injection and unnecessary dependencies,"# Removed dependency injection and unnecessary dependencies.
 # Added tool to clean up test containers.",danielzhou
HADOOP-15558,Implementation of Clay Codes plugin (Coupled Layer MSR codes) ,"[Clay Codes|https://www.usenix.org/conference/fast18/presentation/vajha] are new erasure codes developed as a research project at Codes and Signal Design Lab, IISc Bangalore. A particular Clay code, with storage overhead 1.25x, has been shown to reduce repair network traffic, disk read and repair times by factors of 2.9, 3.4 and 3 respectively compared to the RS codes with the same parameters. 

This Jira aims to introduce Clay Codes to HDFS-EC as one of the pluggable erasure codec.",cmukka20
HADOOP-15554,Improve JIT performance for Configuration parsing,"In investigating a performance regression for small tasks between Hadoop 2 and Hadoop 3, we found that the amount of time spent in JIT was significantly higher. Using jitwatch we were able to determine that, due to a combination of switching from DOM to SAX style parsing and just having more configuration key/value pairs, Configuration.loadResource is now getting compiled with the C2 compiler and taking quite some time. Breaking that very large function up into several smaller ones and eliminating some redundant bits of code improves the JIT performance measurably.",tlipcon
HADOOP-15552,Move logging APIs over to slf4j in hadoop-tools - Part2,"Some classes in Hadoop-tools were not moved to slf4j 
e.g. AliyunOSSInputStream.java, HadoopArchiveLogs.java, HadoopArchiveLogsRunner.java",iapicker
HADOOP-15551,Avoid use of Java8 streams in Configuration.addTags,"Configuration.addTags oddly uses Arrays.stream instead of a more conventional mechanism. When profiling a simple program that uses Configuration, I found that addTags was taking tens of millis of CPU to do very little work the first time it's called, accounting for ~8% of total profiler samples in my program.

{code}
[9] 4.52% 253 self: 0.00% 0 java/lang/invoke/MethodHandleNatives.linkCallSite
[9] 3.71% 208 self: 0.00% 0 java/lang/invoke/MethodHandleNatives.linkMethodHandleConstant
{code}

I don't know much about the implementation details of the Streams stuff, but it seems it's probably meant more for cases with very large arrays or somesuch. Switching to a normal Set.addAll() call eliminates this from the profile.",tlipcon
HADOOP-15550,Avoid static initialization of ObjectMappers,"Various classes statically initialize an ObjectMapper READER instance. This ends up doing a bunch of class-loading of Jackson libraries that can add up to a fair amount of CPU, even if the reader ends up not being used. This is particularly the case with WebHdfsFileSystem, which is class-loaded by a serviceloader even when unused in a particular job. We should lazy-init these members instead of doing so as a static class member.",tlipcon
HADOOP-15549,Upgrade to commons-configuration 2.1 regresses task CPU consumption,"HADOOP-13660 upgraded from commons-configuration 1.x to 2.x. commons-configuration is used when parsing the metrics configuration properties file. The new builder API used in the new version apparently makes use of a bunch of very bloated reflection and classloading nonsense to achieve the same goal, and this results in a regression of >100ms of CPU time as measured by a program which simply initializes DefaultMetricsSystem.

This isn't a big deal for long-running daemons, but for MR tasks which might only run a few seconds on poorly-tuned jobs, this can be noticeable.",tlipcon
HADOOP-15548,Randomize local dirs,"shuffle LOCAL_DIRS, LOG_DIRS and LOCAL_USER_DIRS when launching container. Some applications will process these in exactly the same way in every container (e.g. roundrobin) which can cause disks to get unnecessarily overloaded (e.g. one output file written to first entry specified in the environment variable).

There are two paths for local dir allocation, depending on whether the size is unknown or known.  The unknown path already uses a random algorithm.  The known path initializes with a random starting point, and then goes round-robin after that.  When selecting a dir, it increments the last used by one and then checks sequentially until it finds a dir that satisfies the request.  Proposal is to increment by a random value of between 1 and num_dirs - 1, and then check sequentially from there.  This should result in a more random selection in all cases.",jim_brennan
HADOOP-15547,WASB: improve listStatus performance,"The WASB implementation of Filesystem.listStatus is very slow due to O(n!) algorithm to remove duplicates and uses too much memory due to the extra conversion from BlobListItem to FileMetadata to FileStatus.  It takes over 30 minutes to list 700,000 files.  ",tmarquardt
HADOOP-15546,ABFS: tune imports & javadocs; stabilise tests,"Followup on HADOOP-15540 with some initial review tuning

h2. Tuning
* ordering of imports
* rely on azure-auth-keys.xml to store credentials (change imports, docs,.gitignore)
* log4j -> info
* add a ""."" to the first sentence of all the javadocs I noticed.
* remove @Public annotations except for some constants (which includes some commitment to maintain them).
* move the AbstractFS declarations out of the src/test/resources XML file into core-default.xml for all to use
* other IDE-suggested tweaks

h2. Testing

Review the tests, move to ContractTestUtil assertions, make more consistent to contract test setup, and general work to make the tests work well over slower links, document, etc.",tmarquardt
HADOOP-15545,ABFS initialize() throws string out of bounds exception of the URI isn't fully qualified,"if you try to connect to a store like {{abfs://user@something/}} you'll see a StringIndexOutOfBoundsException....better to have something useful like ""not a FQDN""",stevel@apache.org
HADOOP-15544,"ABFS: validate packing, transient classpath, hadoop fs CLI","Validate the packaging and dependencies of ABFS

* hadoop-cloud-storage artifact to export everything needed
* {{hadoop fs -ls abfs://path}} to work in ASF distributions
* check transient CP (e.g spark)

Spark master;s hadoop-cloud module depends on hadoop-cloud-storage if you build with the hadoop-3.1 profile, so it should automatically get in there. Just need to check that it picks it up too",stevel@apache.org
HADOOP-15541,AWS SDK can mistake stream timeouts for EOF and throw SdkClientExceptions,"I've gotten a few reports of read timeouts not being handled properly in some Impala workloads. What happens is the following sequence of events (credit to Sailesh Mukil for figuring this out):
 * S3AInputStream.read() gets a SocketTimeoutException when it calls wrappedStream.read()
 * This is handled by onReadFailure -> reopen -> closeStream. When we try to drain the stream, SdkFilterInputStream.read() in the AWS SDK fails because of checkLength. The underlying Apache Commons stream returns -1 in the case of a timeout, and EOF.
 * The SDK assumes the -1 signifies an EOF, so assumes the bytes read must equal expected bytes, and because they don't (because it's a timeout and not an EOF) it throws an SdkClientException.

This is tricky to test for without a ton of mocking of AWS SDK internals, because you have to get into this conflicting state where the SDK has only read a subset of the expected bytes and gets a -1.

closeStream will abort the stream in the event of an IOException when draining. We could simply also abort in the event of an SdkClientException. I'm testing that this results in correct functionality in the workloads that seem to hit these timeouts a lot, but all the s3a tests continue to work with that change. I'm going to open an issue with the AWS SDK Github as well, but I'm not sure what the ideal outcome would be unless there's a good way to distinguish between a stream that has timed out and a stream that read all the data without huge rewrites.

 

 ",mackrorysd
HADOOP-15540,ABFS: Commit of core codebase,Commit the core code of the ABFS connector (HADOOP-15407) to its development branch,danielzhou
HADOOP-15539,Make start-build-env.sh usable in non-interactive mode,"The current start-build-env.sh in the project root is useful to start a new build environment. But it's not possible to start the build environment and run the command in one step.

We use the dockerized build environment on jenkins (https://builds.apache.org/job/Hadoop-trunk-ozone-acceptance/) which requires a small modification to optionally run start-build-env.sh in non-interactive mode and execute any command in the container.",elek
HADOOP-15538,Possible RPC deadlock in Client,"We have a jstack collection that spans 13 minutes. One frame per ~1.5 minutes. And for each of the frame, I observed the following:
{code:java}
Found one Java-level deadlock:
=============================
""IPC Parameter Sending Thread #294"":
  waiting to lock monitor 0x00007f68f21f3188 (object 0x0000000621745390, a java.lang.Object),
  which is held by UNKNOWN_owner_addr=0x00007f68332e2800

Java stack information for the threads listed above:
===================================================
""IPC Parameter Sending Thread #294"":
        at sun.nio.ch.SocketChannelImpl.ensureWriteOpen(SocketChannelImpl.java:268)
        - waiting to lock <0x0000000621745390> (a java.lang.Object)
        at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:461)
        - locked <0x0000000621745380> (a java.lang.Object)
        at org.apache.hadoop.net.SocketOutputStream$Writer.performIO(SocketOutputStream.java:63)
        at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
        at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:159)
        at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:117)
        at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
        at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)
        - locked <0x0000000621749850> (a java.io.BufferedOutputStream)
        at java.io.DataOutputStream.flush(DataOutputStream.java:123)
        at org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1072)
        - locked <0x000000062174b878> (a java.io.DataOutputStream)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)

Found one Java-level deadlock:
=============================
""IPC Client (297602875) connection to x.y.z.p:8020 from impala"":
  waiting to lock monitor 0x00007f68f21f3188 (object 0x0000000621745390, a java.lang.Object),
  which is held by UNKNOWN_owner_addr=0x00007f68332e2800

Java stack information for the threads listed above:
===================================================
""IPC Client (297602875) connection to x.y.z.p:8020 from impala"":
        at sun.nio.ch.SocketChannelImpl.readerCleanup(SocketChannelImpl.java:279)
        - waiting to lock <0x0000000621745390> (a java.lang.Object)
        at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:390)
        - locked <0x0000000621745370> (a java.lang.Object)
        at org.apache.hadoop.net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)
        at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
        at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
        at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
        at java.io.FilterInputStream.read(FilterInputStream.java:133)
        at java.io.FilterInputStream.read(FilterInputStream.java:133)
        at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:553)
        at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
        at java.io.BufferedInputStream.read(BufferedInputStream.java:265)
        - locked <0x00000006217476f0> (a java.io.BufferedInputStream)
        at java.io.DataInputStream.readInt(DataInputStream.java:387)
        at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1113)
        at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1006)

Found 2 deadlocks.
{code}
This happens with jdk1.8.0_162 on 2.6.32-696.18.7.el6.x86_64.

The code appears to match [https://insight.io/github.com/AdoptOpenJDK/openjdk-jdk8u/tree/dev/jdk/src/share/classes/sun/nio/ch/SocketChannelImpl.java].

The first thread is blocked at:

[https://insight.io/github.com/AdoptOpenJDK/openjdk-jdk8u/blob/dev/jdk/src/share/classes/sun/nio/ch/SocketChannelImpl.java?line=268]

The second thread is blocked at:
 [https://insight.io/github.com/AdoptOpenJDK/openjdk-jdk8u/blob/dev/jdk/src/share/classes/sun/nio/ch/SocketChannelImpl.java?line=279]

There are two issues here:
 # There seems to be a real deadlock because the stacks remain the same even if the first an last jstack frames captured is 13 minutes apart.
 # Java deadlock report seems to be problematic, two threads that have deadlock should not be blocked on the same lock, but they appear to be in this case: the same SocketChannelImpl's stateLock.

I found a relevant jdk jira [https://bugs.openjdk.java.net/browse/JDK-8007476], it explains where two deadlocks are reported and they are really for the same deadlock.

I don't see a similar report about this issue in jdk jira database, and I'm thinking about filing a jdk jira for that, but would like to throw some discussion here before that.

Issue#1 is important, because the client is hanging, which indicate a real problem; however, without a correct report before issue#2 is fixed, it's not clear how the deadlock really looks like.

Thanks.",yzhangal
HADOOP-15537,Clean up ContainerLaunch and ContainerExecutor pre-HADOOP-15528,"Currently ContainerLaunch, ContainerExecutor use deprecated methods, have unused code, and wrong suppressWarnings.
This Jira cleans up the code and removed around 20 warnings from these files.",giovanni.fumarola
HADOOP-15536,Adding support in FileUtil for the creation of directories,Adding support in FileUtil for the creation of directories.,giovanni.fumarola
HADOOP-15533,Make WASB listStatus messages consistent,"- This change make WASB listStatus error messages to be consistent with the rest of the listStatus error messages.
- Inconsistent error messages cause a few WASB tests to fail only in branch-2. The test bug was introduced in ""https://issues.apache.org/jira/browse/HADOOP-15506"". ",esmanii
HADOOP-15532,TestBasicDiskValidator fails with NoSuchFileException,"TestBasicDiskValidator is failing with NoSuchFileException once in a while.
The daily Linux build shows the error [here|https://builds.apache.org/job/hadoop-qbt-trunk-java8-linux-x86/809/testReport/org.apache.hadoop.util/TestBasicDiskValidator/].",giovanni.fumarola
HADOOP-15531,Use commons-text instead of commons-lang in some classes to fix deprecation warnings,"After upgrading commons-lang from 2.6 to 3.7, some classes such as \{{StringEscapeUtils}} and \{{WordUtils}} become deprecated and move to commons-text.",tasanuma0829
HADOOP-15530,RPC could stuck at senderFuture.get(),"In Client.java, sendRpcRequest does the following

{code}
   /** Initiates a rpc call by sending the rpc request to the remote server.
     * Note: this is not called from the Connection thread, but by other
     * threads.
     * @param call - the rpc request
     */
    public void sendRpcRequest(final Call call)
        throws InterruptedException, IOException {
      if (shouldCloseConnection.get()) {
        return;
      }

      // Serialize the call to be sent. This is done from the actual
      // caller thread, rather than the sendParamsExecutor thread,

      // so that if the serialization throws an error, it is reported
      // properly. This also parallelizes the serialization.
      //
      // Format of a call on the wire:
      // 0) Length of rest below (1 + 2)
      // 1) RpcRequestHeader  - is serialized Delimited hence contains length
      // 2) RpcRequest
      //
      // Items '1' and '2' are prepared here. 
      RpcRequestHeaderProto header = ProtoUtil.makeRpcRequestHeader(
          call.rpcKind, OperationProto.RPC_FINAL_PACKET, call.id, call.retry,
          clientId);

      final ResponseBuffer buf = new ResponseBuffer();
      header.writeDelimitedTo(buf);
      RpcWritable.wrap(call.rpcRequest).writeTo(buf);

      synchronized (sendRpcRequestLock) {
        Future<?> senderFuture = sendParamsExecutor.submit(new Runnable() {
          @Override
          public void run() {
            try {
              synchronized (ipcStreams.out) {
                if (shouldCloseConnection.get()) {
                  return;
                }
                if (LOG.isDebugEnabled()) {
                  LOG.debug(getName() + "" sending #"" + call.id
                      + "" "" + call.rpcRequest);
                }
                // RpcRequestHeader + RpcRequest
                ipcStreams.sendRequest(buf.toByteArray());
                ipcStreams.flush();
              }
            } catch (IOException e) {
              // exception at this point would leave the connection in an
              // unrecoverable state (eg half a call left on the wire).
              // So, close the connection, killing any outstanding calls
              markClosed(e);
            } finally {
              //the buffer is just an in-memory buffer, but it is still polite to
              // close early
              IOUtils.closeStream(buf);
            }
          }
        });

        try {
          senderFuture.get();
        } catch (ExecutionException e) {
          Throwable cause = e.getCause();

          // cause should only be a RuntimeException as the Runnable above
          // catches IOException
          if (cause instanceof RuntimeException) {
            throw (RuntimeException) cause;
          } else {
            throw new RuntimeException(""unexpected checked exception"", cause);
          }
        }
      }
    }
{code}

It's observed that the call can be stuck at {{senderFuture.get();}} with the following stack
{code}
""Thread-13"" #40 prio=5 os_prio=0 tid=0x000000000fb0d000 nid=0xf189c waiting on condition [0x00007f697c582000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00000006187e5ec0> (a java.util.concurrent.FutureTask)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.FutureTask.awaitDone(FutureTask.java:429)
        at java.util.concurrent.FutureTask.get(FutureTask.java:191)
        at org.apache.hadoop.ipc.Client$Connection.sendRpcRequest(Client.java:1088)
        - locked <0x00000006215c1e08> (a java.lang.Object)
        at org.apache.hadoop.ipc.Client.call(Client.java:1483)
        at org.apache.hadoop.ipc.Client.call(Client.java:1441)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
        at com.sun.proxy.$Proxy10.getBlockLocations(Unknown Source)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getBlockLocations(ClientNamenodeProtocolTranslatorPB.java:266)
        at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:258)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
        at com.sun.proxy.$Proxy11.getBlockLocations(Unknown Source)
        at org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:1323)
        at org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1310)
        at org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1298)
        at org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:309)
        at org.apache.hadoop.hdfs.DFSInputStream.openInfo(DFSInputStream.java:275)
        - locked <0x00000006187e5530> (a java.lang.Object)
        at org.apache.hadoop.hdfs.DFSInputStream.<init>(DFSInputStream.java:267)
        at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1629)
        at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:338)
        at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:334)
        at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
        at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:334)
{code}

Given that we support rpcTimeOut, we could chose the second method of Future below:
{code}
  /**
     * Waits if necessary for the computation to complete, and then
     * retrieves its result.
     *
     * @return the computed result
     * @throws CancellationException if the computation was cancelled
     * @throws ExecutionException if the computation threw an
     * exception
     * @throws InterruptedException if the current thread was interrupted
     * while waiting
     */
    V get() throws InterruptedException, ExecutionException;

    /**
     * Waits if necessary for at most the given time for the computation
     * to complete, and then retrieves its result, if available.
     *
     * @param timeout the maximum time to wait
     * @param unit the time unit of the timeout argument
     * @return the computed result
     * @throws CancellationException if the computation was cancelled
     * @throws ExecutionException if the computation threw an
     * exception
     * @throws InterruptedException if the current thread was interrupted
     * while waiting
     * @throws TimeoutException if the wait timed out
     */
    V get(long timeout, TimeUnit unit)
        throws InterruptedException, ExecutionException, TimeoutException;
{code}

In theory, since the RPC at client is serialized, we could just use the main thread to do the execution, instead of using a threadpool to create new thread. This can be discussed in a separate jira.

And why the RPC is not processed and returned by NN is another topic (HADOOP-15538).



                                              ",yzhangal
HADOOP-15529,ContainerLaunch#testInvalidEnvVariableSubstitutionType is not supported in Windows,YARN-5219 introduced 2 unit tests designed for Unix. They currently failing in Windows.,giovanni.fumarola
HADOOP-15528,Deprecate ContainerLaunch#link by using FileUtil#SymLink,"{{ContainerLaunch}} currently uses its own utility to create links (including winutils).
This should be deprecated and rely on {{FileUtil#SymLink}} which is already multi-platform and pure Java.",giovanni.fumarola
HADOOP-15527,loop until TIMEOUT before sending kill -9,"I'm seeing that sometimes daemons keep running for a little while even after ""kill -9"" from daemon-stop scripts.

Debugging more, I see several instances of ""ERROR: Unable to kill ${pid}"".

Saw this specifically with ResourceManager & NodeManager -  {{yarn --daemon stop nodemanager}}. Though it is possible that other daemons may run into this too.

Saw this on both Centos as well as Ubuntu.",vinodkv
HADOOP-15525,s3a: clarify / improve support for mixed ACL buckets,"Scenario: customer wants to only give a Hadoop cluster access to a subtree of an S3 bucket.

For example, assume Hadoop uses some IAM identity ""hadoop"", which they wish to grant full permission to everything under the following path:

s3a://bucket/a/b/c/hadoop-dir

they don't want hadoop user to be able to read/list/delete anything outside of the hadoop-dir ""subdir""

Problems: 

To implement the ""directory structure on flat key space"" emulation logic we use to present a Hadoop FS on top of a blob store, we need to create / delete / list ancestors of {{hadoop-dir}}. (to maintain the invariants (1) zero-byte object with key ending in '/' exists iff empty directory is there and (2) files cannot live beneath files, only directories.)

I'd like us to (1) document a an example with IAM ACLs policies that gets this basic functionality, and consider (2) making improvements to make this easier.

We've discussed some of these issues before but I didn't see a dedicated JIRA.",fabbri
HADOOP-15523,Shell command timeout given is in seconds whereas it is taken as millisec while scheduling,"ShellBasedUnixGroupsMapping has a property {{hadoop.security.groups.shell.command.timeout}} to control how long to wait for the fetch groups command which can be configured in seconds. but while scheduling the time taken is millisecs. so currently if u give value as 60s, it is taken as 60ms.

{code:java}
timeout = conf.getTimeDuration(
CommonConfigurationKeys.
HADOOP_SECURITY_GROUP_SHELL_COMMAND_TIMEOUT_SECS,
CommonConfigurationKeys.
HADOOP_SECURITY_GROUP_SHELL_COMMAND_TIMEOUT_SECS_DEFAULT,
TimeUnit.SECONDS);{code}

Time unit given is in seconds but it should be millisecs

",bilwast
HADOOP-15522,Deprecate Shell#ReadLink by using native java code,"Hadoop uses the shell to read symbolic links. Now that Hadoop relies on Java 7+, we can deprecate all the shell code and rely on the Java APIs.",giovanni.fumarola
HADOOP-15521,Upgrading Azure Storage Sdk version to 7.0.0 and updating corresponding code blocks,"Upgraded Azure Storage Sdk to 7.0.0
Fixed code issues and couple of tests
",esmanii
HADOOP-15520,Add tests for various org.apache.hadoop.util classes,"Created new JUnit test classes for the following classes:
 * org.apache.hadoop.util.CloseableReferenceCount
 * org.apache.hadoop.util.IntrusiveCollection
 * org.apache.hadoop.util.LimitInputStream
 * org.apache.hadoop.util.UTF8ByteArrayUtils

Added new JUnit test cases to the following test classes:
 * org.apache.hadoop.util.TestShell
 * org.apache.hadoop.util.TestStringUtils",arashn
HADOOP-15518,Authentication filter calling handler after request already authenticated,"The hadoop-auth AuthenticationFilter will invoke its handler even if a prior successful authentication has occurred in the current request.  This primarily affects situations where multiple authentication mechanism has been configured.  For example when core-site.xml's has hadoop.http.authentication.type=kerberos and yarn-site.xml has yarn.timeline-service.http-authentication.type=kerberos the result is an attempt to perform two Kerberos authentications for the same request.  This in turn results in Kerberos triggering a replay attack detection.  The javadocs for AuthenticationHandler ([https://github.com/apache/hadoop/blob/trunk/hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/server/AuthenticationHandler.java)] indicate for the authenticate method that
{quote}This method is invoked by the AuthenticationFilter only if the HTTP client request is not yet authenticated.
{quote}
This does not appear to be the case in practice.

I've create a patch and tested on a limited number of functional use cases (e.g. the timeline-service issue noted above).  If there is general agreement that the change is valid I'll add unit tests to the patch.

 ",kminder
HADOOP-15516,Add test cases to cover FileUtil#readLink,"Currently, FileUtil#readLink has no unit tests.",giovanni.fumarola
HADOOP-15515,adl.AdlFilesystem.close() doesn't release locks on open files,"If you write to a file on and Azure ADL filesystem and close the file system but not the file before the process exits, the next time you try open the file for append it fails with:

Exception in thread ""main"" java.io.IOException: APPEND failed with error 0x83090a16 (Failed to perform the requested operation because the file is currently open in write mode by another user or process.). [a67c6b32-e78b-4852-9fac-142a3e2ba963][2018-03-22T20:54:08.3520940-07:00]

 The following moves local file to HDFS if it doesn't exist or appends it's contents if it does:

 
{code:java}
public void addFile(String source, String dest, Configuration conf) throws IOException {

FileSystem fileSystem = FileSystem.get(conf);

// Get the filename out of the file path
String filename = source.substring(source.lastIndexOf('/') + 1,source.length());

// Create the destination path including the filename.
if (dest.charAt(dest.length() - 1) != '/')

{ dest = dest + ""/"" + filename; }

else {
dest = dest + filename;
}

// Check if the file already exists
Path path = new Path(dest);
FSDataOutputStream out;
if (fileSystem.exists(path)) {
System.out.println(""File "" + dest + "" already exists appending"");
out = fileSystem.append(path);
} else {
out = fileSystem.create(path);
}

// Create a new file and write data to it.
InputStream in = new BufferedInputStream(new FileInputStream(new File(
source)));

byte[] b = new byte[1024];
int numBytes = 0;
while ((numBytes = in.read(b)) > 0) {
out.write(b, 0, numBytes);
}

// Close the file system not the file
in.close();
//out.close();
fileSystem.close();
}
{code}

 If ""dest"" is an adl:// location, invoking the function a second time (after the process has exited) it raises the error. If it's a regular hdfs:// file system, it doesn't as all the locks are released. The same exception is also raised if a subsequent append is done using: hdfs dfs  -appendToFile.

As I can't see a way to force lease recovery in this situation, this seems like a bug. org.apache.hadoop.fs.adl.AdlFileSystem inherits close() from org.apache.hadoop.fs.FileSystem

[https://hadoop.apache.org/docs/r3.0.0/api/org/apache/hadoop/fs/adl/AdlFileSystem.html]

Which states:

Close this FileSystem instance. Will release any held locks. This does not seem to be the case",vishwajeet.dusane
HADOOP-15514,NoClassDefFoundError for TimelineCollectorManager when starting MiniYARNCluster,"{code:java}
org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.lang.NoClassDefFoundError: org/apache/hadoop/yarn/server/timelineservice/collector/TimelineCollectorManager

  at java.net.URLClassLoader.findClass(URLClassLoader.java:381)

  at java.lang.ClassLoader.loadClass(ClassLoader.java:424)

  at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)

  at java.lang.ClassLoader.loadClass(ClassLoader.java:357)

  at java.lang.ClassLoader.defineClass1(Native Method)

  at java.lang.ClassLoader.defineClass(ClassLoader.java:763)

  at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)

  at java.net.URLClassLoader.defineClass(URLClassLoader.java:467)

  at java.net.URLClassLoader.access$100(URLClassLoader.java:73)

  at java.net.URLClassLoader$1.run(URLClassLoader.java:368)

  at java.net.URLClassLoader$1.run(URLClassLoader.java:362)

  at java.security.AccessController.doPrivileged(Native Method)

  at java.net.URLClassLoader.findClass(URLClassLoader.java:361)

  at java.lang.ClassLoader.loadClass(ClassLoader.java:424)

  at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)

  at java.lang.ClassLoader.loadClass(ClassLoader.java:357)

  at java.lang.Class.getDeclaredMethods0(Native Method)

  at java.lang.Class.privateGetDeclaredMethods(Class.java:2701)

  at java.lang.Class.getDeclaredMethods(Class.java:1975){code}
 ",rohithsharma
HADOOP-15513,Add additional test cases to cover some corner cases for FileUtil#symlink,Add additional test cases to cover some corner cases for FileUtil#symlink.,giovanni.fumarola
HADOOP-15512,clean up Shell from JDK7 workarounds,"there's some comments in {{Shell}} about JDK7 specific issues (especially {{runCommand()}}. These workarounds don't matter, so can be purged",zvenczel
HADOOP-15511,ITestS3GuardListConsistency#testRollingRenames bad parameters passed to doTestRenameSequence,"Bumped into this while working on HADOOP-15423.

Currently the parameters passed to doTestRenameSequence are the following:

{noformat}
mkdirs: s3a://cloudera-dev-gabor-ireland/test/rolling/2
srcdirs: s3a://cloudera-dev-gabor-ireland/test/rolling/2
dstdirs: s3a://cloudera-dev-gabor-ireland/test/rolling/3
yesdirs: s3a://cloudera-dev-gabor-ireland/test/rolling/3
nodirs: s3a://cloudera-dev-gabor-ireland/test/rolling/1

mkdirs: {}
srcdirs: s3a://cloudera-dev-gabor-ireland/test/rolling/3
dstdirs: s3a://cloudera-dev-gabor-ireland/test/rolling/1
yesdirs: s3a://cloudera-dev-gabor-ireland/test/rolling/1
nodirs: s3a://cloudera-dev-gabor-ireland/test/rolling/2

mkdirs: {}
srcdirs: s3a://cloudera-dev-gabor-ireland/test/rolling/1
dstdirs: s3a://cloudera-dev-gabor-ireland/test/rolling/2
yesdirs: s3a://cloudera-dev-gabor-ireland/test/rolling/2
nodirs: s3a://cloudera-dev-gabor-ireland/test/rolling/3

mkdirs: {}
srcdirs: s3a://cloudera-dev-gabor-ireland/test/rolling/2
dstdirs: s3a://cloudera-dev-gabor-ireland/test/rolling/3
yesdirs: s3a://cloudera-dev-gabor-ireland/test/rolling/3
nodirs: s3a://cloudera-dev-gabor-ireland/test/rolling/1

mkdirs: {}
srcdirs: s3a://cloudera-dev-gabor-ireland/test/rolling/3
dstdirs: s3a://cloudera-dev-gabor-ireland/test/rolling/1
yesdirs: s3a://cloudera-dev-gabor-ireland/test/rolling/1
nodirs: s3a://cloudera-dev-gabor-ireland/test/rolling/2

mkdirs: {}
srcdirs: s3a://cloudera-dev-gabor-ireland/test/rolling/1
dstdirs: s3a://cloudera-dev-gabor-ireland/test/rolling/2
yesdirs: s3a://cloudera-dev-gabor-ireland/test/rolling/2
nodirs: s3a://cloudera-dev-gabor-ireland/test/rolling/3
{noformat}

The problem is that we should check that the srcdir is moved to dstdirs, so it should be check in nodirs. Right now nodirs parameter checks for directories which are completely unrelated to the cases, so this part of the test should be fixed to check for dirs that are related to the cases. Basically the nodirs should be equal to srcdirs in this case.",gabor.bota
HADOOP-15509,Release Hadoop 2.7.7,Time to get a new Hadoop 2.7.x out the door.,stevel@apache.org
HADOOP-15507,Add MapReduce counters about EC bytes read,"HDFS has added Erasure Coding support in HDFS-7285. There are HDFS level [ReadStatistics|https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/ReadStatistics.java] so from DFSClient we can know how much reads are EC/replication.

In order for users to have a better view of how much of their workload is impacted by EC, we can expose EC read bytes to File System Counters, and to MapReduce's job counters. This way, end users can tell from MR jobs directly.",xiaochen
HADOOP-15506,Upgrade Azure Storage Sdk version to 7.0.0 and update corresponding code blocks,"- Upgraded Azure Storage Sdk to 7.0.0
- Fixed code issues and couple of tests
",esmanii
HADOOP-15504,Upgrade Maven and Maven Wagon versions,"I'm not even sure that Hadoop's combination of the relevant dependencies is vulnerable (even if they are, this is a relatively minor vulnerability), but this is at least showing up as an issue in automated vulnerability scans. Details can be found here [https://maven.apache.org/security.html] (CVE-2013-0253, CVE-2012-6153). Essentially the combination of maven 3.0.4 (we use 3.0, and I guess that maps to 3.0.4?) and older versions of wagon plugin don't use SSL properly (note that we neither use the WebDAV provider nor a 2.x version of the SSH plugin, which is why I suspect that the vulnerability does not affect Hadoop).

I know some dependencies can be especially troublesome to upgrade - I suspect that Maven's critical role in our build might make this risky - so if anyone has ideas for how to more completely test this than a full build, please chime in,",mackrorysd
HADOOP-15499,Performance severe drop when running RawErasureCoderBenchmark with NativeRSRawErasureCoder,"Run RawErasureCoderBenchmark  which is a micro-benchmark to test EC codec encoding/decoding performance. 

50 concurrency Native ISA-L coder has the less throughput than 1 concurrency Native ISA-L case. It's abnormal. 

 

bin/hadoop jar ./share/hadoop/common/hadoop-common-3.2.0-SNAPSHOT-tests.jar org.apache.hadoop.io.erasurecode.rawcoder.RawErasureCoderBenchmark encode 3 1 1024 1024
Using 126MB buffer.
ISA-L coder encode 1008MB data, with chunk size 1024KB
Total time: 0.19 s.
Total throughput: 5390.37 MB/s
Threads statistics:
1 threads in total.
Min: 0.18 s, Max: 0.18 s, Avg: 0.18 s, 90th Percentile: 0.18 s.

 

bin/hadoop jar ./share/hadoop/common/hadoop-common-3.2.0-SNAPSHOT-tests.jar org.apache.hadoop.io.erasurecode.rawcoder.RawErasureCoderBenchmark encode 3 50 1024 10240
Using 120MB buffer.
ISA-L coder encode 54000MB data, with chunk size 10240KB
Total time: 11.58 s.
Total throughput: 4662 MB/s
Threads statistics:
50 threads in total.
Min: 0.55 s, Max: 11.5 s, Avg: 6.32 s, 90th Percentile: 10.45 s.

 

RawErasureCoderBenchmark shares a single coder between all concurrent threads. While 

NativeRSRawEncoder and NativeRSRawDecoder has synchronized key work on doDecode and doEncode function. So 50 concurrent threads are forced to use the shared coder encode/decode function one by one. 

 

To resolve the issue, there are two approaches. 
 # Refactor RawErasureCoderBenchmark  to use dedicated coder for each concurrent thread.
 # Refactor NativeRSRawEncoder  and NativeRSRawDecoder  to get better concurrency.  Since the synchronized key work is to try to protect the private variable nativeCoder from being checked in doEncode/doDecode and  being modified in release.  We can use reentrantReadWriteLock to increase the concurrency since doEncode/doDecode can be called multiple times without change the nativeCoder state.

 I prefer approach 2 and will upload a patch later. 

 

 

 

 ",sammi
HADOOP-15498,"TestHadoopArchiveLogs (#testGenerateScript, #testPrepareWorkingDir) fails on Windows","[TestHadoopArchiveLogs#testGenerateScript|https://builds.apache.org/job/hadoop-trunk-win/479/testReport/org.apache.hadoop.tools/TestHadoopArchiveLogs/testGenerateScript/] fails on Windows because script generation uses ""\n"", but description uses System.lineSeparator().

[TestHadoopArchiveLogs#testPrepareWorkingDir|https://builds.apache.org/job/hadoop-trunk-win/479/testReport/org.apache.hadoop.tools/TestHadoopArchiveLogs/testPrepareWorkingDir/] fails on Windows because according to [sticky bit wiki|https://en.wikipedia.org/wiki/Sticky_bit], sticky bit is for Unix-like system.",huanbang1993
HADOOP-15497,TestTrash should use proper test path to avoid failing on Windows,"The following fail on Windows due to improper path:
* [TestHDFSTrash#testNonDefaultFS|https://builds.apache.org/job/hadoop-trunk-win/478/testReport/org.apache.hadoop.hdfs/TestHDFSTrash/testNonDefaultFS/]
* [TestTrash|https://builds.apache.org/job/hadoop-trunk-win/478/testReport/org.apache.hadoop.fs/TestTrash/]
",huanbang1993
HADOOP-15496,TestFsShellList#testList fails on Windows,"[TestFsShellList#testList|https://builds.apache.org/job/hadoop-trunk-win/478/testReport/org.apache.hadoop.fs/TestFsShellList/testList/] fails on Windows because Windows filename does not accept ""\"", while in the test

{code:java}
createFile(new Path(testRootDir, ""abc\bd\tef""));
...
createFile(new Path(testRootDir, ""qq\r123""));
{code}
",huanbang1993
HADOOP-15495,Upgrade common-lang version to 3.7 in hadoop-common-project and hadoop-tools,"commons-lang 2.6 is widely used. Let's upgrade to 3.6.

This jira is separated from HADOOP-10783.",tasanuma0829
HADOOP-15494,TestRawLocalFileSystemContract fails on Windows,"[hadoop-trunk-win #476 TestRawLocalFileSystemContract|https://builds.apache.org/job/hadoop-trunk-win/476/testReport/org.apache.hadoop.fs/TestRawLocalFileSystemContract/] shows following similar errors for 21 tests:
{color:#d04437}java.net.URISyntaxException: Relative path in absolute URI: file://target/test/data/testMultiByteFilesAreFiles{color}",huanbang1993
HADOOP-15493,DiskChecker should handle disk full situation,"DiskChecker#checkDirWithDiskIo creates a file to verify that the disk is writable.

However check should not fail when file creation fails due to disk being full. This avoids marking full disks as _failed_.

Reported by [~kihwal] and [~daryn] in HADOOP-15450. ",arpitagarwal
HADOOP-15490,Multiple declaration of maven-enforcer-plugin found in pom.xml,"Multiple declaration of {{maven-enforcer-plugin}} in {{pom.xml}} is causing the below warning during build.
{noformat}
[WARNING] Some problems were encountered while building the effective model for org.apache.hadoop:hadoop-project:pom:3.2.0-SNAPSHOT
[WARNING] 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.apache.maven.plugins:maven-enforcer-plugin @ org.apache.hadoop:hadoop-main:3.2.0-SNAPSHOT, /Users/nvadivelu/codebase/apache/hadoop/pom.xml, line 431, column 15
[WARNING]
[WARNING] Some problems were encountered while building the effective model for org.apache.hadoop:hadoop-main:pom:3.2.0-SNAPSHOT
[WARNING] 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.apache.maven.plugins:maven-enforcer-plugin @ line 431, column 15
[WARNING]
[WARNING] It is highly recommended to fix these problems because they threaten the stability of your build.
[WARNING]
[WARNING] For this reason, future Maven versions might no longer support building such malformed projects.
{noformat}",nandakumar131
HADOOP-15489,S3Guard to self update on directory listings of S3,"S3Guard updates its table on a getFileStatus call, but not on a directory listing.

While this makes directory listings faster (no need to push out an update), it slows down subsequent queries of the files, such as a sequence of:

{code}
statuses = s3a.listFiles(dir)
for (status: statuses) {
  if (status.isFile) {
      try(is = s3a.open(status.getPath())) {
        ... do something
      }
}
{code}

this is because the open() is doing the getFileStatus check, even after the listing.

Updating the DDB tables after a listing would give those reads a speedup, albeit at the expense of initiating a (bulk) update in the list call. Of course, we could consider making that async, though that design (essentially a write-buffer) would require the buffer to be checked in the reads too. ",gabor.bota
HADOOP-15486,Make NetworkTopology#netLock fair,"Whenever a datanode is restarted, the registration call after the restart received by NameNode lands in {{NetworkTopology#add}} via {{DatanodeManager#registerDatanode}} requires write lock on {{NetworkTopology#netLock}}.

This registration thread is getting starved by flood of {{FSNamesystem.getAdditionalDatanode}} calls, which are triggered by clients those who were writing to the restarted datanode.

The registration call which is waiting for write lock on {{NetworkTopology#netLock}} is holding write lock on {{FSNamesystem#fsLock}}, causing all the other RPC calls which require the lock on {{FSNamesystem#fsLock}} wait.
We can make {{NetworkTopology#netLock}} lock fair so that the registration thread will not starve.",nandakumar131
HADOOP-15484,Upgrade moment.js to version 2.22.1,This Jira aims to upgrade moment.js to version 2.22.1.,ljain
HADOOP-15483,Upgrade jquery to version 3.3.1,This Jira aims to upgrade jquery to version 3.3.1.,ljain
HADOOP-15482,Upgrade jackson-databind to version 2.9.5,This Jira aims to upgrade jackson-databind to version 2.9.5,ljain
HADOOP-15480,AbstractS3GuardToolTestBase.testDiffCommand fails when using dynamo,"When running org.apache.hadoop.fs.s3a.s3guard.ITestS3GuardToolDynamoDB, the testDiffCommand test fails with the following:
{noformat}
testDiffCommand(org.apache.hadoop.fs.s3a.s3guard.ITestS3GuardToolDynamoDB)  Time elapsed: 8.059 s  <<< FAILURE!
java.lang.AssertionError: 
Mismatched metadata store outputs: MS	D	0	s3a://cloudera-dev-gabor-ireland/test/test-diff/ms_only
MS	F	100	s3a://cloudera-dev-gabor-ireland/test/test-diff/ms_only/file-0
MS	F	100	s3a://cloudera-dev-gabor-ireland/test/test-diff/ms_only/file-1
MS	F	100	s3a://cloudera-dev-gabor-ireland/test/test-diff/ms_only/file-3
MS	F	100	s3a://cloudera-dev-gabor-ireland/test/test-diff/ms_only/file-2
MS	F	100	s3a://cloudera-dev-gabor-ireland/test/test-diff/ms_only/file-4
S3	F	0	s3a://cloudera-dev-gabor-ireland/test/test-diff/s3_only/file-1
MS	F	0	s3a://cloudera-dev-gabor-ireland/test/test-diff/s3_only/file-1
S3	F	0	s3a://cloudera-dev-gabor-ireland/test/test-diff/s3_only/file-0
MS	F	0	s3a://cloudera-dev-gabor-ireland/test/test-diff/s3_only/file-0
S3	F	0	s3a://cloudera-dev-gabor-ireland/test/test-diff/s3_only/file-2
MS	F	0	s3a://cloudera-dev-gabor-ireland/test/test-diff/s3_only/file-2
S3	F	0	s3a://cloudera-dev-gabor-ireland/test/test-diff/s3_only/file-3
MS	F	0	s3a://cloudera-dev-gabor-ireland/test/test-diff/s3_only/file-3
S3	F	0	s3a://cloudera-dev-gabor-ireland/test/test-diff/s3_only/file-4
MS	F	0	s3a://cloudera-dev-gabor-ireland/test/test-diff/s3_only/file-4
 expected:<[
s3a://cloudera-dev-gabor-ireland/test/test-diff/ms_only, 
s3a://cloudera-dev-gabor-ireland/test/test-diff/ms_only/file-0, 
s3a://cloudera-dev-gabor-ireland/test/test-diff/ms_only/file-1, 
s3a://cloudera-dev-gabor-ireland/test/test-diff/ms_only/file-3, 
s3a://cloudera-dev-gabor-ireland/test/test-diff/ms_only/file-2, 
s3a://cloudera-dev-gabor-ireland/test/test-diff/ms_only/file-4]> 

but was:<[
s3a://cloudera-dev-gabor-ireland/test/test-diff/ms_only, 
s3a://cloudera-dev-gabor-ireland/test/test-diff/s3_only/file-1, 
s3a://cloudera-dev-gabor-ireland/test/test-diff/ms_only/file-0, 
s3a://cloudera-dev-gabor-ireland/test/test-diff/s3_only/file-0, 
s3a://cloudera-dev-gabor-ireland/test/test-diff/ms_only/file-1, 
s3a://cloudera-dev-gabor-ireland/test/test-diff/ms_only/file-3, 
s3a://cloudera-dev-gabor-ireland/test/test-diff/ms_only/file-2, 
s3a://cloudera-dev-gabor-ireland/test/test-diff/s3_only/file-2, 
s3a://cloudera-dev-gabor-ireland/test/test-diff/s3_only/file-3, 
s3a://cloudera-dev-gabor-ireland/test/test-diff/ms_only/file-4, 
s3a://cloudera-dev-gabor-ireland/test/test-diff/s3_only/file-4]>
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.failNotEquals(Assert.java:743)
	at org.junit.Assert.assertEquals(Assert.java:118)
	at org.apache.hadoop.fs.s3a.s3guard.AbstractS3GuardToolTestBase.testDiffCommand(AbstractS3GuardToolTestBase.java:382)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
{noformat}",gabor.bota
HADOOP-15478,WASB: hflush() and hsync() regression,"HADOOP-14520 introduced a regression in hflush() and hsync().  Previously, for the default case where users upload data as block blobs, these were no-ops.  Unfortunately, HADOOP-14520 accidentally implemented hflush() and hsync() by default, so any data buffered in the stream is immediately uploaded to storage.  This new behavior is undesirable, because block blobs have a limit of 50,000 blocks.  Spark users are now seeing failures due to exceeding the block limit, since Spark frequently invokes hflush().",tmarquardt
HADOOP-15477,Make unjar in RunJar overrideable,"Currently Hadoop's RunJar will unjar the jar provided and look for any jars inside and add them to the classpath. Since most deployments doesn't use jar in jar, but rather uberjars this could be rather time consuming at times and can cause issues related to over consumption of inodes, for something that is in many cases is not used.

For that purpose there should be an env variable to disable this behavior.

 

Edit: As requested by [~ajisakaa] in person here is a more detailed description of the issues we are trying to solve with this.

A good chunk of our workloads are packaged in an uberjar, and are launched as a separate process using the {{hadoop jar}} cli. This is has generally been working out pretty well historically, with sub second launch times and good client isolation. Since bumping the host OS to a version patched with Meltdown/Specter patches we do see from time to time load becoming very high even with only a few client processes running and a single unjar process taking up to 10min. 

While another simple approach would be to abandon using the {{hadoop jar}} cli this would most likely take a lot more work than simply disabling unjar for the time being.",johang
HADOOP-15476,fix logging for split-dns multihome ,Fix debug log statement introduced in [HADOOP-15250].,ajayydv
HADOOP-15475,Fix broken unit tests on Windows,"There are hundreds of unit tests that fail on Windows. This JIRA tracks the effort to fix them.

The main reasons for unit test failures on Windows are:
* Windows/Linux path formats (e.g., HDFS-10256).
* Line separator.
* Locked files: Windows locks files when opening them.
** The typical trigger is not cleaning MiniDFSCluster leaves files locked when a test times out; they need to be cleaned using After.
* Memory lock size.
* Slow DNS resolution (e.g., HDFS-13569).
* Locked ports (e.g., HDFS-11700)",huanbang1993
HADOOP-15474,Rename properties introduced for <tags>,"HADOOP-15007 introduces the following two properties for tagging configuration properties
* hadoop.system.tags
* hadoop.custom.tags

This sounds like {{tags}} fall under {{hadoop.system}} and {{hadoop.custom}} related properties, but what we really want to achieve here is to have two sub-division of {{tags}} namely {{system}} and {{custom}}

For better readability, we can rename them as
* hadoop.tags.system
* hadoop.tags.custom

",zvenczel
HADOOP-15473,Configure serialFilter in KeyProvider to avoid UnrecoverableKeyException caused by JDK-8189997,"There is a new feature in JDK 8u171 called Enhanced KeyStore Mechanisms (http://www.oracle.com/technetwork/java/javase/8u171-relnotes-4308888.html#JDK-8189997).
This is the cause of the following errors in the TestKeyProviderFactory:

{noformat}
Caused by: java.security.UnrecoverableKeyException: Rejected by the jceks.key.serialFilter or jdk.serialFilter property
	at com.sun.crypto.provider.KeyProtector.unseal(KeyProtector.java:352)
	at com.sun.crypto.provider.JceKeyStore.engineGetKey(JceKeyStore.java:136)
	at java.security.KeyStore.getKey(KeyStore.java:1023)
	at org.apache.hadoop.crypto.key.JavaKeyStoreProvider.getMetadata(JavaKeyStoreProvider.java:410)
	... 28 more
{noformat}

This issue causes errors and failures in hbase tests right now (using hdfs) and could affect other products running on this new Java version.",gabor.bota
HADOOP-15472,Fix NPE in DefaultUpgradeComponentsFinder ,"In current upgrades for Yarn native services, we do not support addition/deletion of compoents during upgrade. On trying to upgrade with the same number of components in target spec as the current service spec but with the one of the components having a new target spec and name, see the following NPE in service AM logs

{noformat}
2018-05-15 00:10:41,489 [IPC Server handler 0 on 37488] ERROR service.ClientAMService - Error while trying to upgrade service {} 
java.lang.NullPointerException
	at org.apache.hadoop.yarn.service.UpgradeComponentsFinder$DefaultUpgradeComponentsFinder.lambda$findTargetComponentSpecs$0(UpgradeComponentsFinder.java:103)
	at java.util.ArrayList.forEach(ArrayList.java:1257)
	at org.apache.hadoop.yarn.service.UpgradeComponentsFinder$DefaultUpgradeComponentsFinder.findTargetComponentSpecs(UpgradeComponentsFinder.java:100)
	at org.apache.hadoop.yarn.service.ServiceManager.processUpgradeRequest(ServiceManager.java:259)
	at org.apache.hadoop.yarn.service.ClientAMService.upgrade(ClientAMService.java:163)
	at org.apache.hadoop.yarn.service.impl.pb.service.ClientAMProtocolPBServiceImpl.upgradeService(ClientAMProtocolPBServiceImpl.java:81)
	at org.apache.hadoop.yarn.proto.ClientAMProtocol$ClientAMProtocolService$2.callBlockingMethod(ClientAMProtocol.java:5972)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1682)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
{noformat}",suma.shivaprasad
HADOOP-15471,Hdfs recursive listing operation is very slow,"The hdfs dfs -ls -R command is sequential in nature and is very slow for a HCFS system. We have seen around 6 mins for 40K directory/files structure.

The proposal is to use multithreading approach to speed up recursive list, du and count operations.

We have tried a ForkJoinPool implementation to improve performance for recursive listing operation.

[https://github.com/jasoncwik/hadoop-release/tree/parallel-fs-cli]

commit id : 

82387c8cd76c2e2761bd7f651122f83d45ae8876

Another implementation is to use Java Executor Service to improve performance to run listing operation in multiple threads in parallel. This has significantly reduced the time to 40 secs from 6 mins.

 

 ",ajaysachdev
HADOOP-15469,S3A directory committer commit job fails if _temporary directory created under dest,"The directory staging committer fails in commit job if any temporary files/dirs have been created. Spark work can create such a dir for placement of absolute files.

This is because commitJob() looks for the dest dir existing, not containing non-hidden files.
As the comment says, ""its kind of superfluous"". More specifically, it means jobs which would commit with the classic committer & overwrite=false will fail

Proposed fix: remove the check",stevel@apache.org
HADOOP-15467,"TestDoAsEffectiveUser#testRealUserSetup,TestDoAsEffectiveUser#testRealUserAuthorizationSuccess time out on Windows","{color:#d04437}[INFO] Running org.apache.hadoop.security.TestDoAsEffectiveUser{color}
{color:#d04437}[ERROR] Tests run: 2, Failures: 0, Errors: 2, Skipped: 0, Time elapsed: 8.307 s <<< FAILURE! - in org.apache.hadoop.security.TestDoAsEffectiveUser{color}
{color:#d04437}[ERROR] testRealUserSetup(org.apache.hadoop.security.TestDoAsEffectiveUser) Time elapsed: 4.107 s <<< ERROR!{color}
{color:#d04437}java.lang.Exception: test timed out after 4000 milliseconds{color}
{color:#d04437} at java.net.Inet4AddressImpl.getHostByAddr(Native Method){color}
{color:#d04437} at java.net.InetAddress$2.getHostByAddr(InetAddress.java:932){color}
{color:#d04437} at java.net.InetAddress.getHostFromNameService(InetAddress.java:617){color}
{color:#d04437} at java.net.InetAddress.getCanonicalHostName(InetAddress.java:588){color}
{color:#d04437} at org.apache.hadoop.security.TestDoAsEffectiveUser.configureSuperUserIPAddresses(TestDoAsEffectiveUser.java:103){color}
{color:#d04437} at org.apache.hadoop.security.TestDoAsEffectiveUser.testRealUserSetup(TestDoAsEffectiveUser.java:188){color}
{color:#d04437} at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method){color}
{color:#d04437} at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62){color}
{color:#d04437} at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43){color}
{color:#d04437} at java.lang.reflect.Method.invoke(Method.java:498){color}
{color:#d04437} at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47){color}
{color:#d04437} at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12){color}
{color:#d04437} at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44){color}
{color:#d04437} at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17){color}
{color:#d04437} at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74){color}

{color:#d04437}[ERROR] testRealUserAuthorizationSuccess(org.apache.hadoop.security.TestDoAsEffectiveUser) Time elapsed: 4.002 s <<< ERROR!{color}
{color:#d04437}java.lang.Exception: test timed out after 4000 milliseconds{color}
{color:#d04437} at java.net.Inet4AddressImpl.getHostByAddr(Native Method){color}
{color:#d04437} at java.net.InetAddress$2.getHostByAddr(InetAddress.java:932){color}
{color:#d04437} at java.net.InetAddress.getHostFromNameService(InetAddress.java:617){color}
{color:#d04437} at java.net.InetAddress.getCanonicalHostName(InetAddress.java:588){color}
{color:#d04437} at org.apache.hadoop.security.TestDoAsEffectiveUser.configureSuperUserIPAddresses(TestDoAsEffectiveUser.java:103){color}
{color:#d04437} at org.apache.hadoop.security.TestDoAsEffectiveUser.testRealUserAuthorizationSuccess(TestDoAsEffectiveUser.java:218){color}
{color:#d04437} at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method){color}
{color:#d04437} at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62){color}
{color:#d04437} at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43){color}
{color:#d04437} at java.lang.reflect.Method.invoke(Method.java:498){color}
{color:#d04437} at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47){color}
{color:#d04437} at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12){color}
{color:#d04437} at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44){color}
{color:#d04437} at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17){color}
{color:#d04437} at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74){color}

{color:#d04437}[INFO]{color}
{color:#d04437}[INFO] Results:{color}
{color:#d04437}[INFO]{color}
{color:#d04437}[ERROR] Errors:{color}
{color:#d04437}[ERROR] TestDoAsEffectiveUser.testRealUserAuthorizationSuccess:218->configureSuperUserIPAddresses:103 ╗{color}
{color:#d04437}[ERROR] TestDoAsEffectiveUser.testRealUserSetup:188->configureSuperUserIPAddresses:103 ╗{color}
{color:#d04437}[INFO]{color}
{color:#d04437}[ERROR] Tests run: 2, Failures: 0, Errors: 2, Skipped: 0{color}",huanbang1993
HADOOP-15466,Correct units in adl.http.timeout,"Comment in core-default.xml says seconds, but according to the SDK docs it's getting interpreted as milliseconds ([https://github.com/Azure/azure-data-lake-store-java/blob/master/src/main/java/com/microsoft/azure/datalake/store/ADLStoreOptions.java#L139-L144).] Pinging [~ASikaria] to double check I'm not missing anything.",mackrorysd
HADOOP-15465,Deprecate WinUtils#Symlinks by using native java code,"Hadoop uses the shell to create symbolic links. Now that Hadoop relies on Java 7+, we can deprecate all the shell code and rely on the Java APIs.",giovanni.fumarola
HADOOP-15461,Improvements over the Hadoop support with Windows,"This Jira tracks the effort to improve the interaction between Hadoop and Windows Server.
 * Move away from an external process (winutils.exe) for native code:
 ** Replace by native Java APIs (e.g., symlinks);
 ** Replace by something like JNI or so;
 * Fix the build system to fully leverage cmake instead of msbuild;
 * Possible other improvements;
 * Memory and handle leaks.

 

I did a quick investigation of the performance of WinUtils in YARN. In average NM calls 4.76 times per second and 65.51 per container.

 
| |Requests|Requests/sec|Requests/min|Requests/container|
|*Sum [WinUtils]*|*135354*|*4.761*|*286.160*|*65.51*|
|[WinUtils] Execute -help|4148|0.145|8.769|2.007|
|[WinUtils] Execute -ls|2842|0.0999|6.008|1.37|
|[WinUtils] Execute -systeminfo|9153|0.321|19.35|4.43|
|[WinUtils] Execute -symlink|115096|4.048|243.33|57.37|
|[WinUtils] Execute -task isAlive|4115|0.144|8.699|2.05|

 Interval: 7 hours, 53 minutes and 48 seconds

Each execution of WinUtils does around *140 IO ops*, of which 130 are DDL ops.

This means *666.58* IO ops/second due to WinUtils.

We should start considering to remove WinUtils from Hadoop and creating a JNI interface.",giovanni.fumarola
HADOOP-15459,KMSACLs will fail for other optype if acls is defined for one optype.,"Assume subset of kms-acls xml file.
{noformat}
  <property>
    <name>default.key.acl.DECRYPT_EEK</name>
    <value></value>
    <description>
      default ACL for DECRYPT_EEK operations for all key acls that are not
      explicitly defined.
    </description>
  </property>

<configuration>
  <property>
    <name>key.acl.key1.DECRYPT_EEK</name>
    <value>user1</value>
  </property>

  <property>
    <name>default.key.acl.READ</name>
    <value>*</value>
    <description>
      default ACL for READ operations for all key acls that are not
      explicitly defined.
    </description>
  </property>

<property>
  <name>whitelist.key.acl.READ</name>
  <value>hdfs</value>
  <description>
    Whitelist ACL for READ operations for all keys.
  </description>
</property>
{noformat}
For key {{key1}}, we restricted {{DECRYPT_EEK}} operation to only {{user1}}.
 For other {{READ}} operation(like getMetadata), by default I still want everyone to access all keys via {{default.key.acl.READ}}
 But it doesn't allow anyone to access {{key1}} for any other READ operations.
 As a result of this, if the admin restricted access for one opType then (s)he has to define access for all other opTypes also, which is not desirable.",shahrs87
HADOOP-15458,TestLocalFileSystem#testFSOutputStreamBuilder fails on Windows,"In *org.apache.hadoop.fs.TestLocalFileSystem#testFSOutputStreamBuilder* a FSDataOutputStream object is unnecessarily created and not closed, which makes org.apache.hadoop.fs.TestLocalFileSystem#after fails to delete the folder on Windows.

 ",surmountian
HADOOP-15457,Add Security-Related HTTP Response Header in WEBUIs.,"As of today, YARN web-ui lacks certain security related http response headers. We are planning to add few default ones and also add support for headers to be able to get added via xml config. Planning to make the below two as default.
 * X-XSS-Protection: 1; mode=block
 * X-Content-Type-Options: nosniff

 

Support for headers via config properties in core-site.xml will be along the below lines
{code:java}
<property>
     <name>hadoop.http.header.Strict_Transport_Security</name>
     <value>valHSTSFromXML</value>
 </property>{code}
 

A regex matcher will lift these properties and add into the response header when Jetty prepares the response.",kanwaljeets
HADOOP-15456,create base image for running secure ozone cluster,Create docker image to run secure ozone cluster.,ajayydv
HADOOP-15455,Incorrect debug message in KMSACL#hasAccess,"If the user is in the blacklist ""foo bar"", it prints ""user is not in foo bar"".
else, it prints ""user is in foo bar""

{code:title=KMSACLs#hasAccess()}
if (access) {
      AccessControlList blacklist = blacklistedAcls.get(type);
      access = (blacklist == null) || !blacklist.isUserInList(ugi);
      if (LOG.isDebugEnabled()) {
        if (blacklist == null) {
          LOG.debug(""No blacklist for {}"", type.toString());
        } else if (access) {
          LOG.debug(""user is in {}"" , blacklist.getAclString());
        } else {
          LOG.debug(""user is not in {}"" , blacklist.getAclString());
        }
      }
    }
{code}",study
HADOOP-15454,TestRollingFileSystemSinkWithLocal fails on Windows,"org.apache.hadoop.metrics2.sink.TestRollingFileSystemSinkWithLocal fails on Windows,

*Error message:*

Illegal character in opaque part at index 2: D:\_work\8\s\hadoop-common-project\hadoop-common\target\test\data\4\RollingFileSystemSinkTest\testSilentExistingWrite

*Stack trace:*

java.io.IOException: All datanodes [DatanodeInfoWithStorage[127.0.0.1:53235,DS-4d6119ac-31cc-4f48-8a5b-7f35b36a1c55,DISK]] are bad. Aborting... at org.apache.hadoop.hdfs.DataStreamer.handleBadDatanode(DataStreamer.java:1538) at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1472) at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1244) at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:663)

Drive letter in absolute path in Windows will fail the underlying URI check.

Another issue is that for the failed write test case java.io.File#setWritable(boolean, boolean) is used, which does not work as expected on Windows and should be replaced by org.apache.hadoop.fs.FileUtil#setWritable.",surmountian
HADOOP-15452,Snappy Decpmpressor met ArrayIndexOutOfBoundsException when reduce task fetch map output data,"RT, when reducers tasks fetch  data from mapper tasks, it met ArrayIndexOutOfBoundsException, here is stackTrace:
{code:java}
org.apache.hadoop.mapred.YarnChild: Exception running child : org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in fetcher#1
	at org.apache.hadoop.mapreduce.task.reduce.Shuffle.run(Shuffle.java:134)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:379)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:165)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:160)
Caused by: java.lang.ArrayIndexOutOfBoundsException
	at org.apache.hadoop.io.compress.snappy.SnappyDecompressor.setInput(SnappyDecompressor.java:111)
	at org.apache.hadoop.io.compress.BlockDecompressorStream.decompress(BlockDecompressorStream.java:104)
	at org.apache.hadoop.io.compress.DecompressorStream.read(DecompressorStream.java:85)
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:199)
	at org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput.shuffle(InMemoryMapOutput.java:98)
	at org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyMapOutput(Fetcher.java:549)
	at org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyFromHost(Fetcher.java:346)
	at org.apache.hadoop.mapreduce.task.reduce.Fetcher.run(Fetcher.java:202)
{code}
 

Anyone has ideas?",zvenczel
HADOOP-15450,Avoid fsync storm triggered by DiskChecker and handle disk full situation,"Fix disk checker issues reported by [~kihwal] in HADOOP-13738

There are non-hdfs users of DiskChecker, who use it proactively, not just on failures. This was fine before, but now it incurs heavy I/O due to introduction of fsync() in the code.",arpitagarwal
HADOOP-15449,Increase default timeout of ZK session to avoid frequent NameNode failover,"We observed from several users regarding Namenode flip-over is due to either zookeeper disk slowness (higher fsync cost) or network issue. We would need to avoid flip-over issue to some extent by increasing HA session timeout, ha.zookeeper.session-timeout.ms.

Default value is 5000 ms, seems very low in any production environment.  I would suggest 10000 ms as default session timeout.

 

{code}

..

2018-05-04 03:54:36,848 INFO  zookeeper.ClientCnxn (ClientCnxn.java:run(1140)) - Client session timed out, have not heard from server in 4689ms for sessionid 0x260e24bac500aa3, closing socket connection and attempting reconnect 
2018-05-04 03:56:49,088 INFO  zookeeper.ClientCnxn (ClientCnxn.java:run(1140)) - Client session timed out, have not heard from server in 3981ms for sessionid 0x360fd152b8700fe, closing socket connection and attempting reconnect

.. 

{code}",kpalanisamy
HADOOP-15446,WASB: PageBlobInputStream.skip breaks HBASE replication,"Page Blobs are primarily used by HBASE.  HBASE replication, which apparently has not been used with WASB until recently, performs non-sequential reads on log files using PageBlobInputStream.  There are bugs in this stream implementation which prevent skip and seek from working properly, and eventually the stream state becomes corrupt and unusable.

I believe this bug affects all releases of WASB/HADOOP.  It appears to be a day-0 bug in PageBlobInputStream.  There were similar bugs opened in the past (HADOOP-15042) but the issue was not properly fixed, and no test coverage was added.",tmarquardt
HADOOP-15444,ITestS3GuardToolDynamo should only run with -Ddynamo ,"If you run S3A integration tests with just {{-Ds3guard}} and not {{-Ddynamo}}, then I do not think that ITestS3GuardToolDynamo should run.",fabbri
HADOOP-15443,hadoop shell should allow non-privileged user to start secure daemons.,"With [HDFS-13081] now secure Datanode can be started without root privileges if rpc port is protected via sasl and ssl is enabled for http. However hadoop shell still has check for privilged user in hadoop-functions.sh. Jira intends to amend it, at-least for hdfs.",ajayydv
HADOOP-15442,ITestS3AMetrics.testMetricsRegister can't know metrics source's name,"I've seen this test fail a bunch lately - mainly when the tests are all run (i.e. not individually) but not in parallel, it seems. If you dump out the sources when it fails, you see:

* The sources are numbered in the hundreds, so it's very unlikely that this actually gets the first one.
* The sources are numbered twice. There was logic to have the first one not be numbered, but that got messed up and now all sources are numbered twice, but the first one is only number once.

We could just remove the bad assertion, but then we're only testing the registry and not anything else about the way metrics flow all the way through the whole system. Worth it to fix the failing test, I think - knowing the source gets registered doesn't add a whole lot of value toward end-to-end metrics testing.",mackrorysd
HADOOP-15441,Log kms url and token service at debug level.,"It looks like after HADOOP-14987, any encryption zone operations prints extra INFO log messages as follows:
{code:java}
$ hdfs dfs -copyFromLocal /etc/krb5.conf /scale/
18/05/02 11:54:55 INFO kms.KMSClientProvider: KMSClientProvider for KMS url: https://hadoop3-1.example.com:16000/kms/v1/ delegation token service: kms://https@hadoop3-1.example.com:16000/kms created.
{code}

It might make sense to make it a DEBUG message instead.",gabor.bota
HADOOP-15438,AzureBlobFS - Tests,AzureBlobFS functional and contract tests,esmanii
HADOOP-15437,AzureBlobFS - Services,AzureBlobFS services and factories in the driver.,esmanii
HADOOP-15436,AzureBlobFS - Diagnostics and Utils,AzureBlobFS Diagnostics and Utils classes,esmanii
HADOOP-15435,AzureBlobFS - Constants,AzureBlobFS constants used across the driver.,esmanii
HADOOP-15434,Upgrade to ADLS SDK that exposes current timeout,"HADOOP-15356 aims to expose the ADLS SDK base timeout as a configurable option in Hadoop, but this isn't very testable without being able to read it back after the write logic has been applied.",mackrorysd
HADOOP-15433,AzureBlobFS - Contracts,"All the internal, external contracts for the AzureBlobFS driver.

Contracts include:
- Configuration annotations
- Configuration validation contract
- Custom exceptions
- Service contracts",esmanii
HADOOP-15432,AzureBlobFS - Base package classes and configuration files,"Patch contains:
- AzureBlobFileSystem and SecureAzureBlobFileSystem classes which are the main interfaces Hadoop interacts with.
- Updated Azure pom.xml with updated dependencies, updated parallel tests configurations and maven shader plugin.
- Checkstyle suppression file. Since http layer is generated automatically by another libraries, it will not follow hadoop coding guidelines. Therefore a few rules for checkstyles have been disabled.
- Added test configuration file template to be used by the consumers. Similar to wasb, all the configurations will go into this file.",esmanii
HADOOP-15431,KMSTokenRenewer should work with KMS_DELEGATION_TOKEN which has ip:port as service,"Seen a test failure where a MR job failed to submit.
RM log has:
{noformat}
2018-04-30 15:00:17,864 WARN org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer: Unable to add the application to the delegation token renewer.
java.lang.IllegalArgumentException: Invalid token service IP_ADDR:16000
        at org.apache.hadoop.util.KMSUtil.createKeyProviderFromTokenService(KMSUtil.java:237)
        at org.apache.hadoop.crypto.key.kms.KMSTokenRenewer.createKeyProvider(KMSTokenRenewer.java:100)
        at org.apache.hadoop.crypto.key.kms.KMSTokenRenewer.renew(KMSTokenRenewer.java:57)
        at org.apache.hadoop.security.token.Token.renew(Token.java:414)
        at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$1.run(DelegationTokenRenewer.java:590)
        at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$1.run(DelegationTokenRenewer.java:587)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1920)
        at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.renewToken(DelegationTokenRenewer.java:585)
        at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.handleAppSubmitEvent(DelegationTokenRenewer.java:463)
        at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.access$800(DelegationTokenRenewer.java:79)
        at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$DelegationTokenRenewerRunnable.handleDTRenewerAppSubmitEvent(DelegationTokenRenewer.java:894)
        at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$DelegationTokenRenewerRunnable.run(DelegationTokenRenewer.java:871)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
{noformat}

while client log has

{noformat}
18/04/30 15:53:28 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1525128478242_0001
18/04/30 15:53:28 INFO mapreduce.JobSubmitter: Kind: HDFS_DELEGATION_TOKEN, Service: ha-hdfs:ns1, Ident: (token for systest: HDFS_DELEGATION_TOKEN owner=systest@EXAMPLE.COM, renewer=yarn, realUser=, issueDate=1525128807236, maxDate=1525733607236, sequenceNumber=1038, masterKeyId=20)
18/04/30 15:53:28 INFO mapreduce.JobSubmitter: Kind: HBASE_AUTH_TOKEN, Service: 621a942b-292f-493d-ba50-f9b783704359, Ident: (org.apache.hadoop.hbase.security.token.AuthenticationTokenIdentifier@0)
18/04/30 15:53:28 INFO mapreduce.JobSubmitter: Kind: KMS_DELEGATION_TOKEN, Service: IP_ADDR:16000, Ident: 00 07 73 79 73 74 65 73 74 04 79 61 72 6e 00 8a 01 63 18 c2 c3 d5 8a 01 63 3c cf 47 d5 8e 01 ec 10
18/04/30 15:53:29 INFO mapreduce.JobSubmitter: Cleaning up the staging area /user/systest/.staging/job_1525128478242_0001
18/04/30 15:53:29 WARN security.UserGroupInformation: PriviledgedActionException as:systest@EXAMPLE.COM (auth:KERBEROS) cause:java.io.IOException: org.apache.hadoop.yarn.exceptions.YarnException: Failed to submit application_1525128478242_0001 to YARN : Invalid token service IP_ADDR:16000
18/04/30 15:53:29 INFO client.ConnectionManager$HConnectionImplementation: Closing master protocol: MasterService
18/04/30 15:53:29 INFO client.ConnectionManager$HConnectionImplementation: Closing zookeeper sessionid=0x1630ba2d0001cb5
18/04/30 15:53:29 INFO zookeeper.ZooKeeper: Session: 0x1630ba2d0001cb5 closed
18/04/30 15:53:29 INFO zookeeper.ClientCnxn: EventThread shut down
18/04/30 15:53:29 ERROR util.AbstractHBaseTool: Error running command-line tool
java.io.IOException: org.apache.hadoop.yarn.exceptions.YarnException: Failed to submit application_1525128478242_0001 to YARN : Invalid token service IP_ADDR:16000
	at org.apache.hadoop.mapred.YARNRunner.submitJob(YARNRunner.java:336)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:244)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1307)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1304)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1920)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1304)
	at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1325)
	at org.apache.hadoop.hbase.mapreduce.IntegrationTestBulkLoad.runLinkedListMRJob(IntegrationTestBulkLoad.java:298)
	at org.apache.hadoop.hbase.mapreduce.IntegrationTestBulkLoad.runLoad(IntegrationTestBulkLoad.java:225)
	at org.apache.hadoop.hbase.mapreduce.IntegrationTestBulkLoad.testBulkLoad(IntegrationTestBulkLoad.java:215)
	at org.apache.hadoop.hbase.mapreduce.IntegrationTestBulkLoad.runTestFromCommandLine(IntegrationTestBulkLoad.java:767)
	at org.apache.hadoop.hbase.IntegrationTestBase.doWork(IntegrationTestBase.java:123)
	at org.apache.hadoop.hbase.util.AbstractHBaseTool.run(AbstractHBaseTool.java:112)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
	at com.cloudera.itest.hbase.smoke.TestBulkLoad.testBulkLoad(TestBulkLoad.java:46)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:367)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:274)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:161)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:290)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:242)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:121)
Caused by: org.apache.hadoop.yarn.exceptions.YarnException: Failed to submit application_1525128478242_0001 to YARN : Invalid token service IP_ADDR:16000
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.submitApplication(YarnClientImpl.java:257)
	at org.apache.hadoop.mapred.ResourceMgrDelegate.submitApplication(ResourceMgrDelegate.java:290)
	at org.apache.hadoop.mapred.YARNRunner.submitJob(YARNRunner.java:320)
	... 41 more
{noformat}

Further debugging shows that this fall into the category of:
Server + renewer with HADOOP-14445, submitter without HADOOP-14445.
Unfortunately, [my testing steps in HADOOP-14445|https://issues.apache.org/jira/browse/HADOOP-14445?focusedCommentId=16426501&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16426501] did not cover this scenario.",xiaochen
HADOOP-15430,hadoop fs -mkdir -p path-ending-with-slash/ fails with s3guard,"if you call {{hadoop fs -mkdir -p path/}} on the command line with a path ending in ""/:. you get a DDB error ""An AttributeValue may not contain an empty string""",stevel@apache.org
HADOOP-15428,s3guard bucket-info will create s3guard table if FS is set to do this automatically,"If you call hadoop s3guard bucket-info on a bucket where the fs is set to create a s3guard table on demand, then the DDB table is automatically created. As a result

the {{bucket-info -unguarded}} option cannot be used, and the call has significant side effects (i.e. it can run up bills)",gabor.bota
HADOOP-15426,Make S3guard client resilient to DDB throttle events and network failures,"managed to create on a parallel test run
{code}
org.apache.hadoop.fs.s3a.AWSServiceThrottledException: delete on s3a://hwdev-steve-ireland-new/fork-0005/test/existing-dir/existing-file: com.amazonaws.services.dynamodbv2.model.ProvisionedThroughputExceededException: The level of configured provisioned throughput for the table was exceeded. Consider increasing your provisioning level with the UpdateTable API. (Service: AmazonDynamoDBv2; Status Code: 400; Error Code: ProvisionedThroughputExceededException; Request ID: RDM3370REDBBJQ0SLCLOFC8G43VV4KQNSO5AEMVJF66Q9ASUAAJG): The level of configured provisioned throughput for the table was exceeded. Consider increasing your provisioning level with the UpdateTable API. (Service: AmazonDynamoDBv2; Status Code: 400; Error Code: ProvisionedThroughputExceededException; Request ID: RDM3370REDBBJQ0SLCLOFC8G43VV4KQNSO5AEMVJF66Q9ASUAAJG)
	at 

{code}

We should be able to handle this. 400 ""bad things happened"" error though, not the 503 from S3.

h3. We need a retry handler for DDB throttle operations",stevel@apache.org
HADOOP-15423,Merge fileCache and dirCache into one single cache in LocalMetadataStore,"Right now the s3guard.LocalMetadataStore uses two HashMap in the implementation - one for the file and one for the dir hash.
{code:java}
  /** Contains directories and files. */
  private Cache<Path, PathMetadata> fileCache;

  /** Contains directory listings. */
  private Cache<Path, DirListingMetadata> dirCache;
{code}

It would be nice to have only one hash instead of these two for storing the values. An idea for the implementation would be to have a class with nullable fields:

{code:java}
  static class LocalMetaEntry {
    @Nullable
    public PathMetadata pathMetadata;
    @Nullable
    public DirListingMetadata dirListingMetadata;
  }
{code}

or a Pair (tuple):

{code:java}
Pair<PathMetadata, DirListingMetadata> metaEntry;
{code}

And only one hash/cache for these elements.",gabor.bota
HADOOP-15420,s3guard ITestS3GuardToolLocal failures in diff tests,"Noticed this when testing the patch for HADOOP-13756.

 
{code:java}
[ERROR] Failures:

[ERROR]   ITestS3GuardToolLocal>AbstractS3GuardToolTestBase.testPruneCommandCLI:221->AbstractS3GuardToolTestBase.testPruneCommand:201->AbstractS3GuardToolTestBase.assertMetastoreListingCount:214->Assert.assertEquals:555->Assert.assertEquals:118->Assert.failNotEquals:743->Assert.fail:88 Pruned children count [PathMetadata{fileStatus=S3AFileStatus{path=s3a://bucket-new/test/testPruneCommandCLI/stale; isDirectory=false; length=100; replication=1; blocksize=512; modification_time=1524798258286; access_time=0; owner=hdfs; group=hdfs; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false} isEmptyDirectory=FALSE; isEmptyDirectory=UNKNOWN; isDeleted=false}, PathMetadata{fileStatus=S3AFileStatus{path=s3a://bucket-new/test/testPruneCommandCLI/fresh; isDirectory=false; length=100; replication=1; blocksize=512; modification_time=1524798262583; access_time=0; owner=hdfs; group=hdfs; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false} isEmptyDirectory=FALSE; isEmptyDirectory=UNKNOWN; isDeleted=false}] expected:<1> but was:<2>{code}
 

Looking through the code, I'm noticing a couple of issues.

 

1. {{testDiffCommand()}} is in {{ITestS3GuardToolLocal}}, but it should really be running for all MetadataStore implementations.  Seems like it should live in {{AbstractS3GuardToolTestBase}}.

2. {{AbstractS3GuardToolTestBase#createFile()}} seems wrong. When {{onMetadataStore}} is false, it does a {{ContractTestUtils.touch(file)}}, but the fs is initialized with a MetadataStore present, so seem like the fs will still put the file in the MetadataStore?

There are other tests which explicitly go around the MetadataStore by using {{fs.setMetadataStore(nullMS)}}, e.g. ITestS3AInconsistency. We should do something similar in {{AbstractS3GuardToolTestBase#createFile()}}, minding any issues with parallel test runs.",gabor.bota
HADOOP-15418,Hadoop KMSAuthenticationFilter needs to use getPropsByPrefix instead of iterator to avoid ConcurrentModificationException,The issue is similar to what was fixed in HADOOP-15411. Fixing this in KMSAuthenticationFilter as well.,suma.shivaprasad
HADOOP-15416,s3guard diff assert failure if source path not found,Got an illegal argument exception trying to do a s3guard diff in a test run. Underlying cause: directory in supplied s3a path didn't exist,gabor.bota
HADOOP-15415,copyBytes hangs when the configuration file is corrupted,"The third parameter,  buffSize, is read from the configuration files or user-specified.

When the configuration file is corrupted or the user configures with a wrong value, i.e., 0, the bytesRead will always be 0, making the while loop's condition always true, hanging IOUtils.copyBytes() endlessly.

Here is the snippet of the code. There are four copyBytes in the following code. The 3rd and 4th copyBytes calls the 1st one. The 1st one calls the 2nd one. Hang happens in the while loop of the second copyBytes function.

 
{code:java}
//1st copyBytes
  public static void copyBytes(InputStream in, OutputStream out, int buffSize, boolean close) 
    throws IOException {
    try {
      copyBytes(in, out, buffSize);
      if(close) {
        out.close();
        out = null;
        in.close();
        in = null;
      }
    } finally {
      if(close) {
        closeStream(out);
        closeStream(in);
      }
    }
  }
  
//2nd copyBytes
  public static void copyBytes(InputStream in, OutputStream out, int buffSize) 
    throws IOException {
    PrintStream ps = out instanceof PrintStream ? (PrintStream)out : null;
    byte buf[] = new byte[buffSize];
    int bytesRead = in.read(buf);
    while (bytesRead >= 0) {
      out.write(buf, 0, bytesRead);
      if ((ps != null) && ps.checkError()) {
        throw new IOException(""Unable to write to output stream."");
      }
      bytesRead = in.read(buf);
    }
  }

//3rd copyBytes
  public static void copyBytes(InputStream in, OutputStream out, Configuration conf)
    throws IOException {
    copyBytes(in, out, conf.getInt(""io.file.buffer.size"", 4096), true);
  }
  
//4th copyBytes
  public static void copyBytes(InputStream in, OutputStream out, Configuration conf, boolean close)
    throws IOException {
    copyBytes(in, out, conf.getInt(""io.file.buffer.size"", 4096),  close);
  }
{code}
 ",linyiqun
HADOOP-15411,AuthenticationFilter should use Configuration.getPropsWithPrefix instead of iterator,"Node manager start up fails with the following stack trace

{code}
2018-04-19 13:08:30,638 ERROR nodemanager.NodeManager (NodeManager.java:initAndStartNodeManager(921)) - Error starting NodeManager
org.apache.hadoop.yarn.exceptions.YarnRuntimeException: NMWebapps failed to start.
 at org.apache.hadoop.yarn.server.nodemanager.webapp.WebServer.serviceStart(WebServer.java:117)
 at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)
 at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:121)
 at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)
 at org.apache.hadoop.yarn.server.nodemanager.NodeManager.initAndStartNodeManager(NodeManager.java:919)
 at org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:979)
Caused by: org.apache.hadoop.yarn.webapp.WebAppException: Error starting http server
 at org.apache.hadoop.yarn.webapp.WebApps$Builder.build(WebApps.java:377)
 at org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:424)
 at org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:420)
 at org.apache.hadoop.yarn.server.nodemanager.webapp.WebServer.serviceStart(WebServer.java:112)
 ... 5 more
Caused by: java.io.IOException: java.util.ConcurrentModificationException
 at org.apache.hadoop.http.HttpServer2.<init>(HttpServer2.java:532)
 at org.apache.hadoop.http.HttpServer2.<init>(HttpServer2.java:117)
 at org.apache.hadoop.http.HttpServer2$Builder.build(HttpServer2.java:421)
 at org.apache.hadoop.yarn.webapp.WebApps$Builder.build(WebApps.java:333)
 ... 8 more
Caused by: java.util.ConcurrentModificationException
 at java.util.Hashtable$Enumerator.next(Hashtable.java:1383)
 at org.apache.hadoop.conf.Configuration.iterator(Configuration.java:2853)
 at org.apache.hadoop.security.AuthenticationFilterInitializer.getFilterConfigMap(AuthenticationFilterInitializer.java:73)
 at org.apache.hadoop.http.HttpServer2.getFilterProperties(HttpServer2.java:647)
 at org.apache.hadoop.http.HttpServer2.constructSecretProvider(HttpServer2.java:637)
 at org.apache.hadoop.http.HttpServer2.<init>(HttpServer2.java:525)
 ... 11 more
2018-04-19 13:08:30,639 INFO timeline.HadoopTimelineMetricsSink (AbstractTimelineMetricsSink.java:getCurrentCollectorHost(291)) - No live collector to send metrics to. Metrics to be sent will be discarded. This message will be skipped for the next 20 times.
{code}",suma.shivaprasad
HADOOP-15409,S3AFileSystem.verifyBucketExists to move to s3.doesBucketExistV2,"in S3AFileSystem.initialize(), we check for the bucket existing with verifyBucketExists(), which calls s3.doesBucketExist(). But that doesn't check for auth issues. 

s3. doesBucketExistV2() does at least validate credentials, and should be switched to. This will help things fail faster 

See SPARK-24000",jack-lee
HADOOP-15408,HADOOP-14445 broke Spark.,"Spark bundles hadoop related jars in their package.
 Spark expects backwards compatibility between minor versions.
 Their job failed after we deployed HADOOP-14445 in our test cluster.
{noformat}
2018-04-20 21:09:53,245 INFO [main] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: Executing with tokens:
2018-04-20 21:09:53,273 ERROR [main] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: Error starting MRAppMaster
java.util.ServiceConfigurationError: org.apache.hadoop.security.token.TokenIdentifier: Provider org.apache.hadoop.crypto.key.kms.KMSDelegationToken$
KMSLegacyDelegationTokenIdentifier could not be instantiated
at java.util.ServiceLoader.fail(ServiceLoader.java:232)
at java.util.ServiceLoader.access$100(ServiceLoader.java:185)
at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:384)
at java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:404)
at java.util.ServiceLoader$1.next(ServiceLoader.java:480)
at org.apache.hadoop.security.token.Token.getClassForIdentifier(Token.java:117)
at org.apache.hadoop.security.token.Token.decodeIdentifier(Token.java:138)
at org.apache.hadoop.security.token.Token.identifierToString(Token.java:393)
at org.apache.hadoop.security.token.Token.toString(Token.java:413)
at java.lang.String.valueOf(String.java:2994)
at org.apache.commons.logging.impl.SLF4JLocationAwareLog.info(SLF4JLocationAwareLog.java:155)
at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.initAndStartAppMaster(MRAppMaster.java:1634)
at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.main(MRAppMaster.java:1583)

Caused by: java.lang.NoSuchFieldError: TOKEN_LEGACY_KIND
at org.apache.hadoop.crypto.key.kms.KMSDelegationToken$KMSLegacyDelegationTokenIdentifier.<init>(KMSDelegationToken.java:64)
at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
at java.lang.Class.newInstance(Class.java:442)
at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:380)
... 10 more
2018-04-20 21:09:53,278 INFO [main] org.apache.hadoop.util.ExitUtil: Exiting with status 1
{noformat}
Their classpath looks like {{\{...:hadoop-common-pre-HADOOP-14445.jar:.....:hadoop-common-with-HADOOP-14445.jar:....\}}}

This is because the container loaded {{KMSDelegationToken}} class from an older jar and {{KMSLegacyDelegationTokenIdentifier}} from new jar and it fails when {{KMSLegacyDelegationTokenIdentifier}} wants to read {{TOKEN_LEGACY_KIND}} from {{KMSDelegationToken}} which doesn't exist before.
 Cc [~xiaochen]",shahrs87
HADOOP-15407,Support Windows Azure Storage - Blob file system in Hadoop,"*{color:#212121}Description{color}*
 This JIRA adds a new file system implementation, ABFS, for running Big Data and Analytics workloads against Azure Storage. This is a complete rewrite of the previous WASB driver with a heavy focus on optimizing both performance and cost.
 {color:#212121} {color}
 *{color:#212121}High level design{color}*
 At a high level, the code here extends the FileSystem class to provide an implementation for accessing blobs in Azure Storage. The scheme abfs is used for accessing it over HTTP, and abfss for accessing over HTTPS. The following URI scheme is used to address individual paths:
 {color:#212121} {color}
 {color:#212121}abfs[s]://<filesystem>@<account>.dfs.core.windows.net/<path>{color}
 {color:#212121} {color}
 {color:#212121}ABFS is intended as a replacement to WASB. WASB is not deprecated but is in pure maintenance mode and customers should upgrade to ABFS once it hits General Availability later in CY18.{color}
 {color:#212121}Benefits of ABFS include:{color}
 {color:#212121}·         Higher scale (capacity, throughput, and IOPS) Big Data and Analytics workloads by allowing higher limits on storage accounts{color}
 {color:#212121}·         Removing any ramp up time with Storage backend partitioning; blocks are now automatically sharded across partitions in the Storage backend{color}
{color:#212121}          .         This avoids the need for using temporary/intermediate files, increasing the cost (and framework complexity around committing jobs/tasks){color}
 {color:#212121}·         Enabling much higher read and write throughput on single files (tens of Gbps by default){color}
 {color:#212121}·         Still retaining all of the Azure Blob features customers are familiar with and expect, and gaining the benefits of future Blob features as well{color}
 {color:#212121}ABFS incorporates Hadoop Filesystem metrics to monitor the file system throughput and operations. Ambari metrics are not currently implemented for ABFS, but will be available soon.{color}
 {color:#212121} {color}
 *{color:#212121}Credits and history{color}*
 Credit for this work goes to (hope I don't forget anyone): Shane Mainali, {color:#212121}Thomas Marquardt, Zichen Sun, Georgi Chalakov, Esfandiar Manii, Amit Singh, Dana Kaban, Da Zhou, Junhua Gu, Saher Ahwal, Saurabh Pant, and James Baker. {color}
 {color:#212121} {color}
 *Test*
 ABFS has gone through many test procedures including Hadoop file system contract tests, unit testing, functional testing, and manual testing. All the Junit tests provided with the driver are capable of running in both sequential/parallel fashion in order to reduce the testing time.
 {color:#212121}Besides unit tests, we have used ABFS as the default file system in Azure HDInsight. Azure HDInsight will very soon offer ABFS as a storage option. (HDFS is also used but not as default file system.) Various different customer and test workloads have been run against clusters with such configurations for quite some time. Benchmarks such as Tera*, TPC-DS, Spark Streaming and Spark SQL, and others have been run to do scenario, performance, and functional testing. Third parties and customers have also done various testing of ABFS.{color}
 {color:#212121}The current version reflects to the version of the code tested and used in our production environment.{color}",danielzhou
HADOOP-15406,hadoop-nfs dependencies for mockito and junit are not test scope,hadoop-nfs asks for mockito-all and junit for its unit tests but it does not mark the dependency as being required only for tests.,jlowe
HADOOP-15404,Remove multibyte characters in DataNodeUsageReportUtil,"DataNodeUsageReportUtil created by HDFS-13055 includes multibyte characters. We need to remove them for building it with java9.
{noformat}
mvn javadoc:javadoc --projects hadoop-hdfs-project/hadoop-hdfs-client
...
[ERROR] /hadoop/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/server/protocol/DataNodeUsageReportUtil.java:26: error: unmappable character (0xE2) for encoding US-ASCII
[ERROR]  * the delta between??????current DataNode usage metrics and the usage metrics
{noformat}",tasanuma0829
HADOOP-15402,Prevent double logout of UGI's LoginContext,HADOOP-15294 worked around a LoginContext NPE resulting from a double logout by peering into the Subject.  A cleaner fix is tracking whether the LoginContext is logged in.,daryn
HADOOP-15400,Improve S3Guard documentation on Authoritative Mode implementation,"Part of the design of S3Guard is support for skipping the call to S3 listObjects and serving directory listings out of the MetadataStore under certain circumstances.  This feature is called ""authoritative"" mode.  I've talked to many people about this feature and it seems to be universally confusing.

I suggest we improve / add a section to the s3guard.md site docs elaborating on what Authoritative Mode is.

It is *not* treating the MetadataStore (e.g. dynamodb) as the source of truth in general.

It *is* the ability to short-circuit S3 list objects and serve listings from the MetadataStore in some circumstances: 

For S3A to skip S3's list objects on some *path*, and serve it directly from the MetadataStore, the following things must all be true:
 # The MetadataStore implementation persists the bit {{DirListingMetadata.isAuthorititative}} set when calling {{MetadataStore#put(DirListingMetadata)}}
 # The S3A client is configured to allow metadatastore to be authoritative source of a directory listing (fs.s3a.metadatastore.authoritative=true).
 # The MetadataStore has a full listing for *path* stored in it.  This only happens if the FS client (s3a) explicitly has stored a full directory listing with {{DirListingMetadata.isAuthorititative=true}} before the said listing request happens.

Note that #1 only currently happens in LocalMetadataStore. Adding support to DynamoDBMetadataStore is covered in HADOOP-14154.

Also, the multiple uses of the word ""authoritative"" are confusing. Two meanings are used:
 1. In the FS client configuration fs.s3a.metadatastore.authoritative
 - Behavior of S3A code (not MetadataStore)
 - ""S3A is allowed to skip S3.list() when it has full listing from MetadataStore""

2. MetadataStore
 When storing a dir listing, can set a bit isAuthoritative
 1 : ""full contents of directory""
 0 : ""may not be full listing""

Note that a MetadataStore *MAY* persist this bit. (not *MUST*).

We should probably rename the {{DirListingMetadata.isAuthorititative}} to {{.fullListing}} or at least put a comment where it is used to clarify its meaning.",gabor.bota
HADOOP-15399,KMSAcls should read kms-site.xml file.,"KMSACLs uses {{AccessControlList}} for authorization.
For creating groups membership, the group implementation class that will be instantiated is configured by {{hadoop.security.group.mapping}}.
Today {{KMSACLs}} class reads only {{kms-acls.xml}} file to create {{AccessControlList}}.
{{kms-acls.xml}} doesn't look the right place add the above config.
So KMSAcls should read either kms-site.
[~xiaochen]: Any preference which file should acls load ?
IMO it should be kms-site because that file is mandatory. But all the properties in kms-site.xml starts with {{hadoop.kms}}, I am little bit inclined towards core-site.xml.
 ",shahrs87
HADOOP-15398,StagingTestBase uses methods not available in Mockito 1.8.5,"*Problem:* hadoop trunk compilation is failing
 *Root Cause:*
 compilation error is coming from {{org.apache.hadoop.fs.s3a.commit.staging.StagingTestBase}}. Compilation error is ""The method getArgumentAt(int, Class<UploadPartRequest>) is undefined for the type InvocationOnMock"".

StagingTestBase is using getArgumentAt(int, Class<UploadPartRequest>) method which is not available in mockito-all 1.8.5 version. getArgumentAt(int, Class<UploadPartRequest>) method is available only from version 2.0.0-beta

*Expectations:*
 Either mockito-all version to be upgraded or test case to be written only with available functions in 1.8.5.",arshadmohammad
HADOOP-15396,Some java source files are executable,"{noformat}
$ find . -name ""*.java"" -perm 755
./hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/placement/TestUserGroupMappingPlacementRule.java
./hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/TestResourceLocalizationService.java
./hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/ResourceLocalizationService.java
./hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TestTaskImpl.java
./hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java
./hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskImpl.java
./hadoop-hdfs-project/hadoop-hdfs-client/src/test/java/org/apache/hadoop/hdfs/TestDFSPacket.java
./hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSPacket.java
./hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java
{noformat}
The permission should be 644. Let's fix them.",shashikant
HADOOP-15395,DefaultImpersonationProvider fails to parse proxy user config if username has . in it,"DefaultImpersonationProvider fails to parse proxy user config if username has . in it. 

 ",ajayydv
HADOOP-15394,Backport PowerShell NodeFencer HADOOP-14309 to branch-2,"HADOOP-14309 added PowerShell NodeFencer.
We should backport it to branch-2 and branch-2.9.",elgoiri
HADOOP-15393,Upgrade the version of commons-lang3 to 3.7,"{{mvn javadoc:javadoc}} with jdk10 fails due to MJAVADOC-517. This is reported by [~ajisakaa].

Upgrading commons-lang3 version to the latest version 3.7 works round the issue.",tasanuma0829
HADOOP-15391,"Add missing css file in hadoop-aws, hadoop-aliyun, hadoop-azure and hadoop-azure-datalake modules","The documentation pages for hadoop-aws, hadoop-aliyun, hadoop-azure and hadoop-azure-datalake render error (see screen-shot attached or [here|http://hadoop.apache.org/docs/current/hadoop-aws/tools/hadoop-aws/index.html]). The reason of this is that the css file is missing in these modules.",linyiqun
HADOOP-15390,Yarn RM logs flooded by DelegationTokenRenewer trying to renew KMS tokens,"When looking at a recent issue with [~rkanter] and [~yufeigu], we found that the RM log in a cluster was flooded by KMS token renewal errors below:
{noformat}
$ tail -9 hadoop-cmf-yarn-RESOURCEMANAGER.log
2018-04-11 11:34:09,367 WARN org.apache.hadoop.crypto.key.kms.KMSClientProvider$KMSTokenRenewer: keyProvider null cannot renew dt.
2018-04-11 11:34:09,367 INFO org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer: Renewed delegation-token= [Kind: kms-dt, Service: KMSIP:16000, Ident: (kms-dt owner=user, renewer=yarn, realUser=, issueDate=1522192283334, maxDate=1522797083334, sequenceNumber=15108613, masterKeyId=2674);exp=0; apps=[]], for []
2018-04-11 11:34:09,367 INFO org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer: Renew Kind: kms-dt, Service: KMSIP:16000, Ident: (kms-dt owner=user, renewer=yarn, realUser=, issueDate=1522192283334, maxDate=1522797083334, sequenceNumber=15108613, masterKeyId=2674);exp=0; apps=[] in -1523446449367 ms, appId = []
...
2018-04-11 11:34:09,367 WARN org.apache.hadoop.crypto.key.kms.KMSClientProvider$KMSTokenRenewer: keyProvider null cannot renew dt.
2018-04-11 11:34:09,367 INFO org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer: Renewed delegation-token= [Kind: kms-dt, Service: KMSIP:16000, Ident: (kms-dt owner=user, renewer=yarn, realUser=, issueDate=1522192283334, maxDate=1522797083334, sequenceNumber=15108613, masterKeyId=2674);exp=0; apps=[]], for []
2018-04-11 11:34:09,367 INFO org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer: Renew Kind: kms-dt, Service: KMSIP:16000, Ident: (kms-dt owner=user, renewer=yarn, realUser=, issueDate=1522192283334, maxDate=1522797083334, sequenceNumber=15108613, masterKeyId=2674);exp=0; apps=[] in -1523446449367 ms, appId = []
{noformat}

Further inspection shows the KMS IP is from another cluster. The RM is before HADOOP-14445, so needs to read from config. The config rightfully doesn't have the other cluster's KMS configured.

Although HADOOP-14445 will make this a non-issue by creating the provider from token service, we should fix 2 things here:
- KMS token renewer should throw instead of return 0. Returning 0 when not able to renew shall be considered a bug in the renewer.
- Yarn RM's {{DelegationTokenRenewer}} service should validate the return and not go into this busy loop.",xiaochen
HADOOP-15388,"LocalFilesystem#rename(Path, Path, Options.Rename...) does not handle crc files","ChecksumFilesystem#rename(Path, Path, Options.Rename...) is missing and FilterFileSystem does not care with crc files. That causes abandoned crc files in case of rename.",boky01
HADOOP-15386,FileSystemContractBaseTest#testMoveFileUnderParent duplicates testRenameFileToSelf,"{{FileSystemContractBaseTest#testMoveFileUnderParent}} test copy-pastes {{testRenameFileToSelf}} test, i.e. it tests copying to self instead of copying under parent.

Attached patch fixes {{testMoveFileUnderParent}} to test copying under parent.",medb
HADOOP-15385,Many tests are failing in hadoop-distcp project in branch-2,"Many tests are failing in hadoop-distcp project in branch-2.8
Below are the failing tests.
{noformat}
Failed tests: 
  TestDistCpViewFs.testUpdateGlobTargetMissingSingleLevel:326->checkResult:428 expected:<4> but was:<5>
  TestDistCpViewFs.testGlobTargetMissingMultiLevel:346->checkResult:428 expected:<4> but was:<5>
  TestDistCpViewFs.testGlobTargetMissingSingleLevel:306->checkResult:428 expected:<2> but was:<3>
  TestDistCpViewFs.testUpdateGlobTargetMissingMultiLevel:367->checkResult:428 expected:<6> but was:<8>
  TestIntegration.testUpdateGlobTargetMissingSingleLevel:431->checkResult:577 expected:<4> but was:<5>
  TestIntegration.testGlobTargetMissingMultiLevel:454->checkResult:577 expected:<4> but was:<5>
  TestIntegration.testGlobTargetMissingSingleLevel:408->checkResult:577 expected:<2> but was:<3>
  TestIntegration.testUpdateGlobTargetMissingMultiLevel:478->checkResult:577 expected:<6> but was:<8>
  TestIntegration.testUpdateGlobTargetMissingSingleLevel:431->checkResult:577 expected:<4> but was:<5>
  TestIntegration.testGlobTargetMissingMultiLevel:454->checkResult:577 expected:<4> but was:<5>
  TestIntegration.testGlobTargetMissingSingleLevel:408->checkResult:577 expected:<2> but was:<3>
  TestIntegration.testUpdateGlobTargetMissingMultiLevel:478->checkResult:577 expected:<6> but was:<8>
  TestIntegration.testUpdateGlobTargetMissingSingleLevel:431->checkResult:577 expected:<4> but was:<5>
  TestIntegration.testGlobTargetMissingMultiLevel:454->checkResult:577 expected:<4> but was:<5>
  TestIntegration.testGlobTargetMissingSingleLevel:408->checkResult:577 expected:<2> but was:<3>
  TestIntegration.testUpdateGlobTargetMissingMultiLevel:478->checkResult:577 expected:<6> but was:<8>

Tests run: 258, Failures: 16, Errors: 0, Skipped: 0
{noformat}

{noformat}
rushabhs$ pwd
/Users/rushabhs/hadoop/apacheHadoop/hadoop/hadoop-tools/hadoop-distcp

rushabhs$ git branch
 branch-2
  branch-2.7
* branch-2.8
  branch-2.9
  branch-3.0

 rushabhs$ git log --oneline | head -n3
c4ea1c8bb73 HADOOP-14970. MiniHadoopClusterManager doesn't respect lack of format option. Contributed by Erik Krogen
1548205a845 YARN-8147. TestClientRMService#testGetApplications sporadically fails. Contributed by Jason Lowe
c01b425ba31 YARN-8120. JVM can crash with SIGSEGV when exiting due to custom leveldb logger. Contributed by Jason Lowe.
{noformat}",jlowe
HADOOP-15384,distcp numListstatusThreads option doesn't get to -delete scan,"The distcp {{numListstatusThreads}} option isn't used when configuring the GlobbedCopyListing used in {{CopyComitter.deleteMissing()}}

This means that for large scans of object stores, performance is significantly worse.

Fix: pass the option down from the task conf",stevel@apache.org
HADOOP-15382,Log kinit output in credential renewal thread,"We currently run kinit command in a thread to renew kerberos credentials periodically. 
{code:java}
            Shell.execCommand(cmd, ""-R"");
            if (LOG.isDebugEnabled()) {
              LOG.debug(""renewed ticket"");
            }
{code}
It seems useful to log the output of the kinit too.",gabor.bota
HADOOP-15381,KmsAcls should be reloaded periodically.,"Currently {{KMSACLs}} is getting reloaded depending on whether {{kms-acls}} file is modified or not.
Since the access control is specified by {{AcccessControlList}}, the member may contain {{unix-group}} or {{net-group}}.
If we add a new user to any group, technically the file is not modified but the membership of a group is modified, so we need to reload the {{kms-acls}} file.
There are couple of solutions for this.
1. short-term solution: Reload the file periodically (like lets say every 15 minutes).
2. long-term solution: Implement a {{refreshUserGroupMapping}} like protocol/servlet so whenever we change membership we can invoke that protocol and we get the updates.
",shahrs87
HADOOP-15380,TestViewFileSystemLocalFileSystem#testTrashRoot leaves an unnecessary file,"After running

{code}mvn test -Dtest=TestViewFileSystemLocalFileSystem#testTrashRoot
git status{code}
Git reports an untracked file: {{hadoop-common-project/hadoop-common/.debug.log.crc}}
It seems some cleanup issue.
 ",boky01
HADOOP-15379,Make IrqHandler.bind() public,"{{org.apache.hadoop.service.launcher.IrqHandler.bind()}} is package private

this means you can create an {{Interrupted}} handler in a different package, but you can't bind it to a signal.",ajayydv
HADOOP-15377,Improve debug messages in MetricsConfig.java,"I recently enabled debug level logging in a MR application and was getting a lot of log lines from this class that were just blank, without context.  I've enhanced the log messages to include additional context and a few other small clean up while looking at this class.",belugabehr
HADOOP-15376,Remove double semi colons on imports that make Clover fall over.,"Clover will fall over if there are double semicolons on imports.

The error looks like:
{code:java}
[INFO] Clover free edition.
[INFO] Updating existing database at '/Users/ehiggs/src/hadoop/hadoop-common-project/hadoop-common/target/clover/clover.db'.
[INFO] Processing files at 1.8 source level.
[INFO] /Users/ehiggs/src/hadoop/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/TestIOUtils.java:43:1:expecting EOF, found 'import'
[INFO] Instrumentation error
com.atlassian.clover.api.CloverException: /Users/ehiggs/src/hadoop/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/TestIOUtils.java:43:1:expecting EOF, found 'import'{code}
 

Thankfully we only have one location with this:
{code:java}
$ find . -name \*.java -exec grep '^import .*;;' {} +
./hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/TestIOUtils.java:import org.apache.commons.io.FileUtils;;{code}
 ",ehiggs
HADOOP-15375,Branch-2 pre-commit failed to build docker image,"Branch-2 pre-commit is failing during {{Building base image}}:
{noformat}
...
Step 28/33 : RUN apt-get -y install nodejs &&     ln -s /usr/bin/nodejs /usr/bin/node &&     apt-get -y install npm &&     npm set ca null &&     npm install -g bower &&     npm install -g ember-cli
{noformat}

{noformat}
...
Setting up npm (1.3.10~dfsg-1) ...
npm http GET 
[https://registry.npmjs.org/bower]
npm http GET 
[https://registry.npmjs.org/bower]
npm http GET 
[https://registry.npmjs.org/bower]
npm ERR! Error: CERT_UNTRUSTED
npm ERR!     at SecurePair.<anonymous> (tls.js:1370:32)
npm ERR!     at SecurePair.EventEmitter.emit (events.js:92:17)
npm ERR!     at SecurePair.maybeInitFinished (tls.js:982:10)
npm ERR!     at CleartextStream.read [as _read] (tls.js:469:13)
npm ERR!     at CleartextStream.Readable.read (_stream_readable.js:320:10)
npm ERR!     at EncryptedStream.write [as _write] (tls.js:366:25)
npm ERR!     at doWrite (_stream_writable.js:223:10)
npm ERR!     at writeOrBuffer (_stream_writable.js:213:5)
npm ERR!     at EncryptedStream.Writable.write (_stream_writable.js:180:11)
npm ERR!     at write (_stream_readable.js:583:24)
npm ERR! If you need help, you may report this log at:
npm ERR!     <
[http://github.com/isaacs/npm/issues]
>
npm ERR! or email it to:
npm ERR!     <npm-@googlegroups.com>

npm ERR! System Linux 3.13.0-143-generic
npm ERR! command ""/usr/bin/nodejs"" ""/usr/bin/npm"" ""install"" ""-g"" ""bower""
npm ERR! cwd /root
npm ERR! node -v v0.10.25
npm ERR! npm -v 1.3.10
npm ERR! 
npm ERR! Additional logging details can be found in:
npm ERR!     /root/npm-debug.log
npm ERR! not ok code 0
{noformat}",xiaochen
HADOOP-15374,Add links of the new features of 3.1.0 to the top page,,tasanuma0829
HADOOP-15373,Clear the code of Callqueuemanager,"The CallQueueManager contain two member, putRef and takeRef. They seems shouldn't have to use AtomicReference, it can use the volatile directly.",maobaolong
HADOOP-15372,Race conditions and possible leaks in the Shell class,"YARN-5641 introduced some cleanup code in the Shell class. It has a race condition. {{Shell.runCommand()}} can be called while/after {{Shell.getAllShells()}} returned all the shells to be cleaned up. This new thread can avoid the clean up, so that the process held by it can be leaked causing leaked localized files/etc.

I see another issue as well. {{Shell.runCommand()}} has a finally block with a {{process.destroy();}} to clean up. However, the try catch block does not cover all instructions after the process is started, so for example we can exit the thread and leak the process, if {{timeOutTimer.schedule(timeoutTimerTask, timeOutInterval);}} causes an exception.",ebadger
HADOOP-15370,S3A log message on rm s3a://bucket/ not intuitive,"when you try to delete the root of a bucket from command line, e.g. {{hadoop fs -rm -r -skipTrash s3a://hwdev-steve-new/}}, the output isn't that useful
{code}
2018-04-06 16:35:23,048 [main] INFO  s3a.S3AFileSystem (S3AFileSystem.java:rejectRootDirectoryDelete(1837)) - s3a delete the hwdev-steve-new root directory of true
rm: `s3a://hwdev-steve-new/': Input/output error
2018-04-06 16:35:23,050 [pool-2-thread-1] DEBUG s3a.S3AFileSystem
{code}

the single log message doesn't parse, and the error message raised is lost by the FS -rm CLI command (why?)

",gabor.bota
HADOOP-15369,Avoid usage of ${project.version} in parent poms,"hadoop-project/pom.xml and hadoop-project-dist/pom.xml use _${project.version}_ variable in dependencyManagement and plugin dependencies.

Unfortunatelly it could not work if we use different version in a child project as ${project.version} variable is resolved *after* the inheritance.

From [maven doc|https://maven.apache.org/guides/introduction/introduction-to-the-pom.html#Project_Inheritance]:

{quote}
For example, to access the project.version variable, you would reference it like so:

      <version>${project.version}</version>

One factor to note is that these variables are processed after inheritance as outlined above. This means that if a parent project uses a variable, then its definition in the child, not the parent, will be the one eventually used.
{quote}

The community voted to keep ozone in-tree but use a different release cycle. To achieve this we need different version for selected subproject therefor we can't use ${project.version} any more. 

 ",elek
HADOOP-15368,Apache Hadoop release 3.0.2 to fix deploying shaded jars in artifacts. ,"Apache Hadoop 3.0.1 was released with dummy shaded jars, like

{code}
Repository Path:  /org/apache/hadoop/hadoop-client-runtime/3.0.1/hadoop-client-runtime-3.0.1.jar
Uploaded by:      lei
Size:             44.47 KB
Uploaded Date:    Fri Mar 16 2018 15:50:42 GMT-0700 (PDT)
Last Modified:    Fri Mar 16 2018 15:50:42 GMT-0700 (PDT)
{code}

The community has agreed to release 3.0.2 on the same code base as 3.0.1, but with shaded jars, to fix the artifacts.  During this process, we moved the bug fixes with target version 3.0.2 to 3.0.3.  

This JIRA also serves as the metadata for 3.0.2 release to generate CHANGES.md and RELEASENOTE.md.
",eddyxu
HADOOP-15367,Update the initialization code in the docker hadoop-runner baseimage ,"The hadoop-runner baseimage contains initialization code for both the HDFS namenode/datanode and Ozone/Hdds scm/ksm.

The script name for Ozone/Hdds is changed (from oz to ozone) therefore we need to updated the base image.

This commit also would be a test for the dockerhub automated build.

Please apply the patch on the top of the _docker-hadoop-runner_ branch. ",elek
HADOOP-15366,Add a helper shutdown routine in HadoopExecutor to ensure clean shutdown,"It is recommended to shut down an {{ExecutorService}} in two phases, first by calling {{shutdown}} to reject incoming tasks, and then calling {{shutdownNow}}, if necessary, to cancel any lingering tasks. This Jira aims to add a helper shutdown routine in Hadoop executor  to achieve the same.",shashikant
HADOOP-15365,cannot find symbol sun.misc.Cleaner,"
{code:java}
[ERROR] COMPILATION ERROR : 
[INFO] -------------------------------------------------------------
[ERROR] /hadoop/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/nativeio/NativeIO.java:[332,17] cannot find symbol
  symbol:   class Cleaner
  location: package sun.misc
[ERROR] /hadoop/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/CryptoStreamUtils.java:[40,21] cannot find symbol
  symbol:   class Cleaner
  location: package sun.misc
{code}


We should use reflection to adapt jdk8 and jdk9",maobaolong
HADOOP-15362,Review of Configuration.java,"* Various improvements
 * Fix a lot of checks style errors

When I ran a recent debug log against a MR job, I was spammed from the following messages.  I ask that we move them to 'trace' as there is already a debug level logging preceding them.
{code:java}
LOG.debug(""Handling deprecation for all properties in config"");
foreach item {
-      LOG.debug(""Handling deprecation for "" + (String)item);
+      LOG.trace(""Handling deprecation for {}"", item);
}{code}",belugabehr
HADOOP-15361,RawLocalFileSystem should use Java nio framework for rename,"Currently RawLocalFileSystem uses a fallback logic for cross-volume renames. The fallback logic is a copy-on-fail logic so when rename fails it copies the source then delete it.
 An additional fallback logic was needed for Windows to provide POSIX rename behavior.

Due to the fallback logic RawLocalFileSystem does not pass the contract tests (HADOOP-13082).

With using Java nio framework both could be eliminated since it is not platform dependent and provides cross-volume rename.

In addition the fallback logic for Windows is not correct since Java io overrides the destination only if the source is also a directory but handleEmptyDstDirectoryOnWindows method checks only the destination. That means rename allows to override a directory with a file on Windows but not on Unix.

File#renameTo and Files#move are not 100% compatible:
 If the source is a directory and the destination is an empty directory File#renameTo overrides the source but Files#move is does not. We have to use {{StandardCopyOption.REPLACE_EXISTING}} but it overrides the destination even if the source or the destination is a file. So to make them compatible we have to check that the either the source or the destination is a directory before we add the copy option.

I think the correct strategy is
 * Where the contract test passed so far it should pass after this
 * Where the contract test failed because of Java specific think and not because of the fallback logic we should keep the original behavior.",boky01
HADOOP-15358,SFTPConnectionPool connections leakage,"Methods of SFTPFileSystem operate on poolable ChannelSftp instances, thus some methods of SFTPFileSystem are chained together resulting in establishing multiple connections to the SFTP server to accomplish one compound action, those methods are listed below:
 # mkdirs method
the public mkdirs method acquires a new ChannelSftp from the pool [1]
and then recursively creates directories, checking for the directory existence beforehand by calling the method exists[2] which delegates to the getFileStatus(ChannelSftp channel, Path file) method [3] and so on until it ends up in returning the FilesStatus instance [4]. The resource leakage occurs in the method getWorkingDirectory which calls the getHomeDirectory method [5] which in turn establishes a new connection to the sftp server instead of using an already created connection. As the mkdirs method is recursive this results in creating a huge number of connections.
 # open method [6]. This method returns an instance of FSDataInputStream which consumes SFTPInputStream instance which doesn't return an acquired ChannelSftp instance back to the pool but instead it closes it[7]. This leads to establishing another connection to an SFTP server when the next method is called on the FileSystem instance.


[1] https://github.com/apache/hadoop/blob/736ceab2f58fb9ab5907c5b5110bd44384038e6b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java#L658

[2] https://github.com/apache/hadoop/blob/736ceab2f58fb9ab5907c5b5110bd44384038e6b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java#L321

[3] https://github.com/apache/hadoop/blob/736ceab2f58fb9ab5907c5b5110bd44384038e6b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java#L202

[4] https://github.com/apache/hadoop/blob/736ceab2f58fb9ab5907c5b5110bd44384038e6b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java#L290

[5] https://github.com/apache/hadoop/blob/736ceab2f58fb9ab5907c5b5110bd44384038e6b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java#L640

[6] https://github.com/apache/hadoop/blob/736ceab2f58fb9ab5907c5b5110bd44384038e6b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java#L504

[7] https://github.com/apache/hadoop/blob/736ceab2f58fb9ab5907c5b5110bd44384038e6b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/sftp/SFTPInputStream.java#L123",m.pryahin
HADOOP-15357,Configuration.getPropsWithPrefix no longer does variable substitution,"Before [HADOOP-13556], Configuration.getPropsWithPrefix() used the Configuration.get() method to get the value of the variables.   After [HADOOP-13556], it now uses props.getProperty().

The difference is that Configuration.get() does deprecation handling and more importantly variable substitution on the value.  So if a property has a variable specified with ${variable_name}, it will no longer be expanded when retrieved via getPropsWithPrefix().

Was this change in behavior intentional?  I am using this function in the fix for [MAPREDUCE-7069], but we do want variable expansion to happen.",jim_brennan
HADOOP-15356,Make HTTP timeout configurable in ADLS Connector,"Currently the HTTP timeout for the connections to ADLS are not configurable in Hadoop. This patch enables the timeouts to be configurable based on a core-site config setting. Also, up the ADLS SDK version to 2.2.8, that has default value of 60 seconds - any optimizations to that setting can now be done in Hadoop through core-site.",asikaria
HADOOP-15355,TestCommonConfigurationFields is broken by HADOOP-15312,TestCommonConfigurationFields is failing after HADOOP-15312.,gelixin
HADOOP-15354,hadoop-aliyun & hadoop-azure modules to mark hadoop-common as provided,"Although the aws/openstack and adl modules now declare hadopo-common as ""provided"" the hadoop-aliyun and hadoop-azure modules don't, so it gets into the set of dependencies passed on through hadoop-cloud-storage. It should be switched to provided in the POMs of these modules",stevel@apache.org
HADOOP-15353,Bump default yetus version in the yetus-wrapper,The current precommit hook uses yetus 0.8.0-SNAPSHOT. The default version in the yetus-wrapper script is 0.4.0. It could be adjusted HADOOP_YETUS_VERSION but I suggest to set the default version to 0.7.0 to get results similar to the jenkins results locally without adjustments.,elek
HADOOP-15352,Fix default local maven repository path in create-release script ,"I am testing the create-release script locally. In case the MVNCACHE is not set the local ~/.m2 is used. Which is not good as the packages are downloaded to ~/.m2/org/.../... instead of ~/.m2/repository/org/.../.../...

",elek
HADOOP-15350,[JDK10] Update maven plugin tools to fix compile error in hadoop-maven-plugins module,"{{mvn install -DskipTests}} fails with Java 10.
{noformat}
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-plugin-plugin:3.4:descriptor (default-descriptor) on project hadoop-maven-plugins: Execution default-descriptor of goal org.apache.maven.plugins:maven-plugin-plugin:3.4:descriptor failed. IllegalArgumentException -> [Help 1]
org.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal org.apache.maven.plugins:maven-plugin-plugin:3.4:descriptor (default-descriptor) on project hadoop-maven-plugins: Execution default-descriptor of goal org.apache.maven.plugins:maven-plugin-plugin:3.4:descriptor failed.
{noformat}
Upgrading maven plugin tools to 3.5.1 fixes this.",tasanuma0829
HADOOP-15349,S3Guard DDB retryBackoff to be more informative on limits exceeded,"When S3Guard can't update the DB and so throws an IOE after the retry limit is exceeded, it's not at all informative. Improve logging & exception

",gabor.bota
HADOOP-15346,"S3ARetryPolicy for 400/BadArgument to be ""fail""","The retry policy for the AWS 400/BadArgument response is currently ""treat as a connectivity error"" on the basis that sometimes it works again.

It doesn't, not normally, and by using the connectivity retry policy, unrecoverable failures can take time to surface.

Proposed: switch to a fail fast policy for BadArgumentException",stevel@apache.org
HADOOP-15345,Backport HADOOP-12185 to branch-2.7: NetworkTopology is not efficient adding/getting/removing nodes,As per discussion in HADOOP-15343 backport HADOOP-12185 to branch-2.7,hexiaoqiao
HADOOP-15344,LoadBalancingKMSClientProvider#close should not swallow exceptions,"As [~shahrs87]'s comment on HADOOP-14445 says:
{quote}
LoadBalancingKMSCP never throws IOException back. It just swallows all the IOException and just logs it.
...
Maybe we might want to return MultipleIOException from LoadBalancingKMSCP#close. 
{quote}",zhenyi
HADOOP-15342,Update ADLS connector to use the current SDK version (2.2.7),"Updating the ADLS SDK connector to use the current version of the ADLS SDK (2.2.7).

 

Changelist is here: [https://github.com/Azure/azure-data-lake-store-java/blob/sdk2.2/CHANGES.md]

 

Short summary of what matters:

Change to the MSI token acquisition interface required by the change in REST interface to Azure ActiveDirecotry's VM MSI interface, and improved diagnostics in the SDK for token acquisition failures (better exception message and log message). The diagnostics was requested in HADOOP-15188.

 ",asikaria
HADOOP-15340,Provide meaningful RPC server name for RpcMetrics,"In case of multiple RPC servers in the same JVM it's hard to identify the metric data. The only available information as of now is the port number.

Server name is also added in the constructor of Server.java but it's not used at all.

This patch fix this behaviour:

 1. The server name is saved to a field in Server.java (constructor signature is not changed)
 2. ServerName is added as a tag to the metrics in RpcMetrics
 3. The naming convention for the severs are fix.

About 3: if the server name is not defined the current code tries to identify the name from the class name. Which is not always an easy task as in some cases the server has a protobuf generated dirty name which also could be an inner class.

The patch also improved the detection of the name (if it's not defined). It's a compatible change as the current name is not user ad all.",elek
HADOOP-15339,Support additional key/value propereties in JMX bean registration,"org.apache.hadoop.metrics2.util.MBeans.register is a utility function to register objects to the JMX registry with a given name prefix and name.

JMX supports any additional key value pairs which could be part the the address of the jmx bean. For example: _java.lang:type=MemoryManager,name=CodeCacheManager_

Using this method we can query a group of mbeans, for example we can add the same tag to similar mbeans from namenode and datanode.

This patch adds a small modification to support custom key value pairs and also introduce a new unit test for MBeans utility which was missing until now.",elek
HADOOP-15337,RawLocalFileSystem file status permissions can avoid shelling out in some cases,"While investigating YARN-8054, it was noticed that getting file permissions for RawLocalFileSystem can fail by having too many files open. Upon inspection this happens when getting permissions by launching a shell program (ls -ld on linux) and parsing the results. With the introduction of java 7 posix file systems can accurately get file permissions without launching a shell program.",jeagles
HADOOP-15334,Upgrade Maven surefire plugin,Recent versions of the surefire plugin suppress summary test execution output in quiet mode. This is now fixed in plugin version 2.21.0 (via SUREFIRE-1436).,arpitagarwal
HADOOP-15333,Deprecate or remove Groups#getUserToGroupsMappingServiceWithLoadedConfiguration,Groups#getUserToGroupsMappingServiceWithLoadedConfiguration introduced in YARN-1676 reinitializes its static instance i.e \{{GROUPS}}. This  instance is stored by UGI for local reference. Use of getUserToGroupsMappingServiceWithLoadedConfiguration results in UGI reference detached from Parent class. This jira intends to discuss probable removal or depreciation of this function to remove innocuous bug resulting from it.,ajayydv
HADOOP-15332,Fix typos in hadoop-aws markdown docs,"While reading through https://github.com/apache/hadoop/tree/trunk/hadoop-tools/hadoop-aws/src/site/markdown/tools/hadoop-aws I've found some very obvious typos, and I thought it would be a nice improvement to fix those.",gabor.bota
HADOOP-15331,Fix a race condition causing parsing error of java.io.BufferedInputStream in class org.apache.hadoop.conf.Configuration,"There is a race condition in the way Hadoop handles the Configuration class. The scenario is the following. Let's assume that there are two threads sharing the same Configuration class. One adds some resources to the configuration, while the other one clones it. Resources are loaded lazily in a deferred call to {{loadResources()}}. If the cloning happens after adding the resources but before parsing them, some temporary resources like input stream pointers are cloned. Eventually both copies will load the input stream resources pointing to the same input streams. One parses the input stream XML and closes it updating it's own copy of the resource. The other one has another pointer to the same input stream. When it tries to load it, it will crash with a stream closed exception.

Here is an example unit test:
{code:java}
@Test
public void testResourceRace() {
  InputStream is =
      new BufferedInputStream(new ByteArrayInputStream(
          ""<configuration></configuration>"".getBytes()));
  Configuration conf = new Configuration();
  // Thread 1
  conf.addResource(is);
  // Thread 2
  Configuration confClone = new Configuration(conf);
  // Thread 2
  confClone.get(""firstParse"");
  // Thread 1
  conf.get(""secondParse"");
}{code}
Example real world stack traces:
{code:java}
2018-02-28 08:23:19,589 ERROR org.apache.hadoop.conf.Configuration: error parsing conf java.io.BufferedInputStream@7741d346
com.ctc.wstx.exc.WstxIOException: Stream closed
	at com.ctc.wstx.stax.WstxInputFactory.doCreateSR(WstxInputFactory.java:578)
	at com.ctc.wstx.stax.WstxInputFactory.createSR(WstxInputFactory.java:633)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:2803)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2853)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2817)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2689)
	at org.apache.hadoop.conf.Configuration.get(Configuration.java:1420)
	at org.apache.hadoop.security.authorize.ServiceAuthorizationManager.refreshWithLoadedConfiguration(ServiceAuthorizationManager.java:161)
	at org.apache.hadoop.ipc.Server.refreshServiceAclWithLoadedConfiguration(Server.java:607)
	at org.apache.hadoop.yarn.server.resourcemanager.AdminService.refreshServiceAcls(AdminService.java:586)
	at org.apache.hadoop.yarn.server.resourcemanager.AdminService.startServer(AdminService.java:188)
	at org.apache.hadoop.yarn.server.resourcemanager.AdminService.serviceStart(AdminService.java:165)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)
	at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:121)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:1231)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1421)
{code}
Another example:
{code:java}
2018-02-28 08:23:20,702 ERROR org.apache.hadoop.conf.Configuration: error parsing conf java.io.BufferedInputStream@7741d346
com.ctc.wstx.exc.WstxIOException: Stream closed
	at com.ctc.wstx.stax.WstxInputFactory.doCreateSR(WstxInputFactory.java:578)
	at com.ctc.wstx.stax.WstxInputFactory.createSR(WstxInputFactory.java:633)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:2803)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2853)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2817)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2689)
	at org.apache.hadoop.conf.Configuration.set(Configuration.java:1326)
	at org.apache.hadoop.conf.Configuration.set(Configuration.java:1298)
	at org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebApp.buildRedirectPath(RMWebApp.java:103)
	at org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebApp.getRedirectPath(RMWebApp.java:91)
	at org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebAppFilter.doFilter(RMWebAppFilter.java:125)
	at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:829)
	at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:82)
	at com.google.inject.servlet.ManagedFilterPipeline.dispatch(ManagedFilterPipeline.java:119)
	at com.google.inject.servlet.GuiceFilter$1.call(GuiceFilter.java:133)
	at com.google.inject.servlet.GuiceFilter$1.call(GuiceFilter.java:130)
	at com.google.inject.servlet.GuiceFilter$Context.call(GuiceFilter.java:203)
	at com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.java:130)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)
	at org.apache.hadoop.security.http.XFrameOptionsFilter.doFilter(XFrameOptionsFilter.java:57)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)
	at org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter.doFilter(StaticUserWebFilter.java:110)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)
	at org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1560)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)
	at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:582)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)
	at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:548)
	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:226)
	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1180)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:512)
	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:185)
	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1112)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
	at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:119)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)
	at org.eclipse.jetty.server.Server.handle(Server.java:534)
	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:320)
	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)
	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)
	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)
	at org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)
	at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)
	at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)
	at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.execute(ExecuteProduceConsume.java:100)
	at org.eclipse.jetty.io.ManagedSelector.run(ManagedSelector.java:147)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: Stream closed
	at java.io.BufferedInputStream.getBufIfOpen(BufferedInputStream.java:170)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:336)
	at com.ctc.wstx.io.StreamBootstrapper.ensureLoaded(StreamBootstrapper.java:482)
	at com.ctc.wstx.io.StreamBootstrapper.resolveStreamEncoding(StreamBootstrapper.java:306)
	at com.ctc.wstx.io.StreamBootstrapper.bootstrapInput(StreamBootstrapper.java:167)
	at com.ctc.wstx.stax.WstxInputFactory.doCreateSR(WstxInputFactory.java:573)
	... 50 more
2018-02-28 08:23:20,705 WARN org.eclipse.jetty.servlet.ServletHandler: /jmx
java.lang.RuntimeException: com.ctc.wstx.exc.WstxIOException: Stream closed
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3048)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2817)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2689)
	at org.apache.hadoop.conf.Configuration.set(Configuration.java:1326)
	at org.apache.hadoop.conf.Configuration.set(Configuration.java:1298)
	at org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebApp.buildRedirectPath(RMWebApp.java:103)
	at org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebApp.getRedirectPath(RMWebApp.java:91)
	at org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebAppFilter.doFilter(RMWebAppFilter.java:125)
	at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:829)
	at com.google.inject.servlet.ManagedFilterPipeline.dispatch(ManagedFilterPipeline.java:119)
	at com.google.inject.servlet.GuiceFilter$1.call(GuiceFilter.java:133)
	at com.google.inject.servlet.GuiceFilter$1.call(GuiceFilter.java:130)
	at com.google.inject.servlet.GuiceFilter$Context.call(GuiceFilter.java:203)
	at com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.java:130)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)
	at org.apache.hadoop.security.http.XFrameOptionsFilter.doFilter(XFrameOptionsFilter.java:57)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)
	at org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter.doFilter(StaticUserWebFilter.java:110)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)
	at org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1560)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)
	at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:582)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)
	at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:548)
	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:226)
	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1180)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:512)
	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:185)
	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1112)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
	at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:119)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)
	at org.eclipse.jetty.server.Server.handle(Server.java:534)
	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:320)
	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)
	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)
	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)
	at org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)
	at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)
	at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)
	at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.execute(ExecuteProduceConsume.java:100)
	at org.eclipse.jetty.io.ManagedSelector.run(ManagedSelector.java:147)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)
	at java.lang.Thread.run(Thread.java:748)
Caused by: com.ctc.wstx.exc.WstxIOException: Stream closed
	at com.ctc.wstx.stax.WstxInputFactory.doCreateSR(WstxInputFactory.java:578)
	at com.ctc.wstx.stax.WstxInputFactory.createSR(WstxInputFactory.java:633)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:2803)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2853)
	... 46 more
Caused by: java.io.IOException: Stream closed
	at java.io.BufferedInputStream.getBufIfOpen(BufferedInputStream.java:170)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:336)
	at com.ctc.wstx.io.StreamBootstrapper.ensureLoaded(StreamBootstrapper.java:482)
	at com.ctc.wstx.io.StreamBootstrapper.resolveStreamEncoding(StreamBootstrapper.java:306)
	at com.ctc.wstx.io.StreamBootstrapper.bootstrapInput(StreamBootstrapper.java:167)
	at com.ctc.wstx.stax.WstxInputFactory.doCreateSR(WstxInputFactory.java:573)
	... 49 more
2018-02-28 08:23:20,715 INFO org.{code}",miklos.szegedi@cloudera.com
HADOOP-15330,Remove jdk1.7 profile from hadoop-annotations module,Java 7 is not supported in Hadoop 3. Let's remove the profile.,zhenyi
HADOOP-15328,Fix the typo in HttpAuthentication.md,"There is a typo \{{AuthenticatorHandler}} in HttpAuthentication.md. 

 
{code:java}
If a custom authentication mechanism is required for the HTTP web-consoles, it is possible to implement a plugin to support the alternate authentication mechanism (refer to Hadoop hadoop-auth for details on writing an AuthenticatorHandler).
{code}
There is not an \{{AuthenticatorHandler}} in Hadoop hadoop-auth. \{{AuthenticatorHandler}} should be replaced with \{{AuthenticationHandler}}.

 ",zhenyi
HADOOP-15327,Upgrade MR ShuffleHandler to use Netty4,"This way, we can remove the dependencies on the netty3 (jboss.netty)",bharatviswa
HADOOP-15325,Make Configuration#getPasswordFromCredentialsProvider() a public API,"HADOOP-10607 added a public API Configuration.getPassword() which reads passwords from credential provider and then falls back to reading from configuration if one is not available.

This API has been used throughout Hadoop codebase and downstream applications. It is understandable for old password configuration keys to fallback to configuration to maintain backward compatibility. But for new configuration passwords that don't have legacy, there should be an option to _not_ fallback, because storing passwords in configuration is considered a bad security practice.",zvenczel
HADOOP-15323,AliyunOSS: Improve copy file performance for AliyunOSSFileSystemStore,"Aliyun OSS will support shallow copy which means server will only copy metadata when copy object operation occurs. 

With shallow copy, we can use copyObject api instead of multi-part copy api if we do not change object storage type & encryption type & source object is uploaded by Put / Multipart upload api.

So, we will add a flag to indicate whether support shallow copy or not, default value is  true.",wujinhu
HADOOP-15322,LDAPGroupMapping search tree base improvement,Currently the same ldap base is used for searching posixAccount and posixGroup. This request is to make a separate base for each container (ie posixAccount and posixGroup container),zhengxg3
HADOOP-15321,Reduce the RPC Client max retries on timeouts,"Currently, the [default|https://github.com/apache/hadoop/blob/branch-3.0.0/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/CommonConfigurationKeysPublic.java#L379] number of retries when IPC client catch a {{ConnectTimeoutException}} is 45. This seems unreasonably high.

Given the IPC client timeout is by default 60 seconds, if a DN host is shutdown the client will retry for 45 minutes until aborting. (If host is there but process down, it would throw a connection refused immediately, which is cool)

Creating this Jira to discuss whether we can reduce that to a reasonable number.",xiaochen
HADOOP-15320,Remove customized getFileBlockLocations for hadoop-azure and hadoop-azure-datalake,"hadoop-azure and hadoop-azure-datalake have its own implementation of getFileBlockLocations(), which faked a list of artificial blocks based on the hard-coded block size. And each block has one host with name ""localhost"". Take a look at this code:

[https://github.com/apache/hadoop/blob/release-2.9.0-RC3/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/NativeAzureFileSystem.java#L3485]

This is a unnecessary mock up for a ""remote"" file system to mimic HDFS. And the problem with this mock is that for large (~TB) files we generates lots of artificial blocks, and FileInputFormat.getSplits() is slow in calculating splits based on these blocks.

We can safely remove this customized getFileBlockLocations() implementation, fall back to the default FileSystem.getFileBlockLocations() implementation, which is to return 1 block for any file with 1 host ""localhost"". Note that this doesn't mean we will create much less splits, because the number of splits is still limited by the blockSize in FileInputFormat.computeSplitSize():
{code:java}
return Math.max(minSize, Math.min(goalSize, blockSize));{code}",shanyu
HADOOP-15317,Improve NetworkTopology chooseRandom's loop,"Recently we found a postmortem case where the ANN seems to be in an infinite loop. From the logs it seems it just went through a rolling restart, and DNs are getting registered.

Later the NN become unresponsive, and from the stacktrace it's inside a do-while loop inside {{NetworkTopology#chooseRandom}} - part of what's done in HDFS-10320.

Going through the code and logs I'm not able to come up with any theory (thought about incorrect locking, or the Node object being modified outside of NetworkTopology, both seem impossible) why this is happening, but we should eliminate this loop.

stacktrace:
{noformat}
 Stack:
java.util.HashMap.hash(HashMap.java:338)
java.util.HashMap.containsKey(HashMap.java:595)
java.util.HashSet.contains(HashSet.java:203)
org.apache.hadoop.net.NetworkTopology.chooseRandom(NetworkTopology.java:786)
org.apache.hadoop.net.NetworkTopology.chooseRandom(NetworkTopology.java:732)
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseDataNode(BlockPlacementPolicyDefault.java:757)
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRandom(BlockPlacementPolicyDefault.java:692)
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRandom(BlockPlacementPolicyDefault.java:666)
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseLocalRack(BlockPlacementPolicyDefault.java:573)
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTargetInOrder(BlockPlacementPolicyDefault.java:461)
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:368)
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:243)
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:115)
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4AdditionalDatanode(BlockManager.java:1596)
org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalDatanode(FSNamesystem.java:3599)
org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getAdditionalDatanode(NameNodeRpcServer.java:717)
{noformat}",xiaochen
HADOOP-15316,GenericTestUtils can exceed maxSleepTime,"Probably shouldn't ever cause an issue, especially since Thread.sleep() can cause longer delays beyond your control anyway, but for larger values this could still behave unpredicatably in practice.
{code:java}
Thread.sleep(r.nextInt(maxSleepTime) + minSleepTime);
{code}
should be
{code:java}
Thread.sleep(r.nextInt(maxSleepTime - minSleepTime) + minSleepTime){code}
 ",adam.antal
HADOOP-15313,TestKMS should close providers,"During the review of HADOOP-14445, [~jojochuang] found that we key providers are not closed in tests. Details in [this comment|https://issues.apache.org/jira/browse/HADOOP-14445?focusedCommentId=16397824&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16397824].

We should investigate and handle that in all related tests.",xiaochen
HADOOP-15312,Undocumented KeyProvider configuration keys,"Via HADOOP-14445, I found two undocumented configuration keys: hadoop.security.key.default.bitlength and hadoop.security.key.default.cipher",gelixin
HADOOP-15311,HttpServer2 needs a way to configure the acceptor/selector count,"HttpServer2 starts up with some number of acceptors and selectors, but only allows for the automatic configuration of these based off of the number of available cores:
{code:title=org.eclipse.jetty.server.ServerConnector}
selectors > 0 ? selectors : Math.max(1, Math.min(4, Runtime.getRuntime().availableProcessors() / 2)))
{code}
{code:title=org.eclipse.jetty.server.AbstractConnector}
    if (acceptors < 0) {
      acceptors = Math.max(1, Math.min(4, cores / 8));
    }
{code}
A thread pool is started of size, at minimum, {{acceptors + selectors + 1}}, so in addition to allowing for a higher tuning value under heavily loaded environments, adding configurability for this enables tuning these values down in resource constrained environments such as a MiniDFSCluster.",xkrogen
HADOOP-15308,TestConfiguration fails on Windows because of paths,"We are seeing multiple failures with:
{code}
Illegal character in authority at index 7: file://C:\_work\10\s\hadoop-common-project\hadoop-common\.\test-config-uri-TestConfiguration.xml
{code}
We seem to not be managing the colon of the drive path properly.",surmountian
HADOOP-15307,NFS: flavor AUTH_SYS should use VerifierNone,"When NFS gateway starts and if the portmapper request is denied by rpcbind for any reason (in our case, /etc/hosts.allow did not have the localhost), NFS gateway fails with the following obscure exception:
{noformat}

2018-03-05 12:49:31,976 INFO org.apache.hadoop.oncrpc.SimpleUdpServer: Started listening to UDP requests at port 4242 for Rpc program: mountd at localhost:4242 with workerCount 1
2018-03-05 12:49:31,988 INFO org.apache.hadoop.oncrpc.SimpleTcpServer: Started listening to TCP requests at port 4242 for Rpc program: mountd at localhost:4242 with workerCount 1
2018-03-05 12:49:31,993 TRACE org.apache.hadoop.oncrpc.RpcCall: Xid:692394656, messageType:RPC_CALL, rpcVersion:2, program:100000, version:2, procedure:1, credential:(AuthFlavor:AUTH_NONE), verifier:(AuthFlavor:AUTH_NONE)
2018-03-05 12:49:31,998 FATAL org.apache.hadoop.mount.MountdBase: Failed to start the server. Cause:
java.lang.UnsupportedOperationException: Unsupported verifier flavorAUTH_SYS
at org.apache.hadoop.oncrpc.security.Verifier.readFlavorAndVerifier(Verifier.java:45)
at org.apache.hadoop.oncrpc.RpcDeniedReply.read(RpcDeniedReply.java:50)
at org.apache.hadoop.oncrpc.RpcReply.read(RpcReply.java:67)
at org.apache.hadoop.oncrpc.SimpleUdpClient.run(SimpleUdpClient.java:71)
at org.apache.hadoop.oncrpc.RpcProgram.register(RpcProgram.java:130)
at org.apache.hadoop.oncrpc.RpcProgram.register(RpcProgram.java:101)
at org.apache.hadoop.mount.MountdBase.start(MountdBase.java:83)
at org.apache.hadoop.hdfs.nfs.nfs3.Nfs3.startServiceInternal(Nfs3.java:56)
at org.apache.hadoop.hdfs.nfs.nfs3.Nfs3.startService(Nfs3.java:69)
at org.apache.hadoop.hdfs.nfs.nfs3.PrivilegedNfsGatewayStarter.start(PrivilegedNfsGatewayStarter.java:60)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at org.apache.commons.daemon.support.DaemonLoader.start(DaemonLoader.java:243)
2018-03-05 12:49:32,007 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 1{noformat}
 Reading the code comment for class Verifier, I think this bug existed since its inception
{code:java}
/**
 * Base class for verifier. Currently our authentication only supports 3 types
 * of auth flavors: {@link RpcAuthInfo.AuthFlavor#AUTH_NONE}, {@link RpcAuthInfo.AuthFlavor#AUTH_SYS},
 * and {@link RpcAuthInfo.AuthFlavor#RPCSEC_GSS}. Thus for verifier we only need to handle
 * AUTH_NONE and RPCSEC_GSS
 */
public abstract class Verifier extends RpcAuthInfo {{code}
The verifier should also handle AUTH_SYS too.",gabor.bota
HADOOP-15305,"Replace FileUtils.writeStringToFile(File, String) with (File, String, Charset) to fix deprecation warnings","FileUtils.writeStringToFile(File, String) relies on default charset and should be replaced with FileUtils.writeStringToFile(File, String, Charset).",zhenyi
HADOOP-15304,[JDK10] Migrate from com.sun.tools.doclets to the replacement,"com.sun.tools.doclets.* packages were removed in Java 10. [https://bugs.openjdk.java.net/browse/JDK-8177511]

This causes hadoop-annotations module to fail.
{noformat}
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.1:compile (default-compile) on project hadoop-annotations: Compilation failure: Compilation failure:
[ERROR] /Users/ajisaka/git/hadoop/hadoop-common-project/hadoop-annotations/src/main/java/org/apache/hadoop/classification/tools/IncludePublicAnnotationsStandardDoclet.java:[61,20] cannot find symbol
[ERROR] symbol:   method validOptions(java.lang.String[][],com.sun.javadoc.DocErrorReporter)
[ERROR] location: class com.sun.tools.doclets.standard.Standard
[ERROR] /Users/ajisaka/git/hadoop/hadoop-common-project/hadoop-annotations/src/main/java/org/apache/hadoop/classification/tools/ExcludePrivateAnnotationsStandardDoclet.java:[56,20] cannot find symbol
[ERROR] symbol:   method validOptions(java.lang.String[][],com.sun.javadoc.DocErrorReporter)
[ERROR] location: class com.sun.tools.doclets.standard.Standard
{noformat}",ajisakaa
HADOOP-15302,Enable DataNode/NameNode service plugins with Service Provider interface,"HADOOP-5257 introduced ServicePlugin capabilities for NameNode/DataNode. As of now they could be activated by configuration values. 

I propose to activate plugins with Service Provider Interface. In case of a special service file is added a jar it would be enough to add the plugin to the classpath. It would help to add optional components to NameNode/DataNode with settings the classpath.

This is the same api which could be used in java 9 to consume defined services.",elek
HADOOP-15299,Bump Hadoop's Jackson 2 dependency 2.9.x,"There are a few new CVEs open against Jackson 2.7.x. It doesn't (necessarily) mean Hadoop is vulnerable to the attack - I don't know that it is, but fixes were released for Jackson 2.8.x and 2.9.x but not 2.7.x (which we're on). We shouldn't be on an unmaintained line, regardless. HBase is already on 2.9.x, we have a shaded client now, the API changes are relatively minor and so far in my testing I haven't seen any problems. I think many of our usual reasons to hesitate upgrading this dependency don't apply.",mackrorysd
HADOOP-15297,Make S3A etag => checksum feature optional,"HADOOP-15273 shows how distcp doesn't handle non-HDFS filesystems with checksums.

Exposing Etags as checksums, HADOOP-13282, breaks workflows which back up to s3a.

Rather than revert  I want to make it an option, off by default. Once we are happy with distcp in future, we can turn it on.

Why an option? Because it lines up for a successor to distcp which saves src and dest checksums to a file and can then verify whether or not files have really changed. Currently distcp relies on dest checksum algorithm being the same as the src for incremental updates, but if either of the stores don't serve checksums, silently downgrades to not checking. 
",stevel@apache.org
HADOOP-15296,Fix a wrong link for RBF in the top page,,tasanuma0829
HADOOP-15295,Remove redundant logging related to tags from Configuration,"Remove redundant logging related to tags from Configuration.
{code}
2018-03-06 18:55:46,164 INFO conf.Configuration: Removed undeclared tags:
2018-03-06 18:55:46,237 INFO conf.Configuration: Removed undeclared tags:
2018-03-06 18:55:46,249 INFO conf.Configuration: Removed undeclared tags:
2018-03-06 18:55:46,256 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
{code}",ajayydv
HADOOP-15294,TestUGILoginFromKeytab fails on Java9,"This is the same cause as HADOOP-15291, but this time we may need to fix {{UserGroupInformation}}.
{noformat}
[ERROR] testReloginAfterFailedRelogin(org.apache.hadoop.security.TestUGILoginFromKeytab)  Time elapsed: 1.157 s  <<< ERROR!
org.apache.hadoop.security.KerberosAuthException:
Login failure for user: user1@EXAMPLE.COM javax.security.auth.login.LoginException: java.lang.NullPointerException: invalid null input(s)
	at java.base/java.util.Objects.requireNonNull(Objects.java:246)
	at java.base/javax.security.auth.Subject$SecureSet.remove(Subject.java:1172)
        ...
	at org.apache.hadoop.security.UserGroupInformation$HadoopLoginContext.logout(UserGroupInformation.java:1888)
	at org.apache.hadoop.security.UserGroupInformation.unprotectedRelogin(UserGroupInformation.java:1129)
	at org.apache.hadoop.security.UserGroupInformation.relogin(UserGroupInformation.java:1109)
	at org.apache.hadoop.security.UserGroupInformation.reloginFromKeytab(UserGroupInformation.java:1078)
	at org.apache.hadoop.security.UserGroupInformation.reloginFromKeytab(UserGroupInformation.java:1060)
	at org.apache.hadoop.security.TestUGILoginFromKeytab.testReloginAfterFailedRelogin(TestUGILoginFromKeytab.java:363)
{noformat}",tasanuma0829
HADOOP-15293,TestLogLevel fails on Java 9,"{noformat}
[INFO] Running org.apache.hadoop.log.TestLogLevel
[ERROR] Tests run: 7, Failures: 2, Errors: 0, Skipped: 0, Time elapsed: 9.805 s <<< FAILURE! - in org.apache.hadoop.log.TestLogLevel
[ERROR] testLogLevelByHttpWithSpnego(org.apache.hadoop.log.TestLogLevel)  Time elapsed: 1.179 s  <<< FAILURE!
java.lang.AssertionError: 
 Expected to find 'Unrecognized SSL message' but got unexpected exception: javax.net.ssl.SSLException: Unsupported or unrecognized SSL message
	at java.base/sun.security.ssl.SSLSocketInputRecord.handleUnknownRecord(SSLSocketInputRecord.java:416)
{noformat}",tasanuma0829
HADOOP-15292,Distcp's use of pread is slowing it down.,"Distcp currently uses positioned-reads (in RetriableFileCopyCommand#copyBytes) when the source offset is > 0. This results in unnecessary overheads (new BlockReader being created on the client-side, multiple readBlock() calls to the Datanodes, each of which requires the creation of a BlockSender and an inputstream to the ReplicaInfo).",virajith
HADOOP-15291,TestMiniKdc fails on Java 9,"{noformat}
[INFO] Running org.apache.hadoop.minikdc.TestMiniKdc
[ERROR] Tests run: 3, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 3.748 s <<< FAILURE! - in org.apache.hadoop.minikdc.TestMiniKdc
[ERROR] testKerberosLogin(org.apache.hadoop.minikdc.TestMiniKdc)  Time elapsed: 1.301 s  <<< ERROR!
javax.security.auth.login.LoginException: 
java.lang.NullPointerException: invalid null input(s)
	at java.base/java.util.Objects.requireNonNull(Objects.java:246)
	at java.base/javax.security.auth.Subject$SecureSet.remove(Subject.java:1172)
	at java.base/java.util.Collections$SynchronizedCollection.remove(Collections.java:2039)
	at jdk.security.auth/com.sun.security.auth.module.Krb5LoginModule.logout(Krb5LoginModule.java:1193)
	at java.base/javax.security.auth.login.LoginContext.invoke(LoginContext.java:732)
	at java.base/javax.security.auth.login.LoginContext.access$000(LoginContext.java:194)
	at java.base/javax.security.auth.login.LoginContext$4.run(LoginContext.java:665)
	at java.base/javax.security.auth.login.LoginContext$4.run(LoginContext.java:663)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.login.LoginContext.invokePriv(LoginContext.java:663)
	at java.base/javax.security.auth.login.LoginContext.logout(LoginContext.java:613)
	at org.apache.hadoop.minikdc.TestMiniKdc.testKerberosLogin(TestMiniKdc.java:169)
{noformat}",tasanuma0829
HADOOP-15289,FileStatus.readFields() assertion incorrect,"As covered inHBASE-20123,  ""Backup test fails against hadoop 3; "", I think the assert at the end of {{FileStatus.readFields()}} is wrong; if you run the code with assert=true against a directory, an IOE will get raised.",stevel@apache.org
HADOOP-15288,TestSwiftFileSystemBlockLocation doesn't compile,TestSwiftFileSystemBlockLocation doesn't comple after the switch to the slf4J APIs. one line fix,stevel@apache.org
HADOOP-15287,JDK9 JavaDoc build fails due to one-character underscore identifiers in hadoop-yarn-common,"{{mvn --projects hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common javadoc:javadoc}} fails.
{noformat}
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-javadoc-plugin:3.0.0-M1:javadoc (default-cli) on project hadoop-yarn-common: An error has occurred in Javadoc report generation:
[ERROR] Exit code: 1 - ./hadoop/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/Hamlet.java:50: error: as of release 9, '_' is a keyword, and may not be used as an identifier
[ERROR]   public class HTML<T extends _> extends EImp<T> implements HamletSpec.HTML {
[ERROR]                               ^
[ERROR] ./hadoop/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/Hamlet.java:92: error: as of release 9, '_' is a keyword, and may not be used as an identifier
[ERROR]       return base().$href(href)._();
[ERROR]                                 ^
...
{noformat}

FYI: https://bugs.openjdk.java.net/browse/JDK-8061549",tasanuma0829
HADOOP-15286,Remove unused imports from TestKMSWithZK.java,There are 30+ unused imports in TestKMSWithZK.java. Let's clean them up.,ajayydv
HADOOP-15283,Upgrade from findbugs 3.0.1 to spotbugs 3.1.2 in branch-2 to fix docker image build,"Not sure about other branches, but branch-2.8 pre-commit is failing at building docker images.

See [https://builds.apache.org/job/PreCommit-HDFS-Build/23274]
{noformat}
...
Step 13/31 : RUN mkdir -p /opt/findbugs &&     curl -L -s -S          https://sourceforge.net/projects/findbugs/files/findbugs/3.0.1/findbugs-noUpdateChecks-3.0.1.tar.gz/download          -o /opt/findbugs.tar.gz &&     tar xzf /opt/findbugs.tar.gz --strip-components 1 -C /opt/findbugs
 ---> Running in 59d3581d6f6c

gzip: stdin: not in gzip format
tar: Child returned status 1
tar: Error is not recoverable: exiting now
The command '/bin/sh -c mkdir -p /opt/findbugs &&     curl -L -s -S          https://sourceforge.net/projects/findbugs/files/findbugs/3.0.1/findbugs-noUpdateChecks-3.0.1.tar.gz/download          -o /opt/findbugs.tar.gz &&     tar xzf /opt/findbugs.tar.gz --strip-components 1 -C /opt/findbugs' returned a non-zero code: 2

Total Elapsed time:  11m 54s

ERROR: Docker failed to build image.
{noformat}",ajisakaa
HADOOP-15282,HADOOP-15235 broke TestHttpFSServerWebServer,"As [~xiaochen] pointed out in [this comment|https://issues.apache.org/jira/browse/HADOOP-15235?focusedCommentId=16375379&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16375379] on HADOOP-15235, it broke {{TestHttpFSServerWebServer}}:
{noformat}
2018-02-23 23:13:29,791 WARN  ServletHandler - /webhdfs/v1/
java.lang.IllegalArgumentException: Empty key
	at javax.crypto.spec.SecretKeySpec.<init>(SecretKeySpec.java:96)
	at org.apache.hadoop.security.authentication.util.Signer.computeSignature(Signer.java:93)
	at org.apache.hadoop.security.authentication.util.Signer.sign(Signer.java:59)
	at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:587)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1751)
	at org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1617)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)
	at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:582)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)
	at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:548)
	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:226)
	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1180)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:512)
	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:185)
	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1112)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
	at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:119)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)
	at org.eclipse.jetty.server.Server.handle(Server.java:534)
	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:320)
	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)
	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)
	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)
	at org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)
	at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)
	at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)
	at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)
	at java.lang.Thread.run(Thread.java:745)

java.lang.AssertionError: 
Expected :500
Actual   :200
 <Click to see difference>
{noformat}

This only affects trunk because {{TestHttpFSServerWebServer}} doesn't exist in branch-2",rkanter
HADOOP-15280,TestKMS.testWebHDFSProxyUserKerb and TestKMS.testWebHDFSProxyUserSimple fail in trunk,"I'm seeing these messages on OS X and on Linux.

{noformat}
[ERROR] Failures:
[ERROR] TestKMS.testWebHDFSProxyUserKerb:2526->doWebHDFSProxyUserTest:2625->runServer:158->runServer:176 org.apache.hadoop.security.authentication.client.AuthenticationException: Error while authenticating with endpoint: http://localhost:56112/kms/v1/keys?doAs=foo1
[ERROR] TestKMS.testWebHDFSProxyUserSimple:2531->doWebHDFSProxyUserTest:2625->runServer:158->runServer:176 org.apache.hadoop.security.authentication.client.AuthenticationException: Error while authenticating with endpoint: http://localhost:56206/kms/v1/keys?doAs=foo1 
{noformat}

as well as a [recent PreCommit-HADOOP-Build job|https://builds.apache.org/job/PreCommit-HADOOP-Build/14235/].",bharatviswa
HADOOP-15279,increase maven heap size recommendations,1G is just a bit too low for JDK8+surefire 2.20+hdfs unit tests running in parallel.  Bump it up a bit more.,aw
HADOOP-15278,log s3a at info,"since it was added, hadoop conf/log4j only logs s3a at ERROR, even though in our test/resources it logs at info. We do actually log lots of stuff useful when debugging things

Proposed: drop the log level to INFO here",stevel@apache.org
HADOOP-15277,remove .FluentPropertyBeanIntrospector from CLI operation log output,"When hadoop metrics is started, a message about bean introspection appears.
{code}
18/03/01 18:43:54 INFO beanutils.FluentPropertyBeanIntrospector: Error when creating PropertyDescriptor for public final void org.apache.commons.configuration2.AbstractConfiguration.setProperty(java.lang.String,java.lang.Object)! Ignoring this property.
{code}

When using wasb or s3a,. this message appears in the client logs, because they both start metrics

I propose to raise the log level to ERROR for that class in log4j.properties",stevel@apache.org
HADOOP-15276,branch-2 site not building after ADL troubleshooting doc added,"Toc error on the ADL troubleshooting doc from HADOOP-15090
{code}
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-site-plugin:3.5:site (default-cli) on project hadoop-azure-datalake: Error parsing 'hadoop-trunk/hadoop-tools/hadoop-azure-datalake/src/site/markdown/troubleshooting_adl.md': line [-1] Error parsing the model: Unable to execute macro in the document: toc -> [Help 1]
{code}",stevel@apache.org
HADOOP-15275,Incorrect javadoc for return type of RetryPolicy#shouldRetry,"The return type of {{RetryPolicy#shouldRetry}} has been changed from {{boolean}} to {{RetryAction}}, but the javadoc is not updated.",nandakumar131
HADOOP-15274,Move hadoop-openstack to slf4j,,zhenyi
HADOOP-15273,distcp can't handle remote stores with different checksum algorithms,"When using distcp without {{-skipcrcchecks}} . If there's a checksum mismatch between src and dest store types (e.g hdfs to s3), then the error message will talk about blocksize, even when its the underlying checksum protocol itself which is the cause for failure

bq. Source and target differ in block-size. Use -pb to preserve block-sizes during copy. Alternatively, skip checksum-checks altogether, using -skipCrc. (NOTE: By skipping checksums, one runs the risk of masking data-corruption during file-transfer.)

update:  the CRC check takes always place on a distcp upload before the file is renamed into place. *and you can't disable it then*",stevel@apache.org
HADOOP-15271,Remove unicode multibyte characters from JavaDoc,"{{mvn package -Pdist,native -Dtar -DskipTests}} fails.
{noformat}
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-javadoc-plugin:3.0.0-M1:jar (module-javadocs) on project hadoop-common: MavenReportException: Error while generating Javadoc: 
[ERROR] Exit code: 1 - javadoc: warning - The old Doclet and Taglet APIs in the packages
[ERROR] com.sun.javadoc, com.sun.tools.doclets and their implementations
[ERROR] are planned to be removed in a future JDK release. These
[ERROR] components have been superseded by the new APIs in jdk.javadoc.doclet.
[ERROR] Users are strongly recommended to migrate to the new APIs.
[ERROR] /home/centos/git/hadoop/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java:1652: error: unmappable character (0xE2) for encoding US-ASCII
[ERROR]    * closed automatically ???these the marked paths will be deleted as a result.
[ERROR]                           ^
[ERROR] /home/centos/git/hadoop/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java:1652: error: unmappable character (0x80) for encoding US-ASCII
[ERROR]    * closed automatically ???these the marked paths will be deleted as a result.
[ERROR]                            ^
[ERROR] /home/centos/git/hadoop/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java:1652: error: unmappable character (0x94) for encoding US-ASCII
[ERROR]    * closed automatically ???these the marked paths will be deleted as a result.
{noformat}
JDK9 JavaDoc cannot treat non-ascii characters due to https://bugs.openjdk.java.net/browse/JDK-8188649.",tasanuma0829
HADOOP-15269,S3 returning 400 on the directory /test/ GET of getFileStatus,"Since Monday Feb 26, I'm getting intermittent failures of getFileStatus on a directory

# file path: {{/test}} is returning 404, as expected
# directory path {{//test/}} is returning 400, so failing the entire operation

S3 Ireland. ",stevel@apache.org
HADOOP-15268,Back port HADOOP-13972 to 2.8.1 and 2.8.3,Back port the HADOOP-13972 to branch-2.8.1 and branch-2.8.3,omkarksa
HADOOP-15267,S3A multipart upload fails when SSE-C encryption is enabled,"When I enable SSE-C encryption in Hadoop 3.1 and set  fs.s3a.multipart.size to 5 Mb, storing data in AWS doesn't work anymore. For example, running the following code:
{code}
>>> df1 = spark.read.json('/home/user/people.json')
>>> df1.write.mode(""overwrite"").json(""s3a://testbucket/people.json"")
{code}
shows the following exception:
{code:java}
com.amazonaws.services.s3.model.AmazonS3Exception: The multipart upload initiate requested encryption. Subsequent part requests must include the appropriate encryption parameters.
{code}

After some investigation, I discovered that hadoop-aws doesn't send SSE-C headers in Put Object Part as stated in AWS specification: [https://docs.aws.amazon.com/AmazonS3/latest/API/mpUploadUploadPart.html]
{code:java}
If you requested server-side encryption using a customer-provided encryption key in your initiate multipart upload request, you must provide identical encryption information in each part upload using the following headers.
{code}
 
You can find a patch attached to this issue for a better clarification of the problem.

",vadmeste
HADOOP-15266, [branch-2] Upper/Lower case conversion support for group names in LdapGroupsMapping,"On most LDAP servers the user and group names are case-insensitive. When we use {{JniBasedUnixGroupsMappingWithFallback}} and have {{SSSD}} in place, it is possible to configure {{SSSD}} to force the group names to be returned in lowercase. If we use {{LdapGroupsMapping}}, we don't have any such option.

This jira proposes to introduce a new {{hadoop.security.group.mapping}} implementation based on LdapGroupsMapping which supports force lower/upper case group names.",nandakumar131
HADOOP-15265,Exclude json-smart explicitly in hadoop-auth avoid being pulled in transitively,"this is an extension of - https://issues.apache.org/jira/browse/HADOOP-14903

We need to exclude the dependency explicitly in hadoop-auth pom.xml and add the correct version so that it is not being pulled transitively. 

In Druid we use, [https://github.com/tesla/tesla-aether/blob/master/src/main/java/io/tesla/aether/TeslaAether.java] to fetch dependencies transitively, which is still pulling in wrong version of json-smart jar.
{code:java}
org.apache.hadoop:hadoop-auth:jar:2.7.3.2.6.5.0-129 -> com.nimbusds:nimbus-jose-jwt:jar:4.41.1 -> net.minidev:json-smart:jar:2.3-SNAPSHOT{code}
 

Full Stack trace 
{code:java}
 2018/02/26 03:47:22 INFO    : 2018-02-26T03:47:22,878 ERROR [main] io.druid.cli.PullDependencies - Unable to resolve artifacts for [io.druid.extensions:druid-hdfs-storage:jar:0.10.1.2.6.5.0-129 (runtime) -> [] < [ (https://repo1.maven.org/maven2/, releases+snapshots),  (http://nexus-private.hortonworks.com/nexus/content/groups/public, releases+snapshots),  (http://nexus-private.hortonworks.com/nexus/content/groups/public, releases+snapshots),  (http://nexus-private.hortonworks.com/nexus/content/groups/public, releases+snapshots),  (https://metamx.artifactoryonline.com/metamx/pub-libs-releases-local, releases+snapshots)]].
2018/02/26 03:47:22 INFO    : org.eclipse.aether.resolution.DependencyResolutionException: Failed to collect dependencies at io.druid.extensions:druid-hdfs-storage:jar:0.10.1.2.6.5.0-129 -> org.apache.hadoop:hadoop-client:jar:2.7.3.2.6.5.0-129 -> org.apache.hadoop:hadoop-common:jar:2.7.3.2.6.5.0-129 -> org.apache.hadoop:hadoop-auth:jar:2.7.3.2.6.5.0-129 -> com.nimbusds:nimbus-jose-jwt:jar:4.41.1 -> net.minidev:json-smart:jar:2.3-SNAPSHOT
2018/02/26 03:47:22 INFO    : 	at org.eclipse.aether.internal.impl.DefaultRepositorySystem.resolveDependencies(DefaultRepositorySystem.java:380) ~[aether-impl-0.9.0.M2.jar:?]
2018/02/26 03:47:22 INFO    : 	at io.tesla.aether.internal.DefaultTeslaAether.resolveArtifacts(DefaultTeslaAether.java:289) ~[tesla-aether-0.0.5.jar:0.0.5]
2018/02/26 03:47:22 INFO    : 	at io.druid.cli.PullDependencies.downloadExtension(PullDependencies.java:350) [druid-services-0.10.1.2.6.5.0-129.jar:0.10.1.2.6.5.0-129]
2018/02/26 03:47:22 INFO    : 	at io.druid.cli.PullDependencies.run(PullDependencies.java:249) [druid-services-0.10.1.2.6.5.0-129.jar:0.10.1.2.6.5.0-129]
2018/02/26 03:47:22 INFO    : 	at io.druid.cli.Main.main(Main.java:108) [druid-services-0.10.1.2.6.5.0-129.jar:0.10.1.2.6.5.0-129]
2018/02/26 03:47:22 INFO    : Caused by: org.eclipse.aether.collection.DependencyCollectionException: Failed to collect dependencies at io.druid.extensions:druid-hdfs-storage:jar:0.10.1.2.6.5.0-129 -> org.apache.hadoop:hadoop-client:jar:2.7.3.2.6.5.0-129 -> org.apache.hadoop:hadoop-common:jar:2.7.3.2.6.5.0-129 -> org.apache.hadoop:hadoop-auth:jar:2.7.3.2.6.5.0-129 -> com.nimbusds:nimbus-jose-jwt:jar:4.41.1 -> net.minidev:json-smart:jar:2.3-SNAPSHOT
2018/02/26 03:47:22 INFO    : 	at org.eclipse.aether.internal.impl.DefaultDependencyCollector.collectDependencies(DefaultDependencyCollector.java:292) ~[aether-impl-0.9.0.M2.jar:?]
2018/02/26 03:47:22 INFO    : 	at org.eclipse.aether.internal.impl.DefaultRepositorySystem.resolveDependencies(DefaultRepositorySystem.java:342) ~[aether-impl-0.9.0.M2.jar:?]
2018/02/26 03:47:22 INFO    : 	... 4 more
2018/02/26 03:47:22 INFO    : Caused by: org.eclipse.aether.resolution.ArtifactDescriptorException: Failed to read artifact descriptor for net.minidev:json-smart:jar:2.3-SNAPSHOT
2018/02/26 03:47:22 INFO    : 	at org.apache.maven.repository.internal.DefaultArtifactDescriptorReader.loadPom(DefaultArtifactDescriptorReader.java:335) ~[maven-aether-provider-3.1.1.jar:3.1.1]
2018/02/26 03:47:22 INFO    : 	at org.apache.maven.repository.internal.DefaultArtifactDescriptorReader.readArtifactDescriptor(DefaultArtifactDescriptorReader.java:217) ~[maven-aether-provider-3.1.1.jar:3.1.1]
2018/02/26 03:47:22 INFO    : 	at org.eclipse.aether.internal.impl.DefaultDependencyCollector.process(DefaultDependencyCollector.java:461) ~[aether-impl-0.9.0.M2.jar:?]
2018/02/26 03:47:22 INFO    : 	at org.eclipse.aether.internal.impl.DefaultDependencyCollector.process(DefaultDependencyCollector.java:573) ~[aether-impl-0.9.0.M2.jar:?]
2018/02/26 03:47:22 INFO    : 	at org.eclipse.aether.internal.impl.DefaultDependencyCollector.process(DefaultDependencyCollector.java:573) ~[aether-impl-0.9.0.M2.jar:?]
2018/02/26 03:47:22 INFO    : 	at org.eclipse.aether.internal.impl.DefaultDependencyCollector.process(DefaultDependencyCollector.java:573) ~[aether-impl-0.9.0.M2.jar:?]
2018/02/26 03:47:22 INFO    : 	at org.eclipse.aether.internal.impl.DefaultDependencyCollector.process(DefaultDependencyCollector.java:573) ~[aether-impl-0.9.0.M2.jar:?]
2018/02/26 03:47:22 INFO    : 	at org.eclipse.aether.internal.impl.DefaultDependencyCollector.collectDependencies(DefaultDependencyCollector.java:261) ~[aether-impl-0.9.0.M2.jar:?]
2018/02/26 03:47:22 INFO    : 	at org.eclipse.aether.internal.impl.DefaultRepositorySystem.resolveDependencies(DefaultRepositorySystem.java:342) ~[aether-impl-0.9.0.M2.jar:?]
2018/02/26 03:47:22 INFO    : 	... 4 more
2018/02/26 03:47:22 INFO    : Caused by: org.eclipse.aether.resolution.ArtifactResolutionException: Could not transfer artifact net.minidev:json-smart:pom:2.3-SNAPSHOT from/to  (https://metamx.artifactoryonline.com/metamx/pub-libs-releases-local): Failed to transfer https://metamx.artifactoryonline.com/metamx/pub-libs-releases-local/net/minidev/json-smart/2.3-SNAPSHOT/json-smart-2.3-SNAPSHOT.pom. Error code 409, Conflict

{code}
 ",nishantbangarwa
HADOOP-15264,"AWS ""shaded"" SDK 1.11.271 is pulling in netty 4.1.17","The latest versions of the AWS Shaded SDK are declaring a dependency on netty 4.1.17
{code}
[INFO] +- org.apache.hadoop:hadoop-aws:jar:3.2.0-SNAPSHOT:compile
[INFO] |  \- com.amazonaws:aws-java-sdk-bundle:jar:1.11.271:compile
[INFO] |     +- io.netty:netty-codec-http:jar:4.1.17.Final:compile
[INFO] |     +- io.netty:netty-codec:jar:4.1.17.Final:compile
[INFO] |     +- io.netty:netty-handler:jar:4.1.17.Final:compile
[INFO] |     +- io.netty:netty-buffer:jar:4.1.17.Final:compile
[INFO] |     +- io.netty:netty-common:jar:4.1.17.Final:compile
[INFO] |     +- io.netty:netty-transport:jar:4.1.17.Final:compile
[INFO] |     \- io.netty:netty-resolver:jar:4.1.17.Final:compile
{code}

We either exclude these or roll back HADOOP-15040.",stevel@apache.org
HADOOP-15263,hadoop cloud-storage module to mark hadoop-common as provided; add azure-datalake,"Reviewing hadoop-cloud-storage module for use

* we should cut out hadoop-common so that if something downstream is already doing the heavy lifting of excluding it to get jackson & guava in sync, it's not sneaking back in.
* and add azure-datalake",stevel@apache.org
HADOOP-15262,AliyunOSS: move files under a directory in parallel when rename a directory,"Currently, rename() operation renames files in series. This will be slow if a directory contains many files. So we can improve this by rename files in parallel.",wujinhu
HADOOP-15261,Upgrade commons-io from 2.4 to 2.5,"Hi, after analyzing hadoop-common-project\hadoop-minikdc\pom.xml, we found that Hadoop depends on org.apache.kerby:kerb-simplekdc 1.0.1, which transitivity introduced commons-io:2.5. 
At the same time, hadoop directly depends on a older version of commons-io:2.4. By further look into the source code, these two versions of commons-io have many different features. The dependency conflict problem brings high risks of ""NotClassDefFoundError:"" or ""NoSuchMethodError"" issues at runtime. Please notice this problem. Maybe upgrading commons-io from 2.4 to 2.5 is a good choice. Hope this report can help you. Thanks!
 ",pandamonkey
HADOOP-15259,Provide docker file for the development builds,An other use case for using docker image is creating custom docker image (base image + custom hadoop build). The custom image could be used to test easily the hadoop build on external dockerized cluster (eg. Kubernetes),elek
HADOOP-15258,Create example docker-compose file for documentations,"An other user case for docker is to use it in the documentation. For example in the HA documentation we can provide an example docker-compose file and configuration with all the required settings to getting started easily with an HA cluster.

1. I would add an example to a documetation page
2. It will use the hadoop3 image (which contains latest hadoop3) as the user of the documentation may not build a hadoop",elek
HADOOP-15257,Provide example docker compose file for developer builds,"This issue is about creating example docker-compose files which use the latest build from the hadoop-dist directory.

These docker-compose files would help to run a specific hadoop cluster based on the latest custom build without the need to build customized docker image (with mounting hadoop fro hadoop-dist to the container",elek
HADOOP-15256,Create docker images for latest stable hadoop3 build,Similar to the hadoop2 image we can provide a developer hadoop image which contains the latest hadoop from the binary release.,elek
HADOOP-15255,Upper/Lower case conversion support for group names in LdapGroupsMapping,"On most LDAP servers the user and group names are case-insensitive. When we use {{JniBasedUnixGroupsMappingWithFallback}} and have {{SSSD}} in place, it is possible to configure {{SSSD}} to force the group names to be returned in lowercase. If we use {{LdapGroupsMapping}}, we don't have any such option.

This jira proposes to introduce a new {{hadoop.security.group.mapping}} implementation based on LdapGroupsMapping which supports force lower/upper case group names.",nandakumar131
HADOOP-15254,Correct the wrong word spelling 'intialize',The correct wording of 'intialize' should be 'initialize'.,zhenyi
HADOOP-15253,Should update maxQueueSize when refresh call queue,"When calling {{dfsadmin -refreshCallQueue}} to update CallQueue instance, {{maxQueueSize}} should also be updated.
In case of changing CallQueue instance to FairCallQueue, the length of each queue in FairCallQueue would be 1/priorityLevels of original length of DefaultCallQueue. So it would be helpful for us to set the length of callqueue to a proper value.
",tao jie
HADOOP-15252,Checkstyle version is not compatible with IDEA's checkstyle plugin,"After upgrading to the latest IDEA the IDE throws error messages in every few minutes like
{code:java}
The Checkstyle rules file could not be parsed.
SuppressionCommentFilter is not allowed as a child in Checker
The file has been blacklisted for 60s.{code}
This is caused by some backward incompatible changes in checkstyle source code:
 [http://checkstyle.sourceforge.net/releasenotes.html]
 * 8.1: Make SuppressionCommentFilter and SuppressWithNearbyCommentFilter children of TreeWalker.
 * 8.2: remove FileContentsHolder module as FileContents object is available for filters on TreeWalker in TreeWalkerAudit Event.

IDEA uses checkstyle 8.8

We should upgrade our checkstyle version to be compatible with IDEA's checkstyle plugin.
 Also it's a good time to upgrade maven-checkstyle-plugin as well to brand new 3.0.",boky01
HADOOP-15251,Backport HADOOP-13514 (surefire upgrade) to branch-2,"Tests in branch-2 are not running reliably in Jenkins, and due to SUREFIRE-524, these are not being cleaned up properly (see HADOOP-15153).

Upgrading to a more recent version of the surefire plugin will help make the problem easier to address in branch-2",chris.douglas
HADOOP-15250,Split-DNS MultiHomed Server Network Cluster Network IPC Client Bind Addr Wrong,"We run our Hadoop clusters with two networks attached to each node. These network are as follows a server network that is firewalled with firewalld allowing inbound traffic: only SSH and things like Knox and Hiveserver2 and the HTTP YARN RM/ATS and MR History Server. The second network is the cluster network on the second network interface this uses Jumbo frames and is open no restrictions and allows all cluster traffic to flow between nodes. 

 

To resolve DNS within the Hadoop Cluster we use DNS Views via BIND so if the traffic is originating from nodes with cluster networks we return the internal DNS record for the nodes. This all works fine with all the multi-homing features added to Hadoop 2.x

 Some logic around views:

a. The internal view is used by cluster machines when performing lookups. So hosts on the cluster network should get answers from the internal view in DNS
b. The external view is used by non-local-cluster machines when performing lookups. So hosts not on the cluster network should get answers from the external view in DNS



 

So this brings me to our problem. We created some firewall rules to allow inbound traffic from each clusters server network to allow distcp to occur. But we noticed a problem almost immediately that when YARN attempted to talk to the Remote Cluster it was binding outgoing traffic to the cluster network interface which IS NOT routable. So after researching the code we noticed the following in NetUtils.java and Client.java 

Basically in Client.java it looks as if it takes whatever the hostname is and attempts to bind to whatever the hostname is resolved to. This is not valid in a multi-homed network with one routable interface and one non routable interface. After reading through the java.net.Socket documentation it is valid to perform socket.bind(null) which will allow the OS routing table and DNS to send the traffic to the correct interface. I will also attach the nework traces and a test patch for 2.7.x and 3.x code base. I have this test fix below in my Hadoop Test Cluster.

Client.java:

      
|/*|
| | * Bind the socket to the host specified in the principal name of the|
| | * client, to ensure Server matching address of the client connection|
| | * to host name in principal passed.|
| | */|
| |InetSocketAddress bindAddr = null;|
| |if (ticket != null && ticket.hasKerberosCredentials()) {|
| |KerberosInfo krbInfo =|
| |remoteId.getProtocol().getAnnotation(KerberosInfo.class);|
| |if (krbInfo != null) {|
| |String principal = ticket.getUserName();|
| |String host = SecurityUtil.getHostFromPrincipal(principal);|
| |// If host name is a valid local address then bind socket to it|
| |{color:#FF0000}*InetAddress localAddr = NetUtils.getLocalInetAddress(host);*{color}|
|{color:#FF0000} ** {color}|if (localAddr != null) {|
| |this.socket.setReuseAddress(true);|
| |if (LOG.isDebugEnabled()) {|
| |LOG.debug(""Binding "" + principal + "" to "" + localAddr);|
| |}|
| |*{color:#FF0000}bindAddr = new InetSocketAddress(localAddr, 0);{color}*|
| *{color:#FF0000}{color}* |*{color:#FF0000}}{color}*|
| |}|
| |}|

 

So in my Hadoop 2.7.x Cluster I made the following changes and traffic flows correctly out the correct interfaces:

 

diff --git a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/CommonConfigurationKeys.java b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/CommonConfigurationKeys.java

index e1be271..c5b4a42 100644

--- a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/CommonConfigurationKeys.java

+++ b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/CommonConfigurationKeys.java

@@ -305,6 +305,9 @@

   public static final String  IPC_CLIENT_FALLBACK_TO_SIMPLE_AUTH_ALLOWED_KEY = ""ipc.client.fallback-to-simple-auth-allowed"";

   public static final boolean IPC_CLIENT_FALLBACK_TO_SIMPLE_AUTH_ALLOWED_DEFAULT = false;

 

+  public static final String  IPC_CLIENT_NO_BIND_LOCAL_ADDR_KEY = ""ipc.client.nobind.local.addr"";

+  public static final boolean IPC_CLIENT_NO_BIND_LOCAL_ADDR_DEFAULT = false;

+

   public static final String IPC_CLIENT_CONNECT_MAX_RETRIES_ON_SASL_KEY =

     ""ipc.client.connect.max.retries.on.sasl"";

   public static final int    IPC_CLIENT_CONNECT_MAX_RETRIES_ON_SASL_DEFAULT = 5;

diff --git a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Client.java b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Client.java

index a6f4eb6..7bfddb7 100644

--- a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Client.java

+++ b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Client.java

@@ -129,7 +129,9 @@ public static void setCallIdAndRetryCount(int cid, int rc) {

 

   private final int connectionTimeout;

 

+

   private final boolean fallbackAllowed;

+  private final boolean noBindLocalAddr;

   private final byte[] clientId;

   

   final static int CONNECTION_CONTEXT_CALL_ID = -3;

@@ -642,7 +644,11 @@ private synchronized void setupConnection() throws IOException {

               InetAddress localAddr = NetUtils.getLocalInetAddress(host);

               if (localAddr != null) {

                 this.socket.setReuseAddress(true);

-                this.socket.bind(new InetSocketAddress(localAddr, 0));

+                if (noBindLocalAddr) {

+                  this.socket.bind(null);

+ } else {

+                  this.socket.bind(new InetSocketAddress(localAddr, 0));

+                }

               }

             }

           }",ajayydv
HADOOP-15247,Move commons-net up to 3.6,"Bump up commons-net to latest version, which appears to be 3.6

Uses: netutils, ftpfs, dns utils in registry",stevel@apache.org
HADOOP-15242,Fix typos in hadoop-functions.sh,"In the comments of this file, there is a reference to ""hadoop-layout{color:#d04437}s{color}.sh"", but all the other references in the actual code are for a ""hadoop-layout.sh"" file.  Similarly, the example file is named ""hadoop-layout.sh.example"".",rchiang
HADOOP-15240,"ITestS3AEmptyDirectory.testParallelJobsToAdjacentPaths failure, FileAlreadyExistsException","Transient {{ITestS3AEmptyDirectory.testParallelJobsToAdjacentPaths}} failure; {{FileAlreadyExistsException}} during the setup phase of the test -writing out the work to
{{org.apache.hadoop.fs.FileAlreadyExistsException: File already exists: file:/Users/stevel/Projects/hadoop-trunk/hadoop-tools/hadoop-aws/target/build/test:fork-0002/job_200707120153_0002/_temporary/1/_temporary/attempt_200707120153_0002_m_000000_0/part-m-00000}}

There's something wrong here as that path is a FileOutputCommitter workingDir path, not a partitioned committer. 

Hypotheses: some filesystem caching in the test run is stopping the right committer from being picked up.

It's possible that this has been happening for a while, but only some cleanup failure from a previous run has caught it.

Proposed
* add checks in the AbstractIT test to verify the right committer is picked up in the task context by probing for the working dir: file:// for staging, */__magic/* for magic.
* and verify that success file is there.




",stevel@apache.org
HADOOP-15239,S3ABlockOutputStream.flush() be no-op when stream closed,"when you call flush() on a closed S3A output stream, you get a stack trace. 

This can cause problems in code with race conditions across threads, e.g. FLINK-8543. 

we could make it log@warn ""stream closed"" rather than raise an IOE. It's just a hint, after all.",gabor.bota
HADOOP-15238,ADLS to support per-store configuration: Yetus patch checker,"This is a Yetus only JIRA created to have Yetus review the HADOOP-13972 patch as a .patch file, as the review PR https://github.com/apache/hadoop/pull/339 is stopping this happening in HADOOP-13972

Reviews should go into the PR/other task",ssonker
HADOOP-15236,Fix typo in RequestHedgingProxyProvider and RequestHedgingRMFailoverProxyProvider,"Typo 'configred' in RequestHedgingProxyProvider and RequestHedgingRMFailoverProxyProvider.
{noformat}
 * standbys. Once it receive a response from any one of the configred proxies,
{noformat}",gabor.bota
HADOOP-15235,Authentication Tokens should use HMAC instead of MAC,"We currently use {{MessageDigest}} to compute a ""SHA"" MAC for signing Authentication Tokens.  Firstly, what ""SHA"" maps to is dependent on the JVM and Cryptography Provider.  While they _should_ do something reasonable, it's probably a safer idea to pick a specific algorithm.  It looks like the Oracle JVM picks SHA-1; though something like SHA-256 would be better.

In any case, it would also be better to use an HMAC algorithm instead.

Changing from SHA-1 to SHA-256 or MAC to HMAC won't generate equivalent signatures, so this would normally be an incompatible change because the server wouldn't accept previous tokens it issued with the older algorithm.  However, Authentication Tokens are used as a cheaper shortcut for Kerberos, so it's expected for users to also have Kerberos credentials; in this case, the Authentication Token will be rejected, but it will silently retry using Kerberos, and get an updated token.  So this should all be transparent to the user.

And finally, the code where we verify a signature uses a non-constant-time comparison, which could be subject to timing attacks.  I believe it would be quite difficult in this case to do so, but we're probably better off using a constant-time comparison.",rkanter
HADOOP-15234,Throw meaningful message on null when initializing KMSWebApp,"During KMS startup, if the {{keyProvider}} is null, it will NPE inside KeyProviderExtension.
{noformat}
java.lang.NullPointerException
	at org.apache.hadoop.crypto.key.KeyProviderExtension.<init>(KeyProviderExtension.java:43)
	at org.apache.hadoop.crypto.key.CachingKeyProvider.<init>(CachingKeyProvider.java:93)
	at org.apache.hadoop.crypto.key.kms.server.KMSWebApp.contextInitialized(KMSWebApp.java:170)
{noformat}

We're investigating the exact scenario that could lead to this, but the NPE and log around it can be improved.",zhenyi
HADOOP-15229,Add FileSystem builder-based openFile() API to match createFile(),"Replicate HDFS-1170 and HADOOP-14365 with an API to open files.

A key requirement of this is not HDFS, it's to put in the fadvise policy for working with object stores, where getting the decision to do a full GET and TCP abort on seek vs smaller GETs is fundamentally different: the wrong option can cost you minutes. S3A and Azure both have adaptive policies now (first backward seek), but they still don't do it that well.

Columnar formats (ORC, Parquet) should be able to say ""fs.input.fadvise"" ""random"" as an option when they open files; I can imagine other options too.

The Builder model of [~eddyxu] is the one to mimic, method for method. Ideally with as much code reuse as possible",stevel@apache.org
HADOOP-15225,mvn javadoc:test-javadoc goal throws cannot find symbol,"{code:java}
hadoop/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestReflectionUtils.java:28: error: cannot find symbol
[WARNING] import static org.hamcrest.CoreMatchers.containsString;
[WARNING] ^
[WARNING] symbol:   static containsString
[WARNING] location: class{code}
This happens because mockito-all includes Hamcrest classes but a different version. Let's see TestReflectionUtils as an example:
{{import static org.hamcrest.CoreMatchers.containsString; }} will result in error.
 Somehow mvn javadoc:test-javadoc will find Mockito's CoreMatchers class on the classpath which has no containsString method.
 From Mockito 2 the mockito-all is discontinued so HADOOP-14178 will solve this.

Once HADOOP-14178 is resolved this can be closed as well.",boky01
HADOOP-15223,Replace Collections.EMPTY* with empty* when available,The use of {{Collections.EMPTY_SET}} and {{Collections.EMPTY_MAP}} often causes unchecked assignment and it should be replaced with {{Collections.emptySet()}} and {{Collections.emptyMap()}}. ,zhenyi
HADOOP-15221,Swift driver should not fail if JSONUtils reports UnknowPropertyException,"org.apache.hadoop.fs.swift.exceptions.SwiftJsonMarshallingException: 
org.codehaus.jackson.map.exc.UnrecognizedPropertyException: Unrecognized field 
We know system is keep involving and new field will be added. However, for compatibility point of view, extra field added to json should be logged but may not lead to failure from the robustness point of view.",airbots
HADOOP-15220,Über-jira: S3a phase V: Hadoop 3.2 features,"Über-jira for S3A work for Hadoop 3.2.x

The items from HADOOP-14831 which didn't get into Hadoop-3.1, and anything else",stevel@apache.org
HADOOP-15218,Make Hadoop compatible with Guava 22.0+,"Deprecated HostAndPort#getHostText method was deleted in Guava 22.0 and new HostAndPort#getHost method is not available before Guava 20.0.

This patch implements getHost(HostAndPort) method that extracts host from HostAndPort#toString value.

This is a little hacky, that's why I'm not sure if it worth to merge this patch, but it could be nice if Hadoop will be Guava-neutral.

With this patch Hadoop can be built against latest Guava v24.0.",medb
HADOOP-15217,FsUrlConnection does not handle paths with spaces,"When _FsUrlStreamHandlerFactory_ is registered with _java.net.URL_ (ex: when Spark is initialized), it breaks URLs with spaces (even though they are properly URI-encoded). I traced the problem down to _FSUrlConnection.connect()_ method. It naively gets the path from the URL, which contains encoded spaces, and pases it to _org.apache.hadoop.fs.Path(String)_ constructor. This is not correct, because the docs clearly say that the string must NOT be encoded. Doing so causes double encoding within the Path class (ie: %20 becomes %2520). 

See attached JUnit test. 

This test case mimics an issue I ran into when trying to use Commons Configuration 1.9 AFTER initializing Spark. Commons Configuration uses URL class to load configuration files, but Spark installs _FsUrlStreamHandlerFactory_, which hits this issue. For now, we are using an AspectJ aspect to ""patch"" the bytecode at load time to work-around the issue. 

The real fix is quite simple. All you need to do is replace this line in _org.apache.hadoop.fs.FsUrlConnection.connect()_:
        is = fs.open(new Path(url.getPath()));

with this line:

     is = fs.open(new Path(url.*toUri()*.getPath()));

URI.getPath() will correctly decode the path, which is what is expected by _org.apache.hadoop.fs.Path(String)_ constructor.

 ",zvenczel
HADOOP-15215,s3guard set-capacity command to fail on read/write of 0,"the command {{hadoop s3guard set-capacity -read 0  s3a://bucket}}  will get all the way to the AWS SDK before it's rejected; if you pass in a value of -1 we fail fast.

The CLI check should really be failing on <= 0, not < 0.

You still get a stack trace, so it's not that important.",gabor.bota
HADOOP-15214,Make Hadoop compatible with Guava 21.0,There are only 3 changes that need to be done to make Hadoop compile with Guava 21.0 dependency,medb
HADOOP-15212,Add independent secret manager method for logging expired tokens,"{{AbstractDelegationTokenSecretManager#removeExpiredToken}} has two phases.  First phase synchronizes to collect expired tokens.  Second phase loops over the collected tokens to log them while not holding the monitor.

HDFS-13112 needs to acquire the namesystem lock during the second logging phase, which requires splitting the method apart to allow a method override.",daryn
HADOOP-15209,DistCp to eliminate needless deletion of files under already-deleted directories,"DistCP issues a delete(file) request even if is underneath an already deleted directory. This generates needless load on filesystems/object stores, and, if the store throttles delete, can dramatically slow down the delete operation.

If the distcp delete operation can build a history of deleted directories, then it will know when it does not need to issue those deletes.

Care is needed here to make sure that whatever structure is created does not overload the heap of the process.",stevel@apache.org
HADOOP-15208,DistCp to offer -xtrack <path> option to save src/dest filesets as alternative to delete(),"There are opportunities to improve distcp delete performance and scalability with object stores, but you need to test with production datasets to determine if the optimizations work, don't run out of memory, etc.

By adding the option to save the sequence files of source, dest listings, people (myself included) can experiment with different strategies before trying to commit one which doesn't scale",stevel@apache.org
HADOOP-15206,BZip2 drops and duplicates records when input split size is small,"BZip2 can drop and duplicate record when input split file is small. I confirmed that this issue happens when the input split size is between 1byte and 4bytes.

I am seeing the following 2 problem behaviors.

 

1. Drop record:

BZip2 skips the first record in the input file when the input split size is small

 

Set the split size to 3 and tested to load 100 records (0, 1, 2..99)
{code:java}
2018-02-01 10:52:33,502 INFO  [Thread-17] mapred.TestTextInputFormat (TestTextInputFormat.java:verifyPartitions(317)) - splits[1]=file:/work/count-mismatch2/hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/target/test-dir/TestTextInputFormat/test.bz2:3+3 count=99{code}
> The input format read only 99 records but not 100 records

 

2. Duplicate Record:

2 input splits has same BZip2 records when the input split size is small

 

Set the split size to 1 and tested to load 100 records (0, 1, 2..99)

 
{code:java}
2018-02-01 11:18:49,309 INFO [Thread-17] mapred.TestTextInputFormat (TestTextInputFormat.java:verifyPartitions(318)) - splits[3]=file /work/count-mismatch2/hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/target/test-dir/TestTextInputFormat/test.bz2:3+1 count=99
2018-02-01 11:18:49,310 WARN [Thread-17] mapred.TestTextInputFormat (TestTextInputFormat.java:verifyPartitions(308)) - conflict with 1 in split 4 at position 8
{code}
 

I experienced this error when I execute Spark (SparkSQL) job under the following conditions:

* The file size of the input files are small (around 1KB)

* Hadoop cluster has many slave nodes (able to launch many executor tasks)

 ",tanakahda
HADOOP-15204,Add Configuration API for parsing storage sizes,"Hadoop has a lot of configurations that specify memory and disk size. This JIRA proposes to add an API like {{Configuration.getStorageSize}} which will allow users
 to specify units like KB, MB, GB etc. This is JIRA is inspired by HADOOP-8608 and Ozone. Adding {{getTimeDuration}} support was a great improvement for ozone code base, this JIRA hopes to do the same thing for configs that deal with disk and memory usage.",anu
HADOOP-15203,Support composite trusted channel resolver that supports both whitelist and blacklist,support composite trusted channel resolver that supports both whitelist and blacklist,ajayydv
HADOOP-15202, Deprecate CombinedIPWhiteList to use CombinedIPList ," Deprecate CombinedIPWhiteList to use CombinedIPList. 
Orignal suggestion from [~xyao]",ajayydv
HADOOP-15200,Missing DistCpOptions constructor breaks downstream DistCp projects in 3.0,"Post HADOOP-14267, the constructor for DistCpOptions was removed and will break any project using it for java based implementation/usage of DistCp. This JIRA would track next steps required to reconcile/fix this incompatibility. ",kshukla
HADOOP-15198,Correct the spelling in CopyFilter.java,"configuration is misspelled as ""configuratoin"" in the javadoc for CopyFilter.java

{code}
  /**
   * Public factory method which returns the appropriate implementation of
   * CopyFilter.
   *
   * @param conf DistCp configuratoin
   * @return An instance of the appropriate CopyFilter
   */
  public static CopyFilter getCopyFilter(Configuration conf) {
{code}",msingh
HADOOP-15197,Remove tomcat from the Hadoop-auth test bundle,"We have switched KMS and HttpFS from tomcat to jetty in 3.0. There appears to have some left over tests in Hadoop-auth which were for used for KMS / HttpFS coverage.

We should cleanup the test accordingly.",xiaochen
HADOOP-15196,Zlib decompression fails when file having trailing garbage,"*When file has trailing garbage gzip will ignore.*
{noformat}
gzip -d 2018011309-js.rishenglipin.com.gz

gzip: 2018011309-js.rishenglipin.com.gz: decompression OK, trailing garbage ignored

{noformat}
 *when we use same file and decompress,we got following.*
{noformat}
2018-01-13 14:23:43,151 | WARN  | task-result-getter-3 | Lost task 0.0 in stage 345.0 (TID 5686, node-core-gyVYT, executor 3): java.io.IOException: unknown compression method

        at org.apache.hadoop.io.compress.zlib.ZlibDecompressor.inflateBytesDirect(Native Method)

        at org.apache.hadoop.io.compress.zlib.ZlibDecompressor.decompress(ZlibDecompressor.java:225)

        at org.apache.hadoop.io.compress.DecompressorStream.decompress(DecompressorStream.java:91)

        at org.apache.hadoop.io.compress.DecompressorStream.read(DecompressorStream.java:85)

{noformat}",brahmareddy
HADOOP-15195,"With SELinux enabled, directories mounted with start-build-env.sh may not be accessible.","On a system with SELinux enabled, e.g., Red Hat Linux 7, CentOS 7, Fedora, the host directories - with the Hadoop code and the maven .m2 - mounted with start-build-env.sh may not be accessible to the container. This precludes Hadoop development on such systems.",rybkine
HADOOP-15191,Add Private/Unstable BulkDelete operations to supporting object stores for DistCP,"Large scale DistCP with the -delete option doesn't finish in a viable time because of the final CopyCommitter doing a 1 by 1 delete of all missing files. This isn't randomized (the list is sorted), and it's throttled by AWS.

If bulk deletion of files was exposed as an API, distCP would do 1/1000 of the REST calls, so not get throttled.

Proposed: add an initially private/unstable interface for stores, {{BulkDelete}} which declares a page size and offers a {{bulkDelete(List<Path>)}} operation for the bulk deletion.",stevel@apache.org
HADOOP-15190,Use Jacoco to generate Unit Test coverage reports,"Currently Hadoop is using maven-clover2-plugin for code coverage, which is outdated. And Atlassian open-sourced clover last year so license cannot be purchased although we can switch to use the license-free version called ""openclover"".

This Jira is to replace clover with Jacoco, which is actively maintained by the community.

  ",onpduo
HADOOP-15189,backport HADOOP-15039 to branch-2 and branch-3,,unclegen
HADOOP-15188,"azure datalake AzureADAuthenticator failing, no error info provided","Get a failure in ADLS client, but nothing useful in terms of failure description

{code}
DEBUG oauth2.AzureADAuthenticator: AADToken: starting to fetch token using client creds for client ID <omitted>
DEBUG store.HttpTransport: HTTPRequest,Failed,cReqId:<omitted>,lat:127370,err:HTTP0(null),Reqlen:0,Resplen:0,token_ns:<omitted>,sReqId:null,path:<omitted>,qp:op=GETFILESTATUS&tooid=true&api-version=2016-11-01
{code}
so: we had a failure but the response code is 0, error(null); ""something happened but we don't know what""

Looks like this log message is in the ADLS SDK, and can be translated like this.
{code}
String logline =
  ""HTTPRequest,"" + outcome +
  "",cReqId:"" + opts.requestid +
  "",lat:"" + Long.toString(resp.lastCallLatency) +
  "",err:"" + error +
  "",Reqlen:"" + length +
  "",Resplen:"" + respLength +
  "",token_ns:"" + Long.toString(resp.tokenAcquisitionLatency) +
  "",sReqId:"" + resp.requestId +
  "",path:"" + path +
  "",qp:"" + queryParams.serialize();

{code}

It looks like whatever code tries to parse the JSON response from the OAuth service couldn't make sense of the response, and we end up with nothing back. 

Not sure what can be done in hadoop to handle this, except maybe provide more diags on request failures.
",asikaria
HADOOP-15187,Remove ADL mock test dependency on REST call invoked from Java SDK ,Cleanup unit test which mocks REST calls invoked within dependency SDK.,vishwajeet.dusane
HADOOP-15186,Allow Azure Data Lake SDK dependency version to be set on the command line,For backward/forward release of Java SDK compatibility test against Hadoop driver. Allow Azure Data Lake Java SDK dependency version to override from command line.,vishwajeet.dusane
HADOOP-15185,Update adls connector to use the current version of ADLS SDK,ADL Store SDK version in Hadoop trunk is a coupel of bugfix releases behind the current release from ADL. This Jira is to update the SDK version to current.,asikaria
HADOOP-15184,Add GitHub pull request template,There are many GitHub pull requests which do not follow the contribution guideline (e.g. creating a PR without filing a issue in ASF JIRA). I'd like to add a GitHub pull request template to avoid such things.,ajisakaa
HADOOP-15181,Typo in SecureMode.md,"https://github.com/apache/hadoop/blame/08332e12d055d85472f0c9371fefe9b56bfea1ed/hadoop-common-project/hadoop-common/src/site/markdown/SecureMode.md#L575

""&amp;lt;"" should be unescaped.",masatana
HADOOP-15180,branch-2 : daemon processes' sysout overwrites 'ulimit -a' in daemon's out file,"Whenever the balancer starts, it will redirect the sys out to the out log file.  And balancer writes the system output to the log file, at the same time  script also will try to append ulimit output. 
{noformat}
 # capture the ulimit output
    if [ ""true"" = ""$starting_secure_dn"" ]; then
      echo ""ulimit -a for secure datanode user $HADOOP_SECURE_DN_USER"" >> $log
      # capture the ulimit info for the appropriate user
      su --shell=/bin/bash $HADOOP_SECURE_DN_USER -c 'ulimit -a' >> $log 2>&1
    elif [ ""true"" = ""$starting_privileged_nfs"" ]; then
        echo ""ulimit -a for privileged nfs user $HADOOP_PRIVILEGED_NFS_USER"" >> $log
        su --shell=/bin/bash $HADOOP_PRIVILEGED_NFS_USER -c 'ulimit -a' >> $log 2>&1
    else
      echo ""ulimit -a for user $USER"" >> $log
      ulimit -a >> $log 2>&1
    fi
    sleep 3;
    if ! ps -p $! > /dev/null ; then
      exit 1
    fi

{noformat}
But the problem is first few lines of ulimit is overridding by the log of balancer.

{noformat}
vm1:/opt/install/hadoop/namenode/sbin # cat /opt/HA/AIH283/install/hadoop/namenode/logs/hadoop-root-balancer-vm1.out
Time Stamp               Iteration#  Bytes Already Moved  Bytes Left To Move  Bytes Being Moved
The cluster is balanced. Exiting...
Jan 9, 2018 6:26:26 PM            0                  0 B                 0 B                0 B
Jan 9, 2018 6:26:26 PM   Balancing took 3.446 seconds
x memory size         (kbytes, -m) 13428300
open files                      (-n) 1024
pipe size            (512 bytes, -p) 8
POSIX message queues     (bytes, -q) 819200
real-time priority              (-r) 0
stack size              (kbytes, -s) 8192
cpu time               (seconds, -t) unlimited
max user processes              (-u) 127350
virtual memory          (kbytes, -v) 15992160
file locks                      (-x) unlimited
{noformat}",ranith
HADOOP-15178,Generalize NetUtils#wrapException to handle other subclasses with String Constructor,"NetUtils#wrapException returns an IOException if exception passed to it is not of type SocketException,EOFException,NoRouteToHostException,SocketTimeoutException,UnknownHostException,ConnectException,BindException.
By default, it  should always return instance (subclass of IOException) of same type unless a String constructor is not available.",ajayydv
HADOOP-15177,Update the release year to 2018,"The release year needs to be updated.
{noformat}
$ find . -name ""pom.xml"" | xargs grep -n 2017
./hadoop-project/pom.xml:34:    <release-year>2017</release-year>{noformat}",bharatviswa
HADOOP-15176,Enhance IAM Assumed Role support in S3A client,"Followup HADOOP-15141 with

* Code to generate basic AWS json policies somewhat declaratively (no hand coded strings)
* Tests to simulate users with different permissions down the path of a single bucket
* test-driven changes to S3A client to handle user without full write up the FS tree
* move the new authenticator into the s3a sub-package ""auth"", where we can put more auth stuff (that base s3a package is getting way too big)",stevel@apache.org
HADOOP-15175,DN Reg can Fail when principal doesn't contain hostname and floatingIP is configured.,"Configure principal without hostname (i.e hdfs/hadoop@HADOOP.com)
Configure floatingIP
Start Cluster.
Here DN will fail to register as it can take IP which is not in ""/etc/hosts"".",brahmareddy
HADOOP-15172,Fix the javadoc warning in WriteOperationHelper.java,"Fix the following javadoc warning in WriteOperationHelper.java

{code}
[WARNING] /Users/msingh/code/work/apache/trunk/trunk2/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/WriteOperationHelper.java:87: warning: no @param for conf
[WARNING] protected WriteOperationHelper(S3AFileSystem owner, Configuration conf) {
[WARNING] ^
{code}",msingh
HADOOP-15171,native ZLIB decompressor produces 0 bytes on the 2nd call; also incorrrectly handles some zlib errors,"While reading some ORC file via direct buffers, Hive gets a 0-sized buffer for a particular compressed segment of the file. We narrowed it down to Hadoop native ZLIB codec; when the data is copied to heap-based buffer and the JDK Inflater is used, it produces correct output. Input is only 127 bytes so I can paste it here.
All the other (many) blocks of the file are decompressed without problems by the same code.

{noformat}
2018-01-13T02:47:40,815 TRACE [IO-Elevator-Thread-0 (1515637158315_0079_1_00_000000_0)] encoded.EncodedReaderImpl: Decompressing 127 bytes to dest buffer pos 524288, limit 786432
2018-01-13T02:47:40,816  WARN [IO-Elevator-Thread-0 (1515637158315_0079_1_00_000000_0)] encoded.EncodedReaderImpl: The codec has produced 0 bytes for 127 bytes at pos 0, data hash 1719565039: [e3 92 e1 62 66 60 60 10 12 e5 98 e0 27 c4 c7 f1 e8 12 8f 40 c3 7b 5e 89 09 7f 6e 74 73 04 30 70 c9 72 b1 30 14 4d 60 82 49 37 bd e7 15 58 d0 cd 2f 31 a1 a1 e3 35 4c fa 15 a3 02 4c 7a 51 37 bf c0 81 e5 02 12 13 5a b6 9f e2 04 ea 96 e3 62 65 b8 c3 b4 01 ae fd d0 72 01 81 07 87 05 25 26 74 3c 5b c9 05 35 fd 0a b3 03 50 7b 83 11 c8 f2 c3 82 02 0f 96 0b 49 34 7c fa ff 9f 2d 80 01 00
2018-01-13T02:47:40,816  WARN [IO-Elevator-Thread-0 (1515637158315_0079_1_00_000000_0)] encoded.EncodedReaderImpl: Fell back to JDK decompressor with memcopy; got 155 bytes
{noformat}

Hadoop version is based on 3.1 snapshot.
The size of libhadoop.so is 824403 bytes, and libgplcompression is 78273 FWIW. Not sure how to extract versions from those. ",ljain
HADOOP-15170,Add symlink support to FileUtil#unTarUsingJava ,"Now that JDK7 or later is required, we can leverage java.nio.Files.createSymbolicLink in FileUtil.unTarUsingJava to support archives that contain symbolic links.
",ajayydv
HADOOP-15169,"""hadoop.ssl.enabled.protocols"" should be considered in httpserver2","As of now *hadoop.ssl.enabled.protocols""* will not take effect for all the http servers( only Datanodehttp server will use this config).",brahmareddy
HADOOP-15168,Add kdiag tool to hadoop command,,bharatviswa
HADOOP-15167,[viewfs] ls will fail when user doesn't exist,"
Have Secure federated cluster with atleast two nameservices
Configure viewfs related configs 

 When we run the {{ls}} cmd in HDFS client ,we will call the method: org.apache.hadoop.fs.viewfs.ViewFileSystem.InternalDirOfViewFs#getFileStatus

 it will try to get the group of the kerberos user. If the node has not this user, it fails. 

Throws the following and exits.UserGroupInformation#getPrimaryGroupName

{code}
if (groups.isEmpty()) {
      throw new IOException(""There is no primary group for UGI "" + this);
    }
{code}",brahmareddy
HADOOP-15166,CLI MiniCluster fails with ClassNotFoundException o.a.h.yarn.server.timelineservice.collector.TimelineCollectorManager,"Following CLIMiniCluster.md.vm to start minicluster fails due to:
{code}
Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.yarn.server.timelineservice.collector.TimelineCollectorManager
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:338)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	... 62 more
{code}

",jira.shegalov
HADOOP-15164,DataNode Replica Trash,DataNode Replica Trash will allow administrators to recover from a recent delete request that resulted in catastrophic loss of user data. This is achieved by placing all invalidated blocks in a replica trash on the datanode before completely purging them from the system. The design doc is attached here.,hanishakoneru
HADOOP-15163,Fix S3ACommitter documentation,The current version of the documentation uses {{fs.s3a.committer.tmp.path}} instead of {{fs.s3a.committer.staging.tmp.path}}.,andrioni
HADOOP-15161,s3a: Stream and common statistics missing from metrics,"Input stream statistics aren't being passed through to metrics once merged. Also, the following ""common statistics"" are not being incremented or tracked by metrics:
{code}
OP_APPEND
OP_CREATE
OP_CREATE_NON_RECURSIVE
OP_DELETE
OP_GET_CONTENT_SUMMARY
OP_GET_FILE_CHECKSUM
OP_GET_STATUS
OP_MODIFY_ACL_ENTRIES
OP_OPEN
OP_REMOVE_ACL
OP_REMOVE_ACL_ENTRIES
OP_REMOVE_DEFAULT_ACL
OP_SET_ACL
OP_SET_OWNER
OP_SET_PERMISSION
OP_SET_TIMES
OP_TRUNCATE
{code}

Most of those make sense, but we can easily add OP_CREATE (and it's non-recursive cousin), OP_DELETE, OP_OPEN.",mackrorysd
HADOOP-15160,Confusing text in http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/Compatibility.html,"The text in wire formats, policy, is confusing.

First, there are two subsections with the same heading:

The following changes to a .proto file SHALL be considered incompatible:
The following changes to a .proto file SHALL be considered incompatible:

Second, one of the items listed under the first of those two headings seems like it is a compatible change, not an incompatible change:

Delete an optional field as long as the optional field has reasonable defaults to allow deletions",templedf
HADOOP-15158,AliyunOSS: Supports role based credential in URL,"Currently, AliyunCredentialsProvider supports credential by configuration(core-site.xml). Sometimes, admin wants to create different temporary credential(key/secret/token) for different roles so that one role cannot read data that belongs to another role.
So, our code should support pass in the URI when creates an XXXCredentialsProvider so that we can get user info(role) from the URI",wujinhu
HADOOP-15157,Zookeeper authentication related properties to support CredentialProviders,"The hadoop.zk.auth and ha.zookeeper.auth properties currently support either a plain-text authentication info (in scheme:value format), or a @/path/to/file notation which points to a plain-text file.

This ticket proposes that the hadoop.zk.auth and ha.zookeeper.auth properties can be retrieved via the CredentialProviderAPI that's been configured using the credential.provider.path, with fallback provided to the clear-text value or @/path/to/file notation.",grepas
HADOOP-15156,backport HADOOP-15086 rename fix to branch-2,backport HADOOP-15086 (rename fix) to branch-2,tmarquardt
HADOOP-15155,Error in javadoc of ReconfigurableBase#reconfigureProperty,"There is an error in javadoc of reconfigureProperty#reconfigurePropertyImpl

{code}
   * This method cannot be overridden, subclasses should instead override
   * reconfigureProperty.
{code}
should change to
{code}
   * This method cannot be overridden, subclasses should instead override
   * reconfigurePropertyImpl.
{code}",ajayydv
HADOOP-15154,Abstract new method assertCapability for StreamCapabilities testing,"From Steve's [comment|https://issues.apache.org/jira/browse/HADOOP-15149?focusedCommentId=16306806&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16306806]:
bq.  it'd have been cleaner for the asserts to have been one in a assertCapability(key, StreamCapabilities subject, bool outcome) and had it throw meaningful exceptions on a failure

We can consider abstract such a method to a test util class and use it for all {{StreamCapabilities}} tests as needed.",zvenczel
HADOOP-15153,[branch-2.8] Increase heap memory to avoid the OOM in pre-commit,"Refernce:
https://builds.apache.org/job/PreCommit-HDFS-Build/22528/consoleFull
https://builds.apache.org/job/PreCommit-HDFS-Build/22528/artifact/out/branch-mvninstall-root.txt

{noformat}
[ERROR] unable to create new native thread -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/OutOfMemoryError
{noformat}",brahmareddy
HADOOP-15152,Typo in javadoc of ReconfigurableBase#reconfigurePropertyImpl,"There is a typo in javadoc of ReconfigurableBase#reconfigurePropertyImpl

{code}
   * Subclasses must override this. This method applies the change to
   * all internal data structures derived from the configuration property
   * that is being changed. If this object owns other Reconfigurable objects
   * reconfigureProperty should be called recursively to make sure that
   * to make sure that the configuration of these objects is updated.
{code}",nandakumar131
HADOOP-15151,MapFile.fix creates a wrong index file in case of block-compressed data file.,Index file created with MapFile.fix for an ordered block-compressed data file does not allow to find values for keys existing in the data file via the MapFile.get method.,rybkine
HADOOP-15150,"in FsShell, UGI params should be overidden through env vars(-D arg)","org.apache.hadoop.security.UserGroupInformation#ensureInitialized,will always get the configure from the configuration files.*So that, -D args will not take effect*.
{code}
  private static void ensureInitialized() {
    if (conf == null) {
      synchronized(UserGroupInformation.class) {
        if (conf == null) { // someone might have beat us
          initialize(new Configuration(), false);
        }
      }
    }
  }
{code}",brahmareddy
HADOOP-15149,CryptoOutputStream should implement StreamCapabilities,"Running some tests with HBase on HDFS with encryption, we noticed that CryptoOutputStream doesn't implement StreamCapabilities.

Specifically, we have a HdfsDataOutputStream wrapping a CryptoOutputStream. The calls to {{hasCapability}} on the HDOS will always return false, even though the COS could be wrapping something that supports it, as evidenced by the implementation of hsync and hflush here.",xiaochen
HADOOP-15148,Improve DataOutputByteBuffer,"* Use ArrayDeque instead of LinkedList
* Replace an ArrayList that was being used as a queue with ArrayDeque
* Improve write single byte method to hard-code sizes and save time

{quote}
Resizable-array implementation of the Deque interface. Array deques have no capacity restrictions; they grow as necessary to support usage. They are not thread-safe; in the absence of external synchronization, they do not support concurrent access by multiple threads. Null elements are prohibited. This class is *likely to be* ... *faster than LinkedList when used as a queue.*
{quote}",belugabehr
HADOOP-15146,Remove DataOutputByteBuffer,I can't seem to find any references to {{DataOutputByteBuffer}} maybe it should be deprecated or simply removed?,belugabehr
HADOOP-15145,Remove the CORS related code in JMXJsonServlet,"{{JMXJsonServlet.java}} using hardcoded value for ""Access-Control-Allow-Origin"" and this is added in HADOOP-11385.
But this change is not required after YARN-4009. YARN-4009 added one new filter for CORS support.

Please refer [CORS|https://hadoop.apache.org/docs/r2.7.2/hadoop-project-dist/hadoop-common/HttpAuthentication.html] support in HttpAuthentication document",surendrasingh
HADOOP-15143,NPE due to Invalid KerberosTicket in UGI,"{code}
java.lang.NullPointerException
	at org.apache.hadoop.security.UserGroupInformation.fixKerberosTicketOrder(UserGroupInformation.java:1170) 
	at org.apache.hadoop.security.UserGroupInformation.reloginFromKeytab(UserGroupInformation.java:1247) 
	at org.apache.hadoop.security.UserGroupInformation.checkTGTAndReloginFromKeytab(UserGroupInformation.java:1157) 
{code}

It could be related to jdk issue
http://hg.openjdk.java.net/jdk8u/jdk8u-dev/jdk/rev/fd0e0898721c",msingh
HADOOP-15141,Support IAM Assumed roles in S3A,"Add the ability to use assumed roles in S3A

* Add a property fs.s3a.assumed.role.arn for the ARN of the assumed role
* add a new provider which grabs that and other properties and then creates a {{STSAssumeRoleSessionCredentialsProvider}} from it.
* This also needs to support building up its own list of aws credential  providers, from a different property; make the changes to S3AUtils for that
* Tests
* docs
* and have the AwsProviderList forward closeable to it.
* Get picked up automatically by DDB/s3guard",stevel@apache.org
HADOOP-15139,[Umbrella] Improvements and fixes for Hadoop shaded client work ,"In HADOOP-11656, we have made great progress in splitting out third-party dependencies from shaded hadoop client jar (hadoop-client-api), put runtime dependencies in hadoop-client-runtime, and have shaded version of hadoop-client-minicluster for test. However, there are still some left work for this feature to be fully completed:
- We don't have a comprehensive documentation to guide downstream projects/users to use shaded JARs instead of previous JARs
- We should consider to wrap up hadoop tools (distcp, aws, azure) to have shaded version
- More issues could be identified when shaded jars are adopted in more test and production environment, like HADOOP-15137.

Let's have this umbrella JIRA to track all efforts that left to improve hadoop shaded client effort.

CC [~busbey], [~bharatviswa] and [~vinodkv].",bharatviswa
HADOOP-15137,ClassNotFoundException: org.apache.hadoop.yarn.server.api.DistributedSchedulingAMProtocol when using hadoop-client-minicluster,"I'd like to use hadoop-client-minicluster for hadoop downstream project, but I encounter the following exception when starting hadoop minicluster.  And I check the hadoop-client-minicluster, it indeed does not have this class. Is this something that is missing when packaging the published jar ?

{code}
java.lang.NoClassDefFoundError: org/apache/hadoop/yarn/server/api/DistributedSchedulingAMProtocol

	at java.lang.ClassLoader.defineClass1(Native Method)
	at java.lang.ClassLoader.defineClass(ClassLoader.java:763)
	at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)
	at java.net.URLClassLoader.defineClass(URLClassLoader.java:467)
	at java.net.URLClassLoader.access$100(URLClassLoader.java:73)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:368)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:362)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:361)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at org.apache.hadoop.yarn.server.MiniYARNCluster.createResourceManager(MiniYARNCluster.java:851)
	at org.apache.hadoop.yarn.server.MiniYARNCluster.serviceInit(MiniYARNCluster.java:285)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
{code}",bharatviswa
HADOOP-15133,[JDK9] Ignore com.sun.javadoc.* and com.sun.tools.* in animal-sniffer-maven-plugin to compile with Java 9,"com.sun.javadoc and com.sun.tools are internal APIs and are not included in java18 profile, so signature check fails with JDK9.
{noformat}
$ mvn clean install -DskipTests -DskipShade
(snip)
[INFO] --- animal-sniffer-maven-plugin:1.16:check (signature-check) @ hadoop-annotations ---
[INFO] Checking unresolved references to org.codehaus.mojo.signature:java18:1.0
[ERROR] /Users/ajisaka/git/hadoop/hadoop-common-project/hadoop-annotations/src/main/java/org/apache/hadoop/classification/tools/RootDocProcessor.java:56: Undefined reference: com.sun.javadoc.RootDoc
(snip)
{noformat}",ajisakaa
HADOOP-15129,Datanode caches namenode DNS lookup failure and cannot startup,"On startup, the Datanode creates an InetSocketAddress to register with each namenode. Though there are retries on connection failure throughout the stack, the same InetSocketAddress is reused.

InetSocketAddress is an interesting class, because it resolves DNS names to IP addresses on construction, and it is never refreshed. Hadoop re-creates an InetSocketAddress in some cases just in case the remote IP has changed for a particular DNS name: https://issues.apache.org/jira/browse/HADOOP-7472.

Anyway, on startup, you cna see the Datanode log: ""Namenode...remains unresolved"" -- referring to the fact that DNS lookup failed.

{code:java}
2017-11-02 16:01:55,115 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2017-11-02 16:01:55,153 WARN org.apache.hadoop.hdfs.DFSUtilClient: Namenode for null remains unresolved for ID null. Check your hdfs-site.xml file to ensure namenodes are configured properly.
2017-11-02 16:01:55,156 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2017-11-02 16:01:55,169 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to cluster-32f5-m:8020 starting to offer service
{code}

The Datanode then proceeds to use this unresolved address, as it may work if the DN is configured to use a proxy. Since I'm not using a proxy, it forever prints out this message:


{code:java}
2017-12-15 00:13:40,712 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: cluster-32f5-m:8020
2017-12-15 00:13:45,712 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: cluster-32f5-m:8020
2017-12-15 00:13:50,712 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: cluster-32f5-m:8020
2017-12-15 00:13:55,713 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: cluster-32f5-m:8020
2017-12-15 00:14:00,713 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: cluster-32f5-m:8020

{code}

Unfortunately, the log doesn't contain the exception that triggered it, but the culprit is actually in IPC Client: https://github.com/apache/hadoop/blob/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Client.java#L444.

This line was introduced in https://issues.apache.org/jira/browse/HADOOP-487 to give a clear error message when somebody mispells an address.

However, the fix in HADOOP-7472 doesn't apply here, because that code happens in Client#getConnection after the Connection is constructed.

My proposed fix (will attach a patch) is to move this exception out of the constructor and into a place that will trigger HADOOP-7472's logic to re-resolve addresses. If the DNS failure was temporary, this will allow the connection to succeed. If not, the connection will fail after ipc client retries (default 10 seconds worth of retries).

I want to fix this in ipc client rather than just in Datanode startup, as this fixes temporary DNS issues for all of Hadoop.",karthik palaniappan
HADOOP-15128,TestViewFileSystem tests are broken in trunk,The fix in Hadoop-10054 seems to have caused a test failure. Please take a look. Thanks [~eyang] for reporting this.,hanishakoneru
HADOOP-15124,Slow FileSystem.Statistics counters implementation,"While profiling 1TB TeraGen job on Hadoop 2.8.2 cluster (Google Dataproc, 2 workers, GCS connector) I saw that FileSystem.Statistics code paths Wall time is 5.58% and CPU time is 26.5% of total execution time.

After switching FileSystem.Statistics implementation to LongAdder, consumed Wall time decreased to 0.006% and CPU time to 0.104% of total execution time.

Total job runtime decreased from 66 mins to 61 mins.

These results are not conclusive, because I didn't benchmark multiple times to average results, but regardless of performance gains switching to LongAdder simplifies code and reduces its complexity.",medb
HADOOP-15123,KDiag tries to load krb5.conf from KRB5CCNAME instead of KRB5_CONFIG,"If Kerberos credential cache file location is overridden in environment, then KDiag tries to read its value for Kerberos configuration. For example,

{code:java}
# export KRB5CCNAME=/tmp/krb5cc_1001
# hadoop org.apache.hadoop.security.KDiag
...
...
== Locating Kerberos configuration file ==

Setting kerberos path from environment variable KRB5CCNAME: ""/tmp/krb5cc_1001""
Kerberos configuration file = /tmp/krb5cc_1001
17/12/16 04:06:19 ERROR security.KDiag: java.util.UnknownFormatConversionException: Conversion = '�'
java.util.UnknownFormatConversionException: Conversion = '�'
{code}

Expected Behavior:
1. Kerberos configuration file location should be read from KRB5_CONFIG env. variable instead of KRB5CCNAME. Source: [MIT KRB5 doc|https://web.mit.edu/kerberos/krb5-1.12/doc/admin/conf_files/krb5_conf.html]",vrathor-hw
HADOOP-15122,Lock down version of doxia-module-markdown plugin,"Since HADOOP-14364 we have a SNAPSHOT dependency in the main pom.xml:

{code}
+            <dependency>
+              <groupId>org.apache.maven.doxia</groupId>
+              <artifactId>doxia-module-markdown</artifactId>
+              <version>1.8-SNAPSHOT</version>
+            </dependency>
{code}

Most probably because some feature was missing from last released doxia markdown module.

-I propose to lock down the version and use a fixed instance from the snapshot version. -

UPDATE: in the meantime doxia 1.8 has been released. Snapshot artifacts could not been dowloaded any more:

404: https://repository.apache.org/snapshots/org/apache/maven/doxia/doxia-module-markdown/1.8-SNAPSHOT/doxia-module-markdown-1.8-SNAPSHOT.jar

IMHO Hadoop can be built without changing it to 1.8 fix version.",elek
HADOOP-15121,Encounter NullPointerException when using DecayRpcScheduler,"I set ipc.8020.scheduler.impl to org.apache.hadoop.ipc.DecayRpcScheduler, but got excetion in namenode:
{code}
2017-12-15 15:26:34,662 ERROR impl.MetricsSourceAdapter (MetricsSourceAdapter.java:getMetrics(202)) - Error getting metrics from source DecayRpcSchedulerMetrics2.ipc.8020
java.lang.NullPointerException
        at org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy.getMetrics(DecayRpcScheduler.java:781)
        at org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.getMetrics(MetricsSourceAdapter.java:199)
        at org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.updateJmxCache(MetricsSourceAdapter.java:182)
        at org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.getMBeanInfo(MetricsSourceAdapter.java:155)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getNewMBeanClassName(DefaultMBeanServerInterceptor.java:333)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:319)
        at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
        at org.apache.hadoop.metrics2.util.MBeans.register(MBeans.java:66)
        at org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.startMBeans(MetricsSourceAdapter.java:222)
        at org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.start(MetricsSourceAdapter.java:100)
        at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.registerSource(MetricsSystemImpl.java:268)
        at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:233)
        at org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy.registerMetrics2Source(DecayRpcScheduler.java:709)
        at org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy.<init>(DecayRpcScheduler.java:685)
        at org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy.getInstance(DecayRpcScheduler.java:693)
        at org.apache.hadoop.ipc.DecayRpcScheduler.<init>(DecayRpcScheduler.java:236)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
        at org.apache.hadoop.ipc.CallQueueManager.createScheduler(CallQueueManager.java:102)
        at org.apache.hadoop.ipc.CallQueueManager.<init>(CallQueueManager.java:76)
        at org.apache.hadoop.ipc.Server.<init>(Server.java:2612)
        at org.apache.hadoop.ipc.RPC$Server.<init>(RPC.java:958)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.<init>(ProtobufRpcEngine.java:374)
        at org.apache.hadoop.ipc.ProtobufRpcEngine.getServer(ProtobufRpcEngine.java:349)
        at org.apache.hadoop.ipc.RPC$Builder.build(RPC.java:800)
at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.<init>(NameNodeRpcServer.java:415)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.createRpcServer(NameNode.java:755)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:697)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:905)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:884)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1610)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1678)
{code}
It seems that {{metricsProxy}} in DecayRpcScheduler should initiate its {{delegate}} field in its Initialization method
",tao jie
HADOOP-15117,open(PathHandle) contract test should be exhaustive for default options,"The current {{AbstractContractOpenTest}} covers many, but not all of the permutations of the default {{HandleOpt}}. It could also be refactored to be clearer as documentation",chris.douglas
HADOOP-15115,CLONE - Remove aspectj dependency,"Per discussion on HDFS-2261 and Nicholas' suggestion, we should remove the aspectj dependencies for the moment till the AOP fault-injection tests are fixed to work with maven.",kasha
HADOOP-15114,Add closeStreams(...) to IOUtils,Add closeStreams(...) in IOUtils. Originally suggested by [Jason Lowe|https://issues.apache.org/jira/browse/HDFS-12881?focusedCommentId=16288320&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16288320].,ajayydv
HADOOP-15113,NPE in S3A getFileStatus: null instrumentation on using closed instance,"NPE in getFileStatus in a downstream test of mine; s3a ireland
{{PathMetadata pm = metadataStore.get(path, needEmptyDirectoryFlag);. }}


Something up with the bucket config?",stevel@apache.org
HADOOP-15112,create-release didn't sign artifacts,"While building the 3.0.0 RC1, I had to re-invoke Maven because the create-release script didn't deploy signatures to Nexus. Looking at the repo (and my artifacts), it seems like ""sign"" didn't run properly.

I lost my create-release output, but I noticed that it will log and continue rather than abort in some error conditions. This might have caused my lack of signatures. IMO it'd be better to explicitly fail in these situations.",eddyxu
HADOOP-15111,AliyunOSS: backport HADOOP-14993 to branch-2,"Do a bulk listing off all entries under a path in one single operation, there is no need to recursively walk the directory tree.

Updates:

- override listFiles and listLocatedStatus by using bulk listing
- some minor updates in hadoop-aliyun index.md",unclegen
HADOOP-15110,Gauges are getting logged in exceptions from AutoRenewalThreadForUserCreds,"*scenario*:
-----------------

While Running the renewal command for principal it's printing the direct objects for *renewalFailures *and *renewalFailuresTotal*

{noformat}
bin> ./hdfs dfs -ls /
2017-12-12 12:31:50,910 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2017-12-12 12:31:52,312 WARN security.UserGroupInformation: Exception encountered while running the renewal command for principal_name. (TGT end time:1513070122000, renewalFailures: org.apache.hadoop.metrics2.lib.MutableGaugeInt@1bbb43eb,renewalFailuresTotal: org.apache.hadoop.metrics2.lib.MutableGaugeLong@424a0549)
ExitCodeException exitCode=1: kinit: KDC can't fulfill requested option while renewing credentials

        at org.apache.hadoop.util.Shell.runCommand(Shell.java:994)
        at org.apache.hadoop.util.Shell.run(Shell.java:887)
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)
        at org.apache.hadoop.security.UserGroupInformation$1.run(UserGroupInformation.java:1067)
        at java.lang.Thread.run(Thread.java:745)
{noformat}

*Expected Result*:
it's should be user understandable value.",gelixin
HADOOP-15109,TestDFSIO -read -random doesn't work on file sized 4GB,"TestDFSIO -read -random throws IllegalArgumentException on 4GB file. The cause is:

{code:java}
private long nextOffset(long current) {
      if(skipSize == 0)
        return rnd.nextInt((int)(fileSize));
      if(skipSize > 0)
        return (current < 0) ? 0 : (current + bufferSize + skipSize);
      // skipSize < 0
      return (current < 0) ? Math.max(0, fileSize - bufferSize) :
                             Math.max(0, current + skipSize);
    }
  }
{code}

When {color:#d04437}_filesize_{color} exceeds signed int, (int)(filesize) will be negative and cause Random.nextInt throws  IllegalArgumentException(""n must be positive"").
",ajayydv
HADOOP-15107,Stabilize/tune S3A committers; review correctness & docs,"I'm writing about the paper on the committers, one which, being a proper paper, requires me to show the committers work.

# define the requirements of a ""Correct"" committed job (this applies to the FileOutputCommitter too)
# show that the Staging committer meets these requirements (most of this is implicit in that it uses the V1 FileOutputCommitter to marshall .pendingset lists from committed tasks to the final destination, where they are read and committed.
# Show the magic committer also works.



",stevel@apache.org
HADOOP-15106,FileSystem::open(PathHandle) should throw a specific exception on validation failure,"Callers of {{FileSystem::open(PathHandle)}} cannot distinguish between I/O errors and an invalid handle. The signature should include a specific, checked exception for this case.",chris.douglas
HADOOP-15104,AliyunOSS: change the default value of max error retry,"Currently, default number of times we should retry errors is 20,  however, oss sdk retry delay is       

{code:java}
long delay = (long)Math.pow(2, retries) * 0.3
{code}
 when one error occurs. So, if we retry 20 times, sleep time will be about 3.64 days and it is unacceptable. So we should change the default behavior.




",wujinhu
HADOOP-15098,TestClusterTopology#testChooseRandom fails intermittently,"Flaky test failure:
{code:java}
java.lang.AssertionError
Error
Not choosing nodes randomly
Stack Trace
java.lang.AssertionError: Not choosing nodes randomly
at org.apache.hadoop.net.TestClusterTopology.testChooseRandom(TestClusterTopology.java:170)
{code}
",zvenczel
HADOOP-15093,Deprecation of yarn.resourcemanager.zk-address is undocumented,"""yarn.resourcemanager.zk-address"" was deprecated in 2.9.x and moved to ""hadoop.zk.address"". However this doesn't appear in Deprecated Properties. 

Additionally, the Configuration base class doesn't auto-translate from ""yarn.resourcemanager.zk-address"" to ""hadoop.zk.address"". Only the sub-class YarnConfiguration does the translation. 

Also, the 2.9+ Resource Manager HA documentation still refers to the use of ""yarn.resourcemanager.zk-address"".

https://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/ResourceManagerHA.html",ajayydv
HADOOP-15092,Proxy failures during NamenodeWebHdfsMethods are not logged ,"When GetDelegationToken http request fails with proxy issue, there is no logs from NameNode to indicate it's a proxy issue. Below is the only log. IOExceptions are logged but AuthorizationException are not logged. It will be helpful to log proxy failures from JspHelper getUGI().

{code}
2017-12-05 13:05:02,045 INFO  namenode.GetDelegationTokenServlet (GetDelegationTokenServlet.java:doGet(56)) - Request for token received with no authentication from 172.26.93.73
{code}

",prabhu joseph
HADOOP-15090,Add ADL troubleshooting doc,"Add a troubleshooting section/doc to the ADL docs based on our experiences.

this should not be a substitute for improving the diagnostics/fixing the error messages. ",stevel@apache.org
HADOOP-15086,NativeAzureFileSystem file rename is not atomic,"When multiple threads rename files to the same target path, more than 1 threads can succeed. It's because check and copy file in `rename` is not atomic.

I would expect it's atomic just like HDFS.",tmarquardt
HADOOP-15085,Output streams closed with IOUtils suppressing write errors,"There are a few places in hadoop-common that are closing an output stream with IOUtils.cleanupWithLogger like this:
{code}
  try {
    ...write to outStream...
  } finally {
    IOUtils.cleanupWithLogger(LOG, outStream);
  }
{code}
This suppresses any IOException that occurs during the close() method which could lead to partial/corrupted output without throwing a corresponding exception.  The code should either use try-with-resources or explicitly close the stream within the try block so the exception thrown during close() is properly propagated as exceptions during write operations are.",jim_brennan
HADOOP-15084,Create docker images for latest stable hadoop2 build,"HADOOP-15083 provides an empty runner container without the hadoop. The issue is creating a docker image based on the HADOOP-15083 image with adding the latest stable hadoop2 binary release to it.

It also conains an example docker-compose file to run it.

This image should be uploaded to the dockerhub and could be used for example configurations in the documentation.",elek
HADOOP-15083,Create base image for running hadoop in docker containers,,elek
HADOOP-15082,add AbstractContractRootDirectoryTest test for mkdir / ; wasb to implement the test,"I managed to get a stack trace on an older version of WASB with some coding doing a mkdir(new Path(""/""))....some of the ranger parentage checks didn't handle that specific case.

# Add a new root Fs contract test for this operation
# Have WASB implement the test suite as an integration test.
# if the test fails shows a problem fix",stevel@apache.org
HADOOP-15080,"Aliyun OSS: update oss sdk from 2.8.1 to 2.8.3 to remove its dependency on Cat-x ""json-lib""","Cat-X dependency on org.json via derived json-lib. OSS SDK has a dependency on json-lib. In LEGAL-245, the org.json library (from which json-lib may be derived) is released under a [category-x|https://www.apache.org/legal/resolved.html#json] license.",sammi
HADOOP-15079,ITestS3AFileOperationCost#testFakeDirectoryDeletion failing after OutputCommitter patch,"I see this test failing with ""object_delete_requests expected:<1> but was:<2>"". I printed stack traces whenever this metric was incremented, and found the root cause to be that innerMkdirs is now causing two calls to delete fake directories when it previously caused only one. It is called once inside createFakeDirectory, and once directly inside innerMkdirs later:

{code}
        at org.apache.hadoop.fs.s3a.S3AInstrumentation.incrementCounter(S3AInstrumentation.java:454)
        at org.apache.hadoop.fs.s3a.S3AFileSystem.incrementStatistic(S3AFileSystem.java:1108)
        at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$deleteObjects$8(S3AFileSystem.java:1369)
        at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:313)
        at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:279)
        at org.apache.hadoop.fs.s3a.S3AFileSystem.deleteObjects(S3AFileSystem.java:1366)
        at org.apache.hadoop.fs.s3a.S3AFileSystem.removeKeys(S3AFileSystem.java:1625)
        at org.apache.hadoop.fs.s3a.S3AFileSystem.deleteUnnecessaryFakeDirectories(S3AFileSystem.java:2634)
        at org.apache.hadoop.fs.s3a.S3AFileSystem.finishedWrite(S3AFileSystem.java:2599)
        at org.apache.hadoop.fs.s3a.S3AFileSystem.putObjectDirect(S3AFileSystem.java:1498)
        at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$createEmptyObject$11(S3AFileSystem.java:2684)
        at org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:108)
        at org.apache.hadoop.fs.s3a.Invoker.lambda$retry$3(Invoker.java:259)
        at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:313)
        at org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:255)
        at org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:230)
        at org.apache.hadoop.fs.s3a.S3AFileSystem.createEmptyObject(S3AFileSystem.java:2682)
        at org.apache.hadoop.fs.s3a.S3AFileSystem.createFakeDirectory(S3AFileSystem.java:2657)
        at org.apache.hadoop.fs.s3a.S3AFileSystem.innerMkdirs(S3AFileSystem.java:2021)
        at org.apache.hadoop.fs.s3a.S3AFileSystem.mkdirs(S3AFileSystem.java:1956)
        at org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:2305)
        at org.apache.hadoop.fs.contract.AbstractFSContractTestBase.mkdirs(AbstractFSContractTestBase.java:338)
        at org.apache.hadoop.fs.s3a.ITestS3AFileOperationCost.testFakeDirectoryDeletion(ITestS3AFileOperationCost.java:209)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
        at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
        at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
        at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
        at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
        at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74
{code}

{code}
        at org.apache.hadoop.fs.s3a.S3AInstrumentation.incrementCounter(S3AInstrumentation.java:454)
        at org.apache.hadoop.fs.s3a.S3AFileSystem.incrementStatistic(S3AFileSystem.java:1108)
        at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$deleteObjects$8(S3AFileSystem.java:1369)
        at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:313)
        at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:279)
        at org.apache.hadoop.fs.s3a.S3AFileSystem.deleteObjects(S3AFileSystem.java:1366)
        at org.apache.hadoop.fs.s3a.S3AFileSystem.removeKeys(S3AFileSystem.java:1625)
        at org.apache.hadoop.fs.s3a.S3AFileSystem.deleteUnnecessaryFakeDirectories(S3AFileSystem.java:2634)
        at org.apache.hadoop.fs.s3a.S3AFileSystem.innerMkdirs(S3AFileSystem.java:2025)
        at org.apache.hadoop.fs.s3a.S3AFileSystem.mkdirs(S3AFileSystem.java:1956)
        at org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:2305)
        at org.apache.hadoop.fs.contract.AbstractFSContractTestBase.mkdirs(AbstractFSContractTestBase.java:338)
        at org.apache.hadoop.fs.s3a.ITestS3AFileOperationCost.testFakeDirectoryDeletion(ITestS3AFileOperationCost.java:209)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
        at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
        at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
        at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
        at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
        at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
{code}",stevel@apache.org
HADOOP-15076,Enhance S3A troubleshooting documents and add a performance document,"A recurrent theme in s3a-related JIRAs, support calls etc is ""tried upgrading the AWS SDK JAR and then I got the error ..."". We know here ""don't do that"", but its not something immediately obvious to lots of downstream users who want to be able to drop in the new JAR to fix things/add new features

We need to spell this out quite clearlyi ""you cannot safely expect to do this. If you want to upgrade the SDK, you will need to rebuild the whole of hadoop-aws with the maven POM updated to the latest version, ideally rerunning all the tests to make sure something hasn't broken. 

Maybe near the top of the index.md file, along with ""never share your AWS credentials with anyone""
",stevel@apache.org
HADOOP-15074,SequenceFile#Writer flush does not update the length of the written file.,"SequenceFile#Writer flush does not update the length of the file. This happens because as part of the flush, {{UPDATE_LENGTH}} flag is not passed to the DFSOutputStream#hsync.",shashikant
HADOOP-15072,Upgrade Apache Kerby version to 1.1.0,"Apache Kerby 1.1.0 implements cross-realm support, and also includes a GSSAPI module.
",jiajia
HADOOP-15071,s3a troubleshooting docs to add a couple more failure modes,I've got some more troubleshooting entries to add,stevel@apache.org
HADOOP-15070,add test to verify FileSystem and paths differentiate on user info,"Add a test to verify that userinfo data is (correctly) used to differentiate the entries in the FS cache, so are treated as different filesystems.

* This is criticalk for wasb, which uses the username to identify the container, in a path like {{wasb:container1@stevel.azure.net}}. This works in Hadoop, but SPARK-22587 shows that it may not be followed everywhere (and given there's no documentation, who can fault them?)
* AbstractFileSystem.checkPath looks suspiciously like it's path validation just checks host, not authority. That needs a test too.
* And we should cut the @LimitedPrivate(HDFS, Mapreduce) from Path.makeQualified. If MR needs it, it should be considered open to all apps using the Hadoop APIs. Until I looked at the code I thought it was...",stevel@apache.org
HADOOP-15069,support git-secrets commit hook to keep AWS secrets out of git,"The latest Uber breach looks like it involved AWS keys in git repos.

Nobody wants that, which is why amazon provide [git-secrets|https://github.com/awslabs/git-secrets]; a script you can use to scan a repo and its history, *and* add as an automated check.

Anyone can set this up, but there are a few false positives in the scan, mostly from longs and a few all-upper-case constants. These can all be added to a .gitignore file.

Also: mention git-secrets in the aws testing docs; say ""use it""",stevel@apache.org
HADOOP-15067,"GC time percentage reported in JvmMetrics should be a gauge, not counter","A new GcTimeMonitor class has been recently added, and the corresponding metrics added in JvmMetrics.java, line 190:

{code}
    if (gcTimeMonitor != null) {
      rb.addCounter(GcTimePercentage,
          gcTimeMonitor.getLatestGcData().getGcTimePercentage());
    }
{code}

Since GC time percentage can go up and down, a gauge rather than counter should be used to report it. That is, {{addCounter}} should be replaced with {{addGauge}} above.",misha@cloudera.com
HADOOP-15066,Spurious error stopping secure datanode,"There is a spurious error when stopping a secure datanode.

{code}
# hdfs --daemon stop datanode
cat: /var/run/hadoop/hdfs//hadoop-hdfs-root-datanode.pid: No such file or directory
WARNING: pid has changed for datanode, skip deleting pid file
cat: /var/run/hadoop/hdfs//hadoop-hdfs-root-datanode.pid: No such file or directory
WARNING: daemon pid has changed for datanode, skip deleting daemon pid file
{code}

The error appears benign. The service was stopped correctly.",bharatviswa
HADOOP-15063,IOException may be thrown when read from Aliyun OSS in some case,"IOException will be thrown in this case
1. set part size = n(102400)
2. assume current position = 0, then partRemaining = 102400
3. we call seek(pos = 101802), with pos > position && pos < position + partRemaining, so it will skip pos - position bytes, but partRemaining remains the same
4. if we read bytes more than n - pos, it will throw IOException.

Current code:
{code:java}
@Override
  public synchronized void seek(long pos) throws IOException {
    checkNotClosed();
    if (position == pos) {
      return;
    } else if (pos > position && pos < position + partRemaining) {
      AliyunOSSUtils.skipFully(wrappedStream, pos - position);
      // we need update partRemaining here
      position = pos;
    } else {
      reopen(pos);
    }
  }
{code}

Logs:
java.io.IOException: Failed to read from stream. Remaining:101802

	at org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream.read(AliyunOSSInputStream.java:182)
	at org.apache.hadoop.fs.FSInputStream.read(FSInputStream.java:75)
	at org.apache.hadoop.fs.FSDataInputStream.read(FSDataInputStream.java:92)

How to re-produce:
1. create a file with 10MB size
2. 
{code:java}
int seekTimes = 150;
for (int i = 0; i < seekTimes; i++) {
      long pos = size / (seekTimes - i) - 1;
      LOG.info(""begin seeking for pos: "" + pos);
      byte []buf = new byte[1024];
      instream.read(pos, buf, 0, 1024);
}
{code}
",wujinhu
HADOOP-15062,TestCryptoStreamsWithOpensslAesCtrCryptoCodec fails on Debian 9,"{code}
[ERROR] org.apache.hadoop.crypto.TestCryptoStreamsWithOpensslAesCtrCryptoCodec  Time elapsed: 0.478 s  <<< FAILURE!
java.lang.AssertionError: Unable to instantiate codec org.apache.hadoop.crypto.OpensslAesCtrCryptoCodec, is the required version of OpenSSL installed?
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertNotNull(Assert.java:621)
	at org.apache.hadoop.crypto.TestCryptoStreamsWithOpensslAesCtrCryptoCodec.init(TestCryptoStreamsWithOpensslAesCtrCryptoCodec.java:43)
{code}
This happened due to the following openssl change:
https://github.com/openssl/openssl/commit/ff4b7fafb315df5f8374e9b50c302460e068f188",miklos.szegedi@cloudera.com
HADOOP-15060,TestShellBasedUnixGroupsMapping.testFiniteGroupResolutionTime flaky,"{code}
[ERROR] testFiniteGroupResolutionTime(org.apache.hadoop.security.TestShellBasedUnixGroupsMapping)  Time elapsed: 61.975 s  <<< FAILURE!
java.lang.AssertionError: 
Expected the logs to carry a message about command timeout but was: 2017-11-22 00:10:57,523 WARN  security.ShellBasedUnixGroupsMapping (ShellBasedUnixGroupsMapping.java:getUnixGroups(181)) - unable to return groups for user foobarnonexistinguser
PartialGroupNameException The user name 'foobarnonexistinguser' is not found. 
	at org.apache.hadoop.security.ShellBasedUnixGroupsMapping.resolvePartialGroupNames(ShellBasedUnixGroupsMapping.java:275)
	at org.apache.hadoop.security.ShellBasedUnixGroupsMapping.getUnixGroups(ShellBasedUnixGroupsMapping.java:178)
	at org.apache.hadoop.security.ShellBasedUnixGroupsMapping.getGroups(ShellBasedUnixGroupsMapping.java:97)
	at org.apache.hadoop.security.TestShellBasedUnixGroupsMapping.testFiniteGroupResolutionTime(TestShellBasedUnixGroupsMapping.java:278)
{code}",miklos.szegedi@cloudera.com
HADOOP-15059,3.0 deployment cannot work with old version MR tar ball which breaks rolling upgrade,"I tried to deploy 3.0 cluster with 2.9 MR tar ball. The MR job is failed because following error:
{noformat}
2017-11-21 12:42:50,911 INFO [main] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: Created MRAppMaster for application appattempt_1511295641738_0003_000001
2017-11-21 12:42:51,070 WARN [main] org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2017-11-21 12:42:51,118 FATAL [main] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: Error starting MRAppMaster
java.lang.RuntimeException: Unable to determine current user
	at org.apache.hadoop.conf.Configuration$Resource.getRestrictParserDefault(Configuration.java:254)
	at org.apache.hadoop.conf.Configuration$Resource.<init>(Configuration.java:220)
	at org.apache.hadoop.conf.Configuration$Resource.<init>(Configuration.java:212)
	at org.apache.hadoop.conf.Configuration.addResource(Configuration.java:888)
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.main(MRAppMaster.java:1638)
Caused by: java.io.IOException: Exception reading /tmp/nm-local-dir/usercache/jdu/appcache/application_1511295641738_0003/container_e03_1511295641738_0003_01_000001/container_tokens
	at org.apache.hadoop.security.Credentials.readTokenStorageFile(Credentials.java:208)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:907)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:820)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:689)
	at org.apache.hadoop.conf.Configuration$Resource.getRestrictParserDefault(Configuration.java:252)
	... 4 more
Caused by: java.io.IOException: Unknown version 1 in token storage.
	at org.apache.hadoop.security.Credentials.readTokenStorageStream(Credentials.java:226)
	at org.apache.hadoop.security.Credentials.readTokenStorageFile(Credentials.java:205)
	... 8 more
2017-11-21 12:42:51,122 INFO [main] org.apache.hadoop.util.ExitUtil: Exiting with status 1: java.lang.RuntimeException: Unable to determine current user
{noformat}
I think it is due to token incompatiblity change between 2.9 and 3.0. As we claim ""rolling upgrade"" is supported in Hadoop 3, we should fix this before we ship 3.0 otherwise all MR running applications will get stuck during/after upgrade.",jlowe
HADOOP-15058,create-release site build outputs dummy shaded jars due to skipShade,,andrew.wang
HADOOP-15056,Fix TestUnbuffer#testUnbufferException failure,"Hello! I am a new contributor and actually contributing to open source for the very first time. :) 

I pulled down Hadoop today and when running the tests I encountered a failure with the TestUnbuffer#testUnbufferException test.

The unbuffer code has recently gone through some changes and I believe this test case may have been overlooked. Using today's git commit (659e85e304d070f9908a96cf6a0e1cbafde6a434), and upon running the test case, there is an expect mock for an exception UnsupportedOperationException that is no longer being thrown. 

It would appear that a test like this would be valuable so my initial proposed patch did not remove it. Instead, I removed the conditions that were guarding the cast from being able to fire -- as was the previous behavior. Now when we encounter an object that doesn't have the UNBUFFERED StreamCapability, it will throw an error as it did prior to the recent changes. 

Please review and let me know what you think! :D",jackbearden
HADOOP-15055,Add s3 metrics from AWS SDK to s3a metrics tracking,,mackrorysd
HADOOP-15054,upgrade hadoop dependency on commons-codec to 1.11,"https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-auth/3.0.0-beta1 retains the dependency on an old commons-codec version (1.4).
And hadoop-common.
Would it be possible to consider an upgrade to 1.11?",bharatviswa
HADOOP-15047,Python is required for -Preleasedoc but not documented in branch-2.8,"Python is required for -Preleasedoc but not documented in branch-2.8.

* In trunk and branch-3.0, it was documented by HADOOP-10854.
* In branch-2 and branch-2.9, it was documented by YARN-4849.",bharatviswa
HADOOP-15046,Document Apache Hadoop does not support Java 9 in BUILDING.txt,"Now the java version is documented as ""JDK 1.8+"" or ""JDK 1.7+"", we should update this to ""JDK 1.8"" or ""JDK 1.7 or 1.8"" to exclude Java 9.",hanishakoneru
HADOOP-15045,ISA-L build options are documented in branch-2,"In branch-2, YARN-4849 unintentionally documented ISA-L build options in BUILDING.txt. This issue is similar to YARN-7398.",ajisakaa
HADOOP-15044,Wasb getFileBlockLocations() returns too many locations.,"The wasb mimicking of {{getFileBlockLocations()}} uses the length of the file as the number to use to calculate the # of blocks to create (i.e. file.length/blocksize), when it should be just the range of the request.

As a result, you always get the number of blocks in the total file, not the number spanning the range of (start, len). If this is less (i.e start > 0 or len < file.length), you end up with some 0-byte-range blocks at the end",stevel@apache.org
HADOOP-15042,Azure PageBlobInputStream.skip() can return negative value when numberOfPagesRemaining is 0,{{PageBlobInputStream::skip-->skipImpl}} returns negative values when {{numberOfPagesRemaining=0}}. This can cause wrong position to be set in NativeAzureFileSystem::seek() and can lead to errors.,rajesh.balamohan
HADOOP-15041,"XInclude support in .xml configuration file is broken after ""5eb7dbe9b31a45f57f2e1623aa1c9ce84a56c4d1"" commit","XInclude support in .xml configuration file is broken after following check-in.  

Since there is no JIRA number in the following commit message, create a new JIRA to track the issue. 

commit 5eb7dbe9b31a45f57f2e1623aa1c9ce84a56c4d1
Author: Arun Suresh <asuresh@apache.org>
Date:   Thu Nov 9 15:15:51 2017 -0800

Fixing Job History Server Configuration parsing. (Jason Lowe via asuresh)",asuresh
HADOOP-15040,Upgrade AWS SDK to 1.11.271: NPE bug spams logs w/ Yarn Log Aggregation,"My colleagues working with Yarn log aggregation found that they were getting this message spammed in their logs when they used an s3a:// URI for logs (yarn.nodemanager.remote-app-log-dir):

{noformat}
getting attribute Region of com.amazonaws.management:type=AwsSdkMetrics threw an exception
javax.management.RuntimeMBeanException: java.lang.NullPointerException
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.rethrow(DefaultMBeanServerInterceptor.java:839)
	at 
<snip>
Caused by: java.lang.NullPointerException
	at com.amazonaws.metrics.AwsSdkMetrics.getRegion(AwsSdkMetrics.java:729)
	at com.amazonaws.metrics.MetricAdmin.getRegion(MetricAdmin.java:67)
	at sun.reflect.GeneratedMethodAccessor132.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at sun.reflect.misc.Trampoline.invoke(MethodUtil.java:71)
	at sun.reflect.GeneratedMethodAccessor12.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
{noformat}

This happens even though the aws sdk cloudwatch metrics reporting was disabled (default), which is a bug. 

I filed a [github issue|https://github.com/aws/aws-sdk-java/issues/1375|] and it looks like a fix should be coming around SDK release 1.11.229 or so.  ",fabbri
HADOOP-15039,Move SemaphoredDelegatingExecutor to hadoop-common,"Detailed discussions in HADOOP-14999 and HADOOP-15027.

share {{SemaphoredDelegatingExecutor}} and move it to {{hadoop-common}}.

cc [~stevel@apache.org] ",unclegen
HADOOP-15037,Add site release notes for OrgQueue and resource types,Let's add some small blurbs and doc links to the site release notes for these features.,andrew.wang
HADOOP-15036,Update LICENSE.txt for HADOOP-14840,"As noticed by [~anu]:

Looks like HADOOP-14840 added a dependency on “oj! Algorithms - version 43.0”, but we have just added “oj! Algorithms - version 43.0” to the
“LICENSE.txt”. The right addition to the LICENESE.txt should contain the original MIT License, especially “Copyright (c) 2003-2017 Optimatika”.",asuresh
HADOOP-15033,Use java.util.zip.CRC32C for Java 9 and above,"java.util.zip.CRC32C implementation is available since Java 9.
https://docs.oracle.com/javase/9/docs/api/java/util/zip/CRC32C.html
Platform specific assembler intrinsics make it more effective than any pure Java implementation.

Hadoop is compiled against Java 8 but class constructor may be accessible with method handle on 9 to instances implementing Checksum in runtime.",dchuyko
HADOOP-15032,Enable Optimize Hadoop RPC encryption performance for branch-2,"Base on HADOOP-10768, this ticket is targeted to enable Optimize Hadoop RPC encryption performance for branch-2",dapengsun
HADOOP-15031,Fix javadoc issues in Hadoop Common,"Fix the following javadocs warning in Hadoop Common

{code}
[WARNING] Javadoc Warnings
[WARNING] /Users/msingh/code/work/apache/trunk/trunk1/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java:982: warning - Tag @link: can't find createPathHandle(FileStatus, HandleOpt[]) in org.apache.hadoop.fs.FileSystem
[WARNING] /Users/msingh/code/work/apache/trunk/trunk1/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/Options.java:348: warning - Tag @link: can't find getPathHandle(FileStatus, HandleOpt[]) in org.apache.hadoop.fs.FileSystem
{code}",msingh
HADOOP-15030,[branch-2] Include hadoop-cloud-storage-project in the main hadoop pom modules,"During validation of 2.9.0. RC, [~vrushalic] noticed that the hadoop-cloud-storage-project is not included in the main hadoop pom modules so it's not being managed including mvn versions:set for releases. This was fixed in trunk with HADOOP-14004, doing the same for branch-2.",subru
HADOOP-15029,AliyunOSS:  Add User-Agent header in HTTP requests to the OSS server,,unclegen
HADOOP-15027,AliyunOSS: Support multi-thread pre-read to improve sequential read from Hadoop to Aliyun OSS performance,"Currently, AliyunOSSInputStream uses single thread to read data from AliyunOSS,  so we can do some refactoring by using multi-thread pre-read to improve read performance.",wujinhu
HADOOP-15026,Rebase ResourceEstimator start/stop scripts for branch-2,HADOOP-14840 introduced the {{ResourceEstimatorService}} which was cherry-picked from trunk to branch-2. The start/stop scripts need minor alignment with branch-2.,rui li
HADOOP-15025,Ensure singleton for ResourceEstimatorService,HADOOP-15013 fixed static findbugs warnings but this has lead to the singleton being broken for {{ResourceEstimatorService}}. This jira tracks the fix for the same.,rui li
HADOOP-15024,AliyunOSS: support user agent configuration and include that & Hadoop version information to oss server,"Provide oss client side Hadoop version to oss server, to help build access statistic metrics. ",sammi
HADOOP-15023,ValueQueue should also validate (lowWatermark * numValues) > 0 on construction,"ValueQueue has precondition checks for each item independently, but does not check {{(int)(lowWatermark * numValues) > 0}}. If the product is low enough, casting to int will wrap that to 0, causing problems later when filling / getting from the queue.

[code|https://github.com/apache/hadoop/blob/branch-3.0.0-beta1/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/kms/ValueQueue.java#L224]",xiaochen
HADOOP-15022,s3guard IT tests increase R/W capacity of the test table by 1,"Just noticed playing with the CLI that my allocated IOPs was 153; reset it to 10 R & 10 W; after a few of the IT test runs it is now 13 each

assumption: every test run of the S3Guard CLI is increasing the allocated IO",stevel@apache.org
HADOOP-15018,Update JAVA_HOME in create-release for Xenial Dockerfile,"create-release expects the Oracle JDK when setting JAVA_HOME. HADOOP-14816 no longer includes the Oracle JDK, so we need to update this to point to OpenJDK instead.",andrew.wang
HADOOP-15016,Cost-Based RPC FairCallQueue with Reservation support,"FairCallQueue is introduced to provide RPC resource fairness among different users. In current implementation, each user is weighted equally, and the processing priority for different RPC calls are based on how many requests that user sent before. This works well when the cluster is shared among several end-users.

However, this has some limitations when a cluster is shared among both end-users and some service jobs, like some ETL jobs which run under a service account and need to issue lots of RPC calls. When NameNode becomes quite busy, this set of jobs can be easily backoffed and low-prioritied. We cannot simply treat this type jobs as ""bad"" user who randomly issues too many calls, as their calls are normal calls. Also, it is unfair to weight a end-user and a heavy service user equally when allocating RPC resources.

One idea here is to introduce reservation support to RPC resources. That is, for some services, we reserve some RPC resources for their calls. This idea is very similar to how YARN manages CPU/memory resources among different resource queues. A little more details here: Along with existing FairCallQueue setup (like using 4 queues with different priorities), we would add some additional special queues, one for each special service user. For each special service user, we provide a guarantee RPC share (like 10% which can be aligned with its YARN resource share), and this percentage can be converted to a weight used in WeightedRoundRobinMultiplexer. A quick example, we have 4 default queues with default weights (8, 4, 2, 1), and two special service users (user1 with 10% share, and user2 with 15% share). So finally we'll have 6 queues, 4 default queues (with weights 8, 4, 2, 1) and 2 special queues (user1Queue weighted 15*10%/75%=2, and user2Queue weighted 15*15%/75%=3).

For new coming RPC calls from special service users, they will be put directly to the corresponding reserved queue; for other calls, just follow current implementation.

By default, there is no special user and all RPC requests follow existing FairCallQueue implementation.

Would like to hear more comments on this approach; also want to know any other better solutions? Will put a detailed design once get some early comments.",ywskycn
HADOOP-15015,TestConfigurationFieldsBase to use SLF4J for logging,"{{TestConfigurationFieldsBase}} has a protected ""configDebug"" field used to turn logging on/off

{code}
      if (configDebug) {
        System.out.println(""Field: "" + f);
      }
{code}

Presumably its there to allow people with code access to debug their classes. But if we switch to SLF4J you get controllable logging @ runtime.

",stevel@apache.org
HADOOP-15014,KMS should log the IP address of the clients,"Currently KMSMDCFilter only captures http request url and method, but not the remote address of the client.
 Storing this information in a thread local variable would help external authorizer plugins to do more thorough checks. ",zsombor
HADOOP-15013,Fix ResourceEstimator findbugs issues,Just see any recent report.,asuresh
HADOOP-15012,"Add readahead, dropbehind, and unbuffer to StreamCapabilities","A split from HADOOP-14872 to track changes that enhance StreamCapabilities class with READAHEAD, DROPBEHIND, and UNBUFFER capability.

Discussions and code reviews are done in HADOOP-14872.",jzhuge
HADOOP-15009,hadoop-resourceestimator's shell scripts are a mess,"#1:

There's no reason for estimator.sh to exist.  Just make it a subcommand under yarn or whatever.  

#2:

In it's current form, it's missing a BUNCH of boilerplate that makes certain functionality completely fail.

#3

start/stop-estimator.sh is full of copypasta that doesn't actually do anything/work correctly.  Additionally, if estimator.sh doesn't exist, neither does this since yarn --daemon start/stop will do everything as necessary.  
",ajayydv
HADOOP-15008,Metrics sinks may emit too frequently if multiple sink periods are configured,"If there are multiple metrics sink periods configured, depending on what those periods are, some sinks may emit too frequently. For example with the following:
{code:title=hadoop-metrics2.properties}
namenode.sink.file10.class=org.apache.hadoop.metrics2.sink.FileSink
namenode.sink.file5.class=org.apache.hadoop.metrics2.sink.FileSink
namenode.sink.file10.filename=namenode-metrics_per10.out
namenode.sink.file5.filename=namenode-metrics_per5.out
namenode.sink.file10.period=10
namenode.sink.file5.period=5
{code}
I get the following:
{code}
± for f in namenode-metrics_per*.out; do echo ""$f"" && grep ""metricssystem.MetricsSystem"" $f | awk '{last=curr; curr=$1} END { print curr-last }'; done
namenode-metrics_per10.out
5000
namenode-metrics_per5.out
5000
{code}
i.e., for both metrics files, each record is 5000 ms apart, even though one of the sinks has been configured to emit at 10s intervals
",xkrogen
HADOOP-15007,Stabilize and document Configuration <tag> element,"HDFS-12350 (moved to HADOOP-15005). Adds the ability to tag properties with a <tag> value.

We need to make sure that this feature is backwards compatible & usable in production. That's docs, testing, marshalling etc.

",ajayydv
HADOOP-15005,Support meta tag element in Hadoop XML configurations,"We should tag the hadoop/hdfs config so that we can retrieve properties by there usage/application like PERFORMANCE, NAMENODE etc. Right now we don't have an option available to group or list related properties together. Grouping properties through some restricted set of Meta tags and then exposing them in Configuration class will be useful for end users.
For example, here is an config file with tags.

{code}
<configuration>
   <property>
      <name>dfs.namenode.servicerpc-bind-host</name>
      <value>localhost</value>
      <tag> REQUIRED </tag>
   </property>
   
  <property>
      <name>dfs.namenode.fs-limits.min-block-size</name>
      <value> 1048576 </value>
      <tag> PERFORMANCE,REQUIRED</tag>
   </property>

 <property>
      <name>dfs.namenode.logging.level</name>
      <value>Info</value>
      <tag>HDFS, DEBUG </tag>
   </property>
      
<configuration>
{code}",ajayydv
HADOOP-15003,Merge S3A committers into trunk: Yetus patch checker,"This is a Yetus only JIRA created to have Yetus review the HADOOP-13786/HADOOP-14971 patch as a .patch file, as the review PR [https://github.com/apache/hadoop/pull/282] is stopping this happening in HADOOP-14971.

Reviews should go into the PR/other task",stevel@apache.org
HADOOP-15002,AliyunOSS: refactor storage statistics to oss,,unclegen
HADOOP-15001,AliyunOSS: provide one memory-based buffering mechanism in outpustream to oss,,unclegen
HADOOP-14999,AliyunOSS: provide one asynchronous multi-part based uploading mechanism,"This mechanism is designed for uploading file in parallel and asynchronously:
 - improve the performance of uploading file to OSS server. Firstly, this mechanism splits result to multiple small blocks and upload them in parallel. Then, getting result and uploading blocks are asynchronous.
 - avoid buffering too large result into local disk. To cite an extreme example, there is a task which will output 100GB or even larger, we may need to output this 100GB to local disk and then upload it. Sometimes, it is inefficient and limited to disk space.

This patch reuse {{SemaphoredDelegatingExecutor}} as executor service and depends on HADOOP-15039.

Attached {{asynchronous_file_uploading.pdf}} illustrated the difference between previous {{AliyunOSSOutputStream}} and {{AliyunOSSBlockOutputStream}}, i.e. this asynchronous multi-part based uploading mechanism.

1. {{AliyunOSSOutputStream}}: we need to output the whole result to local disk before we can upload it to OSS. This will poses two problems：
 - if the output file is too large, it will run out of the local disk.
 - if the output file is too large, task will wait long time to upload result to OSS before finish, wasting much compute resource.

2. {{AliyunOSSBlockOutputStream}}: we cut the task output into small blocks, i.e. some small local file, and each block will be packaged into a uploading task. These tasks will be submitted into {{SemaphoredDelegatingExecutor}}. {{SemaphoredDelegatingExecutor}} will upload this blocks in parallel, this will improve performance greatly.

3. Each task will retry 3 times to upload block to Aliyun OSS. If one of those tasks failed, the whole file uploading will failed, and we will abort current uploading.",unclegen
HADOOP-14998,Make AuthenticationFilter @Public,"{{org.apache.hadoop.security.authentication.server.AuthenticationFilter}} is currently marked as {{\@Private}} and {{\@Unstable}}.  
{code:java}
@InterfaceAudience.Private
@InterfaceStability.Unstable
public class AuthenticationFilter implements Filter {
{code}

However, many other projects (e.g. Oozie, Hive, Solr, HBase, etc) have been using it for quite some time without having any compatibility issues AFAIK.  It doesn't seem to have had any breaking changes in quite some time.  On top of that, it implements {{javax.servlet.Filter}}, so it can't change too widely anyway.  {{AuthenticationFilter}} provides a lot of useful code for dealing with tokens, Kerberos, etc, and we should encourage related projects to re-use this code instead of rolling their own.

I propose we change it to {{\@Public}} and {{\@Evolving}}.",bharatviswa
HADOOP-14997, Add hadoop-aliyun as dependency of hadoop-cloud-storage,add {{hadoop-aliyun}} dependency in cloud storage modules,unclegen
HADOOP-14996,wasb: ReadFully occasionally fails when using page blobs,"Looks like there is a functional bug or concurrency bug in the PageBlobInputStream implemenation of ReadFully.

1) Use 1 mapper to copy results in success:
hadoop distcp -m 1 wasb://hbt-lifetime@salsbx01sparkdata.blob.core.windows.net/hive_tables  wasb://hbt-lifetime-bkp@supporttestl2.blob.core.windows.net/hdi_backup

2) Turn on DEBUG log by setting mapreduce.map.log.level=DEBUG in ambari. Then run with more than 1 mapper:

Saw debug log like this:
{code}
2017-10-27 06:18:53,545 DEBUG [main] org.apache.hadoop.fs.azure.NativeAzureFileSystem: Seek to position 136251. Bytes skipped 136210
2017-10-27 06:18:53,549 DEBUG [main] org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: Closing page blob output stream.
2017-10-27 06:18:53,549 DEBUG [main] org.apache.hadoop.fs.azure.AzureNativeFileSystemStore: java.util.concurrent.ThreadPoolExecutor@73dce0e6[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]
2017-10-27 06:18:53,549 DEBUG [main] org.apache.hadoop.security.UserGroupInformation: PrivilegedActionException as:mssupport (auth:SIMPLE) cause:java.io.EOFException
2017-10-27 06:18:53,553 WARN [main] org.apache.hadoop.mapred.YarnChild: Exception running child : java.io.EOFException
at java.io.DataInputStream.readFully(DataInputStream.java:197)
at java.io.DataInputStream.readFully(DataInputStream.java:169)
at org.apache.hadoop.io.SequenceFile$Reader.sync(SequenceFile.java:2693)
at org.apache.hadoop.mapreduce.lib.input.SequenceFileRecordReader.initialize(SequenceFileRecordReader.java:58)
at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.initialize(MapTask.java:548)
at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:786)
at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)
at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:170)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:422)
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1866)
at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:164)
{code}",tmarquardt
HADOOP-14993,AliyunOSS: Override listFiles and listLocatedStatus ,"Do a bulk listing off all entries under a path in one single operation, there is no need to recursively walk the directory tree.

Updates:
- override listFiles and listLocatedStatus by using bulk listing
- some minor updates in hadoop-aliyun index.md",unclegen
HADOOP-14992,Upgrade Avro patch version,"Hadoop uses Avro 1.7.4, we should upgrade to 1.7.7",bharatviswa
HADOOP-14991,Add missing figures to Resource Estimator tool,"The figures in the documentation for the Resource Estimator tool is missing in HADOOP-14840. This jira tracks adding them. 

",rui li
HADOOP-14990,Clean up jdiff xml files added for 2.8.2 release,The jdiff xml files for 2.8.2 release have been committed to trunk (sha id a25b5aa0cf5189247fa38e7b0a188d568eba1b6c). Do we still need it here ? ,djp
HADOOP-14987,Improve KMSClientProvider log around delegation token checking,"KMSClientProvider#containsKmsDt uses SecurityUtil.buildTokenService(addr) to build the key to look for KMS-DT from the UGI's token map. The token lookup key here varies depending  on the KMSClientProvider's configuration value for hadoop.security.token.service.use_ip. In certain cases, the token obtained with non-matching hadoop.security.token.service.use_ip setting will not be recognized by KMSClientProvider. This ticket is opened to improve logs for troubleshooting KMS delegation token related issues like this.  
",xyao
HADOOP-14986,Enforce JDK limitations,"The compiler can prevent using incompatible language features, but it does not guard against link errors.",chris.douglas
HADOOP-14985,Remove subversion related code from VersionInfoMojo.java,"When building Apache Hadoop, we can see the following message:
{noformat}
[WARNING] [svn, info] failed with error code 1
{noformat}
We have migrated to the code base from svn to git, so the message is useless.",ajayydv
HADOOP-14983,DistCp diff option support for snapshot root descendant directories,"HDFS-12544 enables SnapshotDiff generation on any snapshot root descendant directory. Previous to this, all snapshot diff operations had to be based on snapshot root directory. DistCp already supports update diff option by which snapshot diff report between any given two snapshots can be used to identify the difference between source and target, and apply the diff to the target to make it in sync with source. But, the snapshot source and target paths need to be a snapshot root directory for the diff option to work.

{noformat}
DistCp -update -diff <oldSnapshot> <newSnapshot> <source-snapshot-root-dir> <target-snapshot-root-dir>
{noformat}

Better if DistCp can also support update diff option on any descendant directories under snapshot root, provided HDFS-12544 is enabled.",manojg
HADOOP-14982,Clients using FailoverOnNetworkExceptionRetry can go into a loop if they're used without authenticating with kerberos in HA env,"If HA is configured for the Resource Manager in a secure environment, using the mapred client goes into a loop if the user is not authenticated with Kerberos.

{noformat}
[root@pb6sec-1 ~]# mapred job -list
17/10/25 06:37:43 INFO client.ConfiguredRMFailoverProxyProvider: Failing over to rm36
17/10/25 06:37:43 WARN ipc.Client: Exception encountered while connecting to the server : javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]
17/10/25 06:37:43 INFO retry.RetryInvocationHandler: java.io.IOException: Failed on local exception: java.io.IOException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]; Host Details : local host is: ""host_redacted/IP_redacted""; destination host is: ""com.host2.redacted:8032; , while invoking ApplicationClientProtocolPBClientImpl.getApplications over rm36 after 1 failover attempts. Trying to failover after sleeping for 160ms.
17/10/25 06:37:43 INFO client.ConfiguredRMFailoverProxyProvider: Failing over to rm25
17/10/25 06:37:43 INFO retry.RetryInvocationHandler: java.net.ConnectException: Call From host_redacted/IP_redacted to com.host.redacted:8032 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking ApplicationClientProtocolPBClientImpl.getApplications over rm25 after 2 failover attempts. Trying to failover after sleeping for 582ms.
17/10/25 06:37:44 INFO client.ConfiguredRMFailoverProxyProvider: Failing over to rm36
17/10/25 06:37:44 WARN ipc.Client: Exception encountered while connecting to the server : javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]
17/10/25 06:37:44 INFO retry.RetryInvocationHandler: java.io.IOException: Failed on local exception: java.io.IOException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]; Host Details : local host is: ""host_redacted/IP_redacted""; destination host is: ""com.host2.redacted:8032; , while invoking ApplicationClientProtocolPBClientImpl.getApplications over rm36 after 3 failover attempts. Trying to failover after sleeping for 977ms.
17/10/25 06:37:45 INFO client.ConfiguredRMFailoverProxyProvider: Failing over to rm25
17/10/25 06:37:45 INFO retry.RetryInvocationHandler: java.net.ConnectException: Call From host_redacted/IP_redacted to com.host.redacted:8032 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking ApplicationClientProtocolPBClientImpl.getApplications over rm25 after 4 failover attempts. Trying to failover after sleeping for 1667ms.
17/10/25 06:37:46 INFO client.ConfiguredRMFailoverProxyProvider: Failing over to rm36
17/10/25 06:37:46 WARN ipc.Client: Exception encountered while connecting to the server : javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]
17/10/25 06:37:46 INFO retry.RetryInvocationHandler: java.io.IOException: Failed on local exception: java.io.IOException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]; Host Details : local host is: ""host_redacted/IP_redacted""; destination host is: ""com.host2.redacted:8032; , while invoking ApplicationClientProtocolPBClientImpl.getApplications over rm36 after 5 failover attempts. Trying to failover after sleeping for 2776ms.
17/10/25 06:37:49 INFO client.ConfiguredRMFailoverProxyProvider: Failing over to rm25
17/10/25 06:37:49 INFO retry.RetryInvocationHandler: java.net.ConnectException: Call From host_redacted/IP_redacted to com.host.redacted:8032 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking ApplicationClientProtocolPBClientImpl.getApplications over rm25 after 6 failover attempts. Trying to failover after sleeping for 1055ms.
17/10/25 06:37:50 INFO client.ConfiguredRMFailoverProxyProvider: Failing over to rm36
17/10/25 06:37:50 WARN ipc.Client: Exception encountered while connecting to the server : javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]
17/10/25 06:37:50 INFO retry.RetryInvocationHandler: java.io.IOException: Failed on local exception: java.io.IOException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]; Host Details : local host is: ""host_redacted/IP_redacted""; destination host is: ""com.host2.redacted:8032; , while invoking ApplicationClientProtocolPBClientImpl.getApplications over rm36 after 7 failover attempts. Trying to failover after sleeping for 2608ms.
...
{noformat}

The reason is that the retry handler sees a {{ConnectException}}, then fails over to the inactive RM. This obviously doesn't work, so it comes back to the active and whole process starts again. The RetryHandler should examine if the {{ConnectException}} is actually caused by a {{GSSException}} (and probably check the ""No valid credentials provided"" message) and if so, it should not perform a failover.",pbacsko
HADOOP-14981,SnappyCodec.checkNativeCodeLoaded raises UnsatisfiedLinkError if native code is missing,"I'm getting a stack trace on trunk because there is no native lib on the path and Snappy has been invoked
{code}
  at java.lang.Thread.run(Thread.java:745)
  ...  Cause: java.lang.UnsatisfiedLinkError: org.apache.hadoop.util.NativeCodeLoader.buildSupportsSnappy()Z
  at org.apache.hadoop.util.NativeCodeLoader.buildSupportsSnappy(Native Method)
  at org.apache.hadoop.io.compress.SnappyCodec.checkNativeCodeLoaded(SnappyCodec.java:63)
{code}
HADOOP-13684 touched these lines in the past, ",jojochuang
HADOOP-14980,[JDK9] Upgrade maven-javadoc-plugin to 3.0.0-M1,"Build source code with {code} mvn clean package -Pdist -DskipTests -Dmaven.javadoc.skip=True -Dtar 
{code} 


{code}
ERROR] -----------------------------------------------------
[ERROR] realm =    plugin>org.apache.maven.plugins:maven-javadoc-plugin:2.10.4
[ERROR] strategy = org.codehaus.plexus.classworlds.strategy.SelfFirstStrategy
[ERROR] urls[0] = file:/root/.m2/repository/org/apache/maven/plugins/maven-javadoc-plugin/2.10.4/maven-javadoc-plugin-2.10.4.jar
[ERROR] urls[1] = file:/root/.m2/repository/org/slf4j/slf4j-jdk14/1.5.6/slf4j-jdk14-1.5.6.jar
[ERROR] urls[2] = file:/root/.m2/repository/org/slf4j/jcl-over-slf4j/1.5.6/jcl-over-slf4j-1.5.6.jar
[ERROR] urls[3] = file:/root/.m2/repository/commons-cli/commons-cli/1.2/commons-cli-1.2.jar
[ERROR] urls[5] = file:/root/.m2/repository/org/sonatype/plexus/plexus-sec-dispatcher/1.3/plexus-sec-dispatcher-1.3.jar
[ERROR] urls[6] = file:/root/.m2/repository/org/sonatype/plexus/plexus-cipher/1.4/plexus-cipher-1.4.jar
[ERROR] urls[7] = file:/root/.m2/repository/org/codehaus/plexus/plexus-interpolation/1.11/plexus-interpolation-1.11.jar
[ERROR] urls[8] = file:/root/.m2/repository/backport-util-concurrent/backport-util-concurrent/3.1/backport-util-concurrent-3.1.jar
[ERROR] urls[9] = file:/root/.m2/repository/org/apache/maven/reporting/maven-reporting-api/3.0/maven-reporting-api-3.0.jar
[ERROR] urls[10] = file:/root/.m2/repository/org/apache/maven/maven-archiver/2.5/maven-archiver-2.5.jar
[ERROR] urls[11] = file:/root/.m2/repository/org/apache/maven/shared/maven-invoker/2.2/maven-invoker-2.2.jar
[ERROR] urls[14] = file:/root/.m2/repository/org/apache/maven/doxia/doxia-sink-api/1.4/doxia-sink-api-1.4.jar
[ERROR] urls[15] = file:/root/.m2/repository/org/apache/maven/doxia/doxia-logging-api/1.4/doxia-logging-api-1.4.jar
[ERROR] urls[16] = file:/root/.m2/repository/org/apache/maven/doxia/doxia-site-renderer/1.4/doxia-site-renderer-1.4.jar
[ERROR] urls[17] = file:/root/.m2/repository/org/apache/maven/doxia/doxia-core/1.4/doxia-core-1.4.jar
[ERROR] urls[18] = file:/root/.m2/repository/xerces/xercesImpl/2.9.1/xercesImpl-2.9.1.jar
[ERROR] urls[19] = file:/root/.m2/repository/xml-apis/xml-apis/1.3.04/xml-apis-1.3.04.jar
[ERROR] urls[20] = file:/root/.m2/repository/org/apache/maven/doxia/doxia-decoration-model/1.4/doxia-decoration-model-1.4.jar
[ERROR] urls[21] = file:/root/.m2/repository/org/apache/maven/doxia/doxia-module-xhtml/1.4/doxia-module-xhtml-1.4.jar
[ERROR] urls[22] = file:/root/.m2/repository/org/apache/maven/doxia/doxia-module-fml/1.4/doxia-module-fml-1.4.jar
[ERROR] urls[23] = file:/root/.m2/repository/org/codehaus/plexus/plexus-i18n/1.0-beta-7/plexus-i18n-1.0-beta-7.jar
[ERROR] urls[24] = file:/root/.m2/repository/org/codehaus/plexus/plexus-velocity/1.1.7/plexus-velocity-1.1.7.jar
[ERROR] urls[27] = file:/root/.m2/repository/org/apache/velocity/velocity-tools/2.0/velocity-tools-2.0.jar
[ERROR] urls[28] = file:/root/.m2/repository/commons-beanutils/commons-beanutils/1.7.0/commons-beanutils-1.7.0.jar
[ERROR] urls[29] = file:/root/.m2/repository/commons-digester/commons-digester/1.8/commons-digester-1.8.jar
[ERROR] urls[30] = file:/root/.m2/repository/commons-chain/commons-chain/1.1/commons-chain-1.1.jar
[ERROR] urls[31] = file:/root/.m2/repository/commons-validator/commons-validator/1.3.1/commons-validator-1.3.1.jar
[ERROR] urls[32] = file:/root/.m2/repository/dom4j/dom4j/1.1/dom4j-1.1.jar
[ERROR] urls[33] = file:/root/.m2/repository/sslext/sslext/1.2-0/sslext-1.2-0.jar
[ERROR] urls[34] = file:/root/.m2/repository/org/apache/struts/struts-core/1.3.8/struts-core-1.3.8.jar
[ERROR] urls[35] = file:/root/.m2/repository/antlr/antlr/2.7.2/antlr-2.7.2.jar
[ERROR] urls[36] = file:/root/.m2/repository/org/apache/struts/struts-taglib/1.3.8/struts-taglib-1.3.8.jar
[ERROR] urls[37] = file:/root/.m2/repository/org/apache/struts/struts-tiles/1.3.8/struts-tiles-1.3.8.jar
[ERROR] urls[38] = file:/root/.m2/repository/commons-collections/commons-collections/3.2.1/commons-collections-3.2.1.jar
[ERROR] urls[24] = file:/root/.m2/repository/org/codehaus/plexus/plexus-velocity/1.1.7/plexus-velocity-1.1.7.jar
[ERROR] urls[25] = file:/root/.m2/repository/org/apache/velocity/velocity/1.5/velocity-1.5.jar
[ERROR] urls[26] = file:/root/.m2/repository/oro/oro/2.0.8/oro-2.0.8.jar
[ERROR] urls[27] = file:/root/.m2/repository/org/apache/velocity/velocity-tools/2.0/velocity-tools-2.0.jar
[ERROR] urls[28] = file:/root/.m2/repository/commons-beanutils/commons-beanutils/1.7.0/commons-beanutils-1.7.0.jar
[ERROR] urls[29] = file:/root/.m2/repository/commons-digester/commons-digester/1.8/commons-digester-1.8.jar
[ERROR] urls[30] = file:/root/.m2/repository/commons-chain/commons-chain/1.1/commons-chain-1.1.jar
[ERROR] urls[31] = file:/root/.m2/repository/commons-validator/commons-validator/1.3.1/commons-validator-1.3.1.jar
[ERROR] urls[32] = file:/root/.m2/repository/dom4j/dom4j/1.1/dom4j-1.1.jar
[ERROR] urls[33] = file:/root/.m2/repository/sslext/sslext/1.2-0/sslext-1.2-0.jar
[ERROR] urls[34] = file:/root/.m2/repository/org/apache/struts/struts-core/1.3.8/struts-core-1.3.8.jar
[ERROR] urls[35] = file:/root/.m2/repository/antlr/antlr/2.7.2/antlr-2.7.2.jar
[ERROR] urls[36] = file:/root/.m2/repository/org/apache/struts/struts-taglib/1.3.8/struts-taglib-1.3.8.jar
[ERROR] urls[37] = file:/root/.m2/repository/org/apache/struts/struts-tiles/1.3.8/struts-tiles-1.3.8.jar
[ERROR] urls[38] = file:/root/.m2/repository/commons-collections/commons-collections/3.2.1/commons-collections-3.2.1.jar
[ERROR] urls[39] = file:/root/.m2/repository/commons-lang/commons-lang/2.4/commons-lang-2.4.jar
[ERROR] urls[40] = file:/root/.m2/repository/commons-io/commons-io/2.5/commons-io-2.5.jar
[ERROR] urls[41] = file:/root/.m2/repository/org/apache/httpcomponents/httpclient/4.2.3/httpclient-4.2.3.jar
[ERROR] urls[42] = file:/root/.m2/repository/org/apache/httpcomponents/httpcore/4.2.2/httpcore-4.2.2.jar
[ERROR] urls[43] = file:/root/.m2/repository/commons-codec/commons-codec/1.6/commons-codec-1.6.jar
[ERROR] urls[44] = file:/root/.m2/repository/commons-logging/commons-logging/1.1.1/commons-logging-1.1.1.jar
[ERROR] urls[45] = file:/root/.m2/repository/log4j/log4j/1.2.14/log4j-1.2.14.jar
[ERROR] urls[46] = file:/root/.m2/repository/com/thoughtworks/qdox/qdox/1.12.1/qdox-1.12.1.jar
[ERROR] urls[47] = file:/root/.m2/repository/junit/junit/3.8.1/junit-3.8.1.jar
[ERROR] urls[48] = file:/root/.m2/repository/org/codehaus/plexus/plexus-archiver/3.3/plexus-archiver-3.3.jar
[ERROR] urls[49] = file:/root/.m2/repository/org/codehaus/plexus/plexus-io/2.7.1/plexus-io-2.7.1.jar
[ERROR] urls[50] = file:/root/.m2/repository/org/apache/commons/commons-compress/1.11/commons-compress-1.11.jar
[ERROR] urls[51] = file:/root/.m2/repository/org/iq80/snappy/snappy/0.4/snappy-0.4.jar
[ERROR] urls[52] = file:/root/.m2/repository/org/tukaani/xz/1.5/xz-1.5.jar
[ERROR] urls[53] = file:/root/.m2/repository/org/codehaus/plexus/plexus-utils/3.0.24/plexus-utils-3.0.24.jar
[ERROR] Number of foreign imports: 1
[ERROR] import: Entry[import  from realm ClassRealm[project>org.apache.hadoop:hadoop-main:3.1.0-SNAPSHOT, parent: ClassRealm[maven.api, parent: null]]]
[ERROR]
[ERROR] -----------------------------------------------------: begin 0, end 3, length 1
[ERROR] -> [Help 1]
{code}

",xianyuxiaoer2
HADOOP-14979,Upgrade maven-dependency-plugin to 3.0.2,"When i build hadoop with jdk9 with command
{code}
mvn clean package -Pdist -DskipTests -Dmaven.javadoc.skip=True -Dtar 
{code}
Build successfully but has some error in the build log
{code}
java.lang.NoSuchMethodException: jdk.internal.module.ModuleReferenceImpl.descriptor()
        at java.base/java.lang.Class.getDeclaredMethod(Class.java:2432)
        at org.apache.maven.plugins.dependency.utils.DependencyStatusSets.getModuleDescriptor(DependencyStatusSets.java:272)
        at org.apache.maven.plugins.dependency.utils.DependencyStatusSets.buildArtifactListOutput(DependencyStatusSets.java:227)
        at org.apache.maven.plugins.dependency.utils.DependencyStatusSets.getOutput(DependencyStatusSets.java:165)
        at org.apache.maven.plugins.dependency.resolvers.ResolveDependenciesMojo.doExecute(ResolveDependenciesMojo.java:90)
        at org.apache.maven.plugins.dependency.AbstractDependencyMojo.execute(AbstractDependencyMojo.java:143)
        at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:134)
{code}

this should upgrade maven-dependency-plugin to solve.see https://issues.apache.org/jira/browse/MDEP-571
",kellyzly
HADOOP-14977,Xenial dockerfile needs ant in main build for findbugs,"findbugs doesn't work without ant installed, for whatever reason:

{code}
[warning] /usr/bin/setBugDatabaseInfo: Unable to locate ant in /usr/share/java
[warning] /usr/bin/convertXmlToText: Unable to locate ant in /usr/share/java
[warning] /usr/bin/filterBugs: Unable to locate ant in /usr/share/java
{code}",ajisakaa
HADOOP-14976,Set HADOOP_SHELL_EXECNAME explicitly in scripts,"Some Hadoop shell scripts infer their own name using this bit of shell magic:

{code}
 18     MYNAME=""${BASH_SOURCE-$0}""
 19     HADOOP_SHELL_EXECNAME=""${MYNAME##*/}""
{code}

e.g. see the [hdfs|https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/bin/hdfs#L18] script.

The inferred shell script name is later passed to _hadoop-functions.sh_ which uses it to construct the names of some environment variables. E.g. when invoking _hdfs datanode_, the options variable name is inferred as follows:
{code}
# HDFS + DATANODE + OPTS -> HDFS_DATANODE_OPTS
{code}

This works well if the calling script name is standard {{hdfs}} or {{yarn}}. If a distribution renames the script to something like foo.bar, , then the variable names will be inferred as {{FOO.BAR_DATANODE_OPTS}}. This is not a valid bash variable name.",arpitagarwal
HADOOP-14974,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestContainerAllocation fails in trunk,"{code}
org.apache.hadoop.metrics2.MetricsException: Metrics source QueueMetrics,q0=root already exists!

	at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.newSourceName(DefaultMetricsSystem.java:152)
	at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.sourceName(DefaultMetricsSystem.java:125)
	at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:239)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueueMetrics.forQueue(CSQueueMetrics.java:141)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AbstractCSQueue.<init>(AbstractCSQueue.java:131)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.<init>(ParentQueue.java:90)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerQueueManager.parseQueue(CapacitySchedulerQueueManager.java:267)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerQueueManager.initializeQueues(CapacitySchedulerQueueManager.java:158)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.initializeQueues(CapacityScheduler.java:639)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.initScheduler(CapacityScheduler.java:331)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.serviceInit(CapacityScheduler.java:391)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
	at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:108)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceInit(ResourceManager.java:756)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.createAndInitActiveServices(ResourceManager.java:1152)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceInit(ResourceManager.java:317)
	at org.apache.hadoop.yarn.server.resourcemanager.MockRM.serviceInit(MockRM.java:1313)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
	at org.apache.hadoop.yarn.server.resourcemanager.MockRM.<init>(MockRM.java:161)
	at org.apache.hadoop.yarn.server.resourcemanager.MockRM.<init>(MockRM.java:140)
	at org.apache.hadoop.yarn.server.resourcemanager.MockRM.<init>(MockRM.java:136)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestContainerAllocation.testExcessReservationThanNodeManagerCapacity(TestContainerAllocation.java:90)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
{code}",jzhuge
HADOOP-14973,[s3a] Log StorageStatistics,"S3A is currently storing much more detailed metrics via StorageStatistics than are logged in a MapReduce job. Eventually, it would be nice to get Spark, MapReduce and other workloads to retrieve and store these metrics, but it may be some time before they all do that. I'd like to consider having S3A publish the metrics itself in some form. This is tricky, as S3A has no daemon but lives inside various other processes.

Perhaps writing to a log file at some configurable interval and on close() would be the best we could do. Other ideas would be welcome.",mackrorysd
HADOOP-14972,"S3A add histogram metrics types for latency, etc.","We'd like metrics to track latencies for various operations, such as latencies for various request types, etc. This may need to be done different from current metrics types that are just counters of type long, and it needs to be done intelligently as these measurements are very numerous, and are primarily interesting due to the outliers that are unpredictably far from normal. A few ideas on how we might implement something like this:

* An adaptive, sparse histogram type. I envision something configurable with a maximumum granularity and a maximum number of bins. Initially, datapoints are tallied in bins with the maximum granularity. As we reach the maximum number of bins, bins are merged in even / odd pairs. There's some complexity here, especially to make it perform well and allow safe concurrency, but I like the ability to configure reasonable limits and retain as much granularity as possible without knowing the exact shape of the data beforehand.

* LongMetrics named ""read_latency_600ms"", ""read_latency_800ms"" to represent bins. This was suggested to me by [~fabbri]. I initially did not like the idea of having either so many hard-coded bins for however many op types, but this could also be done dynamically (we just hard-code which measurements we take, and with what granularity to group them, e.g. read_latency, 200 ms). The resulting dataset could be sparse and dynamic to allow for extreme outliers, but the granularity is still pre-determined.

* We could also simply track a certain number of the highest latencies, and basic descriptive statistics like a running average, min / max, etc. Inherently more limited in what it can show us, but much simpler and might still provide some insight when analyzing performance.",mackrorysd
HADOOP-14971,Merge S3A committers into trunk,Merge the HADOOP-13786 committer into trunk. This branch is being set up as a github PR for review there & to keep it out the mailboxes of the watchers on the main JIRA,stevel@apache.org
HADOOP-14970,MiniHadoopClusterManager doesn't respect lack of format option,"The CLI MiniCluster, {{MiniHadoopClusterManager}}, says that by default it does not format its directories, and provides the {{-format}} option to specify that it should do so. However, it builds its {{MiniDFSCluster}} like:
{code}
      dfs = new MiniDFSCluster.Builder(conf).nameNodePort(nnPort)
          .nameNodeHttpPort(nnHttpPort).numDataNodes(numDataNodes)
          .startupOption(dfsOpts).build();
{code}
{{MiniDFSCluster.Builder}}, by default, sets {{format}} to true, so even though the {{startupOption}} is {{REGULAR}}, it will still format regardless of whether or not the flag is supplied.",xkrogen
HADOOP-14969,Improve diagnostics in secure DataNode startup,"When DN secure mode configuration is incorrect, it throws the following exception from Datanode#checkSecureConfig
{code}
  private static void checkSecureConfig(DNConf dnConf, Configuration conf,
      SecureResources resources) throws RuntimeException {
    if (!UserGroupInformation.isSecurityEnabled()) {
      return;
    }
...
    throw new RuntimeException(""Cannot start secure DataNode without "" +
      ""configuring either privileged resources or SASL RPC data transfer "" +
      ""protection and SSL for HTTP.  Using privileged resources in "" +
      ""combination with SASL RPC data transfer protection is not supported."");
{code}
The DN should print more useful diagnostics as to what exactly what went wrong.
Also when starting secure DN with resources then the startup scripts should launch the SecureDataNodeStarter class. If no SASL is configured and SecureDataNodeStarter is not used, then we could mention that too.",ajayydv
HADOOP-14966,Handle JDK-8071638 for hadoop-common,"Impact modules
-- YARN nodemanger cache clean up
-- Mapreduce Log/History cleaner

Will add jira in YARN & MAPREDUCE to track the same

",bibinchundatt
HADOOP-14965,"s3a input stream ""normal"" fadvise mode to be adaptive","HADOOP-14535 added seek optimisation to wasb, but rather than require the caller to declare sequential vs random, it works out for itself.

# defaults to sequential, lazy seek
# if the caller ever seeks backwards, switches to random IO.

This means that on the use pattern of columnar stores: of go to end of file, read summary, then go to columns and work forwards, will switch to random IO after that first seek back (cost: one aborted HTTP connection)/.

Where this should benefit the most is in downstream apps where you are working with different data sources in the same object store/running of the same app config, but have different read patterns. I'm seeing exactly this in some of my spark tests, where it's near impossible to set things up so that .gz files are read sequentially, but ORC data is read in random IO

I propose the ""normal"" fadvise => adaptive, sequential==sequential always, random => random from the outset.",stevel@apache.org
HADOOP-14964,AliyunOSS: backport Aliyun OSS module to branch-2,,sammi
HADOOP-14960,Add GC time percentage monitor/alerter,"Currently class {{org.apache.hadoop.metrics2.source.JvmMetrics}} provides several metrics related to GC. Unfortunately, all these metrics are not as useful as they could be, because they don't answer the first and most important question related to GC and JVM health: what percentage of time my JVM is paused in GC? This percentage, calculated as the sum of the GC pauses over some period, like 1 minute, divided by that period - is the most convenient measure of the GC health because:
- it is just one number, and it's clear that, say, 1..5% is good, but 80..90% is really bad
- it allows for easy apple-to-apple comparison between runs, even between different apps
- when this metric reaches some critical value like 70%, it almost always indicates a ""GC death spiral"", from which the app can recover only if it drops some task(s) etc.

The existing ""total GC time"", ""total number of GCs"" etc. metrics only give numbers that can be used to rougly estimate this percentage. Thus it is suggested to add a new metric to this class, and possibly allow users to register handlers that will be automatically invoked if this metric reaches the specified threshold.",misha@cloudera.com
