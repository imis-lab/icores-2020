key,summary,description,assignee
AIRFLOW-5503,Tree view layout on HDPI displays broken,"The tree view layout on HDPI displays is broken and doesn't make good usage of screen space.

Associated pull request fixes the issue by using devicePixelRatio: [https://developer.mozilla.org/en-US/docs/Web/API/Window/devicePixelRatio]

Please see attachments for the before and after of applying this patch",robinedwards
AIRFLOW-5495,Remove unneeded parens in dataproc.py,"Note: This ticket's being created to facilitate a new contributor's workshop for Airflow. After the workshop has completed, I'll mark these all available for anyone that might like to take them on.

The parens around {{self.custom_image_project_id}} don't need to be there; we should remove them.

airflow/gcp/operators/dataproc.py:409
{code:java}
elif self.custom_image:
    project_id = self.custom_image_project_id if (self.custom_image_project_id) else self.project_id
    custom_image_url = 'https://www.googleapis.com/compute/beta/projects/' \ {code}",adankro
AIRFLOW-5482,Deprecate Schedule Interval on task level,It has been 4 years since it was deprecated. [https://github.com/apache/airflow/commit/3e8bb2abf18c3a130c52288e25f5f7d114e407ad],kaxilnaik
AIRFLOW-5481,Allow Deleting Renamed DAGs,"If we rename a dag_id but do not rename the file, then we cannot delete the old dag.",kaxilnaik
AIRFLOW-5479,Normalize get_conn method in GCP Kubernetes hook,,tobked
AIRFLOW-5477,Rewrite Google PubSub Hook to Google Cloud Python,,tobked
AIRFLOW-5476,Typo in BREEZE.rst,You can choose the optional flags you need with breez+s+ → You can choose the optional flags you need with breez+e+,dongjin.lee.kr
AIRFLOW-5474,Add Basic Auth to Druid hook,Use login and password from druid ingestion connection to add Basic HTTP auth to druid hook. If login and/or password is None then ensure hook still works.,awelsh93
AIRFLOW-5471,Fix docstring in GcpTransferServiceOperationsListOperator,,tobked
AIRFLOW-5470,Add Apache Livy REST operator,"Add a hook, operator and sensor to post and monitor Spark batches through the Apache Livy REST API.

[https://livy.incubator.apache.org/docs/latest/rest-api.html]",luca cavazzana
AIRFLOW-5469,Airflow CLI render error on unicode in the stdout,"As a non-English user, lots of my hive scripts come along with unicode characters as string values. The hive queries render correctly on Web, but not in CLI. Below is the log of the render output:
{quote}/usr/local/lib/python2.7/dist-packages/airflow/utils/helpers.py:385: DeprecationWarning: Importing 'HivePartitionSensor' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.

DeprecationWarning)

/usr/local/lib/python2.7/dist-packages/airflow/utils/helpers.py:385: DeprecationWarning: Importing 'EmailOperator' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.

DeprecationWarning)

/usr/local/lib/python2.7/dist-packages/airflow/models.py:4007: PendingDeprecationWarning: The requested task could not be added to the DAG because a task with task_id emails_done is already in the DAG. Starting in Airflow 2.0, trying to overwrite a task will raise an exception.

category=PendingDeprecationWarning)

Traceback (most recent call last):

File ""/usr/local/bin/airflow"", line 32, in <module>

args.func(args)

File ""/usr/local/lib/python2.7/dist-packages/airflow/utils/cli.py"", line 74, in wrapper

return f(*args, **kwargs)

File ""/usr/local/lib/python2.7/dist-packages/airflow/bin/cli.py"", line 601, in render

"""""".format(attr, getattr(task, attr))))

UnicodeEncodeError: 'ascii' codec can't encode characters in position 2138-2139: ordinal not in range(128)

exitCode 1
{quote}
 The issue is simply the string format in the render function doesn't support unicode. [https://github.com/apache/airflow/blob/1.10.0/airflow/bin/cli.py#L603]",congzzz
AIRFLOW-5467,"Tasks in the subdag not cleared when dag_id contains ""."" and starts after retry","Tasks in the subdag not cleared when dag_id contains ""."" and starts after retry",youkio
AIRFLOW-5459,use a dynamic tmp location in dataflow operator,use a dynamic tmp location in dataflow operator,marengaz
AIRFLOW-5451,Spark Submit Hook don't set namespace if default,,fokko
AIRFLOW-5450,Google Analytics code throws error on startup,"On loading of the airflow site, this error appears:

```

Uncaught DOMException: Failed to execute 'appendChild' on 'Node': This node type does not support this method.
 at https://airflow.apache.org/:1003:45
 at [https://airflow.apache.org/:1004:9]

```

This blocks Google Analytics from reporting anything.",aizhamal
AIRFLOW-5449,Can't commit blank values on variable edits,"On Airflow, when going to ""admin >> variables"" and attempting to change the value of an existing variable to nothing, the changes aren't being set properly and saving the edit does nothing. This bug fix should allow users to blank out the values of existing variables.",andrew.desousa
AIRFLOW-5448,Make Kubernetes Executor compatible with Istio service mesh,"Istio service mesh is not compatible by default with Kubernetes Jobs. The normal behavior is that a Job will be started, get an istio-proxy sidecar attached to it via the istio mutating webhook, run until completion, then the 'main' container in the pod stops, but istio-proxy hangs around indefinitely. This applies to the Kubernetes Executor.

Very recently, Istio implemented an endpoint that can be called to cleanly exit the proxy, specifically designed for this use case.
 - explanation: [https://github.com/istio/istio/issues/15041]
 - istio PR implementing it: [https://github.com/istio/istio/pull/15406]

Astronomer will make a contribution to handle cleanly exit the istio-proxy by default. This will help Astronomer and other Airflow users making use of the Kubernetes Executor in combination with Istio.

Original PR for solving this issue: [https://github.com/astronomer/airflow/pull/47]

We will integrate into the Astronomer fork, then upstream after complying with all Airflow PR standards.",sjmiller609
AIRFLOW-5447,KubernetesExecutor hangs on task queueing,"Starting in 1.10.4, and continuing in 1.10.5, when using the KubernetesExecutor, with the webserver and scheduler running in the kubernetes cluster, tasks are scheduled, but when added to the task queue, the executor process hangs indefinitely. Based on log messages, it appears to be stuck at this line https://github.com/apache/airflow/blob/v1-10-stable/airflow/contrib/executors/kubernetes_executor.py#L761",dimberman
AIRFLOW-5446,Rewrite Google KMS Hook with Google Api Client,,tobked
AIRFLOW-5444,action_logging missing important fields such as Dag Id and Task Id for POST actions,"For example, when user admin cleared a task example_bash_operator, the log looks like this:
{code:python}
Id	Dttm	Dag Id	Task Id	Event	Execution Date	Owner	Extra
78	09-09T02:04:56.663074+00:00	example_bash_operator	graph	admin	[('dag_id', 'example_bash_operator'), ('root', '')]
77	09-09T02:04:56.606590+00:00	None	clear	admin	[]
76	09-09T02:04:55.155144+00:00	None	clear	admin	[]
75	09-09T02:04:48.419288+00:00	example_bash_operator	task_instances	09-08T00:00:00+00:00	admin	[('dag_id', 'example_bash_operator'), ('execution_date', '2019-09-08T00:00:00+00:00')]
{code}
Some important fields for the event ""clear"" are not populated: Dag Id, Task Id. These fields would have been very helpful for troubleshooting or audit purposes.

The same problem happens for event ""failed"". It probably happens to many other actions too.
{code:python}
Id	Dttm	Dag Id	Task Id	Event	Execution Date	Owner	Extra
78	09-09T02:04:56.663074+00:00	example_bash_operator	graph	admin	[('dag_id', 'example_bash_operator'), ('root', '')]
77	09-09T02:04:56.606590+00:00	None	clear	admin	[]
76	09-09T02:04:55.155144+00:00	None	clear	admin	[]
75	09-09T02:04:48.419288+00:00	example_bash_operator	task_instances	09-08T00:00:00+00:00	admin	[('dag_id', 'example_bash_operator'), ('execution_date', '2019-09-08T00:00:00+00:00')]
{code}
 

For POST actions, the request object looks like this. request.args is empty. request.form is what we need to find those fields. So the action_logging decorator needs to be fixed to look for request.form when the action is POST.
{code:python}
request.args: 
 ImmutableMultiDict([])
 request.form: 
 ImmutableMultiDict([('task_id', 'run_this_last'), ('execution_date', '2019-09-08T00:00:00+00:00'), ('dag_id', 'example_bash_operator'), ('confirmed', 'true'), ('recursive', 'true'),... ('downstream', 'true'), ('origin', 'http://localhost:8080/graph?dag_id=example_bash_operator')])
{code}",yuqian90
AIRFLOW-5443,Use alpine image in Kubernetes's sidecar,,dimberman
AIRFLOW-5442,Druid broker hook get pandas DataFrame implementation,The  *get_pandas_df* of *DruidDbApiHook* returns NotImplementedError.,blcksrx
AIRFLOW-5426,Adjust import path in Dataproc example,,urbaszek
AIRFLOW-5424,Type annotations for GCP hooks,,tobked
AIRFLOW-5423,Type annotations for GCP sensors,,tobked
AIRFLOW-5422,Add type annotations to GCP operators,,tobked
AIRFLOW-5419,Marking task failed doesn't kill the task when it is run through impersonation(run_as_user),"Airflow dag which is run using ""run_as_user"" doesn't get killed when SIGTERM signal is sent to the process.

When task is marked for failure by user or by scheduler - SIGTERM signal is sent to kill the process, but task has a different user id and it requires sudo kill to be killed.

 ",bharathpalaksha
AIRFLOW-5417,Webserver won't start with long DagBag parsing,"When DB disconnect during DagBag creation, webserver will fail to start. This becomes a big issue when DagBag is large enough that creating it takes more time than DB server side connection expire limit. As it will fail webserver start up determinisitically.",yrqls21
AIRFLOW-5416,Simplify the process of continually developing airflow-on-k8s images,"Currently, if a user wants to develop an airflow docker image to push to a k8s cluster, they need to re-download all airflow pip dependencies EVERY time they run `scripts/ci/kubernetes/docker/build.sh`. This PR will allow users to create a ""lower layer"" of all pip dependencies to significantly reduce build times.",dimberman
AIRFLOW-5415,Druid Broker Hook does not respect authentication parameters,Currently the druid broker hook does not pass the login and password from the airflow connection to the broker. ,blcksrx
AIRFLOW-5413,Get pod configuration from JSON or Yaml,"Currently the Kubernetes components of Airflow try to recreate the Kubernetes API, but don't let users pass in their own JSON or YAML. This feature would allow users to do this.",davlum
AIRFLOW-5412,Add get_conn/get_client tests to hooks,,tobked
AIRFLOW-5411,Too much noise is printed during pre-commit checks,There is far too much noise printed during pre-commit checks (for example here [https://travis-ci.org/apache/airflow/jobs/581153290] ). It used to be much less but now the docker build command output is printed in auto-commit output when it fails.,higrys
AIRFLOW-5406,Allow spark_submit_hook to run without pip install kubernetes/airflow[kubernetes],Allow spark_submit_hook to run without pip install kubernetes/airflow[kubernetes],dimberman
AIRFLOW-5403,Fix input check in GKE Operator,,tobked
AIRFLOW-5402,Remove deprecated logger,,fokko
AIRFLOW-5398,Update contrib example DAGs to context manager,,eladk
AIRFLOW-5396,Add button to Auto Refresh Airflow UI,"Currently, to follow up the tasks running on Airflow UI (mainly when we are at development) we should click at button ""Refresh"". When we need to know the finishing of a task we have to click many times at this button.

It would be useful to have a new button ""Auto Refresh"" which we could click and just wait the UI showing us the status while Airflow executes.

We can do another thing meanwhile and just return to the UI screen when it's finished.

So this button should:

when clicked start an auto refresh 

when clicked again stop the auto refresh",lucas.fonseca
AIRFLOW-5395,Fix unloading with column names containing a dot in RedshiftToS3Operator,,feluelle
AIRFLOW-5392,README.md update,Add [Logitravel Group|[http://example.com]|https://www.logitravel.com/] and [Bluekiri|https://bluekiri.com/] to companies that uses Apache Airflow,sergio.soto
AIRFLOW-5390,Remove provide_context,,fokko
AIRFLOW-5387,Pagination is broken when the config variable hide_paused_dags_by_default is set to True because showPaused variable is removed,"Url parameters are clean to remove useless parameters:
{code:python}
// https://github.com/apache/airflow/blob/master/airflow/www/utils.py#L68-L71
    if 'showPaused' in kwargs:
      v = kwargs['showPaused']
      if v or v is None:
         kwargs.pop('showPaused')
{code}

When the configuration variable _hide_paused_dags_by_default_ is set to True, removing the parameter showPaused disable access to the correct paginated pages when navigating to ""Show Paused DAGs"" .
Pagination is fine for the first page but when going on second page, only ""active"" DAGs are presented.

Fix: 
- showPaused parameter should not be removed and therefore lines 68 to 71 (included) could be removed.

 

 ",alrolorojas
AIRFLOW-5382,Add possibility to send dag_folder path to TriggerDagRunOperator,"Hi, Airflow team!
We found that to use TriggerDagRunOperator become very slow in our architecture because there is a lot of dags in default DagBag. And default dag folder is parsed by TriggerDagRunOperator by default in trigger_dag method. We will appreciate if you approve adding a not required parameter 'dag_folder' to get possible send path to separate directory with triggered dag and avoid parsing whole DagFolder dir. 

I will open a PR is idea is not too bad.  ",xnuinside
AIRFLOW-5376,Codecov does not work for new CI  :(,,higrys
AIRFLOW-5373,Super fast pre-commit check for basic python2 compatibility (for cherry-picking),"I thought about a super-fast way of protecting against bad python3 cherry-picks in our v1-10-test branch. From the experience, it looks like we have two types of problems most often:
 # super()
 # python3 type annotations in definition of function parameters/return values
 # python3 type annotations in variables

I tried to find some good ""proper"" solution to check automatically if the code is python2-compliant but surprisingly could not find anything fast and good (actually there are plenty of python3 compat checkers and auto-converters but I could not find good verification is some python3 constructs are used in python2 code).

However it came to me that we can likely do a simple grep that should be able to catch vast majority of those case with very limited (if at all) false positives. It turned out to be a good idea for 1 and 2 (which is vast majority of cases I think) :

Such simple and straigthforward regexp does the work beautifully:

 
{code:java}
"".super\\(\\)|^\\s+def\\s*\\S*\\([^):]*:.*\\)|^\\sdef\\s*\\S*\\(.*\\):\\s*\\-\\>\\s*\\S*"" 
{code}
I managed to find two actual problems (but the code was in comments so no impact ! )- Python3 incompatibilitites in v1-10-test this way (!!!). In master we have 830 matching lines (attached) so I think it's rather good.

I also added another check with py_compile run on all files. It works perfectly for v1-10-test and for master produces plenty of syntax errors. Attached as well. 

 

 

 

 ",higrys
AIRFLOW-5370,Add typehint to GCP's Task Hook,,ryan.yuan
AIRFLOW-5369,Add interactivity to pre-commit image building,"Currently when images are out-dated for pre-commit it just fails with message how to re-run it with rebuild next time. Also when you already run pre-commit, ^C does not work as expected - the main script is killed but the docker images running the checks continue running in the background until they finish. This is pretty annoying as killing such running docker containers is not trivial and for pylint/mypy/flake we can run multiple containers if we run it on many modified files. This is because wer are not using dumb-init to run the checks.

 

This is discouraging a bit, so instead a bit of interactivity can be added:

1) If image gets out-dated a question is asked whether to rebuild it while pre-commit is executed

2) If you run pre-commit directly you do not get asked for rebuild because you can run multiple pre-commit scripts in parallel  (pre-commit does it) so you should fail fast (with helpful instructions)

3) If you run pre-commit via breeze, it is optimised because only the image that is actually needed is rebuilt (and question is asked then)

4) Additionally - you should be able to press ^C and kill all containers running in the background as well as the main script. ",higrys
AIRFLOW-5362,Imports are not ordered properly,Imports in the projects are not ordered properly and there's no unified way to sort them. This causes big headache resolving conflicts and the lookings of the code base :D,yrqls21
AIRFLOW-5360,Type annotations for BaseSensorOperator,,tobked
AIRFLOW-5359,Update type annotations in BaseOperator,,tobked
AIRFLOW-5357,Fix Content-Type for exported variables.json file,"Credits to Anurag Jain for reporting this:

It was observed that the content type is set incorrectly while exporting variables in Apache Airflow. 

>>> 
>>> Steps:
>>> 
>>> 1. Open the Apache Airflow
>>> 2. Create a new variable at /admin/variable/
>>> 3. Keep the key as <input> and value as <input>
>>> 4. Save this variable
>>> 5. Export this variable using Mozilla Firefox Browser
>>> 6. Observe that the downloaded file is saved as <name>.json.htm instead of <name>.json. This happens since Apache airflow sets Response Content-Type as text/html instead of application/json which causes Browser to interpret it as a HTML ",kaxilnaik
AIRFLOW-5351,Move all GCP Cloud SQL tests in 1 file,"There are 2 file currently where these tests live:

1) tests/gcp/hooks/test_cloud_sql.py
2) tests/hooks/test_gcp_sql_hook.py",kaxilnaik
AIRFLOW-5350,Fix bug in the num_retires field in BigQueryHook,"The `num_retries` extra is no set in old connections that were created before 1.10.4, for those fields it's value is None which causes the below error:

From the StackOverflow Post:


{noformat}
 [2019-08-27 02:49:58,076] {cli.py:516} INFO - Running <TaskInstance: cadastro_remessas_paises2.gcs_to_bq 2019-08-27T02:42:43.970619+00:00 [running]> on host cadastroremessaspaises2gcstobq-78f1ea099c3b4e718ba707cb03ffda1e 
    [2019-08-27 02:49:58,136] {logging_mixin.py:95} INFO - [[34m2019-08-27 02:49:58,136[0m] {[34mdiscovery.py:[0m271} INFO[0m - URL being requested: GET [1mhttps://www.googleapis.com/discovery/v1/apis/bigquery/v2/rest[0m[0m 
    [2019-08-27 02:49:59,259] {logging_mixin.py:95} INFO - [[34m2019-08-27 02:49:59,259[0m] {[34mmy_functions_google.py:[0m2224} INFO[0m - Project not included in [1mdestination_project_dataset_table[0m: [1mcadastro_remessas.paises2[0m; using project ""[1mbigdata-staging[0m""[0m 
    [2019-08-27 02:49:59,266] {logging_mixin.py:95} INFO - [[34m2019-08-27 02:49:59,266[0m] {[34mdiscovery.py:[0m867} INFO[0m - URL being requested: POST https://www.googleapis.com/bigquery/v2/projects/bigdata-staging/jobs?alt=json[0m 
    [2019-08-27 02:49:59,266] {taskinstance.py:1047} ERROR - unsupported operand type(s) for +: 'NoneType' and 'int' Traceback (most recent call last):   File ""/usr/local/lib/python3.7/site-packages/airflow/models/taskinstance.py"", line 922, in _run_raw_task
        result = task_copy.execute(context=context)   
        File ""/airflow/dags/git/subfolder/my_functions_google.py"", line 2502, in execute
        cluster_fields=self.cluster_fields)   
        File ""/airflow/dags/git/subfolder/my_functions_google.py"", line 1396, in run_load
        return self.run_with_configuration(configuration)   
        File ""/airflow/dags/git/subfolder/my_functions_google.py"", line 1414, in run_with_configuration
        .execute(num_retries=self.num_retries)   
        File ""/usr/local/lib/python3.7/site-packages/googleapiclient/_helpers.py"", line 130, in positional_wrapper
        return wrapped(*args, **kwargs)   
        File ""/usr/local/lib/python3.7/site-packages/googleapiclient/http.py"", line 851, in execute
        method=str(self.method), body=self.body, headers=self.headers)   
        File ""/usr/local/lib/python3.7/site-packages/googleapiclient/http.py"", line 153, in _retry_request
        for retry_num in range(num_retries + 1): TypeError: unsupported operand type(s) for +: 'NoneType' and 'int'
{noformat}
",kaxilnaik
AIRFLOW-5348,Deprecated chart view doesn't escape label,,ashb
AIRFLOW-5345,Extract SqlSensor's get_hook functionality to an auxiliary function,"Due to security reasons we have an alternative PostgreSQL hook that fetches credentials dynamically.

We would still like to use the SqlSensor functionality as is as our hook is still backed by sqlalchemy.

By extracting SqlSensor's get_hook into a method we can override the _get_hook method without affecting the functionality of the hook.

 ",gmaliar
AIRFLOW-5342,Can't run initdb using Microsoft SQL server,"I'm using Microsoft SQL server 2016 as metadata db (for the sql_alchemy_conn), and pymssql as the Python driver.

When running airflow initdb, airflow creates a table named `TaskInstance` with column `pool` that is initially Nullable (when the table is created).

Later, an index named `ti_pool` on a few columns, including `pool` column in table `TaskInstance`.

Then, airflow will try to alter table `TaskInstance` and change the column `pool` to `NOT NULL`.

This does not work on Microsoft SQL Server since a column with an index defined on it cannot be changed, unless the index is deleted before.",mortenbpost
AIRFLOW-5339,Wait forever spark hook if python version is 3,"I found spark hook is not terminating forever.

Because Python 3 *{{subprocess.Popen}}* return {{*bytes*}}, *{{iter(sp.stdout.readlin, '')}}* pattern codes are not work.

I will submit PR.",toughrogrammer
AIRFLOW-5338,Add a RedsfhitToS3Operator,Create an Airflow operator that queries Redshift and persists the results to S3. We should be able to leverage the existing code in https://github.com/apache/airflow/blob/master/airflow/contrib/operators/dynamodb_to_s3.py to handle the flush to s3 logic. We should abstract that logic to a base class and let RedshiftToS3Operator and DynamodbToS3Operator inherits that base class,milton0825
AIRFLOW-5337,Add a PostgresToS3Operator,Create an Airflow operator that queries Postgres and persists the results to S3. We should be able to leverage the existing code in https://github.com/apache/airflow/blob/master/airflow/contrib/operators/dynamodb_to_s3.py to handle the flush to s3 logic. We should abstract that logic to a base class and let PostgresToS3Operator and DynamodbToS3Operator inherits that base class,milton0825
AIRFLOW-5335,Update GCSHook methods so they need min IAM perms,"After we refactored to using Storage client in GCS, we need more IAM permissions.

This is because we use `get_bucket` method which requires `storage.bucket.list` permission. Instead of that if we use `bucket` method that creates Bucket object we don't need the above permission.",kaxilnaik
AIRFLOW-5320, Add system tests for PubSub,,urbaszek
AIRFLOW-5318,BigQuery - Option to specify location when creating an empty dataset,"h2. Problem

To specify the location for a new empty dataset, the user has to add it to the _dataset_reference_.
h2. Proposed improvement

Add _location_ as an extra argument to the BigQueryCreateEmptyDatasetOperator to specify the location of the new dataset.",mbanayosy
AIRFLOW-5316,Checklicence with Apache RAT tool takes long time on Mac,"It takes long time because you cannot make it run selectively only on changed files and Mac OSX filesystem for Docker is slow enough to make it run for 30 seconds.

 

The solution is to change it so that it is only triggered by  a file that does not change too often so that it is not run locally but it will be run on CI with --al-files flag.",higrys
AIRFLOW-5315,TaskInstance not updating from DB when user changes executor_config,"There is a bug with the {{executor_config}} that does not reflect changes after update in “Task Instance Attribute” (it reflects on Task Attributes, though).",dimberman
AIRFLOW-5313,Add params support for awsbatch_operator,AWS Batch supports [passing parameters to jobs|https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/batch.html#Batch.Client.submit_job].  Update AWSBatchOperator to include an optional parameters value.,jrowen
AIRFLOW-5312,KubernetesPodOperator hangs when kubernetes API times out,"The KubernetesPodOperator launches a Pod in Kubernetes and then continues to watch its state and stream its logs. This is done via the Kubernetes Api. Not all calls to the Kubernetes Api have a timeout defined, so the operator will wait indefinitely for a response and therefore get stuck.

This is the same issue as https://issues.apache.org/jira/browse/AIRFLOW-5282 but now in the KubernetesPodOperator.

The solution is to add a timeout on all Api calls, and handle the errors (catching them, adding tenacity retrying where appropriate).

 ",rdeboo
AIRFLOW-5309,Use assert_called_once or has_calls in tests,Using mock.assert_call_with method can result in flaky tests (ex. iterating through dict in python 3.5 which does not store order of elements). That's why it's better to use assert_called_once_with or has_calls methods.,urbaszek
AIRFLOW-5306,Fix the display of links when they contain special characters,,kamil.bregula
AIRFLOW-5305,Sort extra links by name,,kamil.bregula
AIRFLOW-5304,Fix extra links in BigQueryOperator with multiple queries,,kamil.bregula
AIRFLOW-5303,Use project_id from GCP credentials,,kamil.bregula
AIRFLOW-5302,Fix bug in none_skipped Trigger Rule ,"The task been set none_skipped run immediately even before upstream tasks complete execution.

Check the attached gif",kaxilnaik
AIRFLOW-5296,Do Not pickle DAGs by default,Pickling is not safe and some fields are not pickleable.,kaxilnaik
AIRFLOW-5292,Allow ECSOperator to tag tasks,In its current state the ECSOperator does not allow tagging tasks. Goal of this ticket is to enable the ECSOperator to tag tasks.,phoffmann
AIRFLOW-5290,Add is updated before and between to GoogleCloudStorageHook,"GoogleCloudStorageHook provides the function to test if a blob is updated after a given timestamp, but not before in between two timestamps.",derrik
AIRFLOW-5289,Make `body` parameter templated for all gcp operators,"Several gcp operators make the `body` parameter templated, but a handful don't. It's often useful to control `body` fields via templates, and we should be consistent, so let's make `body` templated for all gcp operators.",jmcarp
AIRFLOW-5286,Add requeue logic to airflow scheduler and executor,"Airflow queued tasks sometime stuck for long time without being picked up. In many cases the root cause is hard to debug. 

The proposed solution here is to add requeue logic in airflow scheduler and executor. For tasks with a queued state in metaDB. If the task did not show in executor.queued_tasks in a certain inteval, we requeue the task so that stuck tasks can be released and picked up. The solution was used in Airbnb and proven to be helpful. ",ellenwang
AIRFLOW-5284,Replace warn by warning,"Got a warning on using a deprecated method:
dag_processing.py:689: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead",fokko
AIRFLOW-5282,KubernetesExecutor hangs when kubernetes API times out,"While migrating to a different Kubernetes cluster, we observe that the scheduler hangs very frequently. No output is generated in the logs. The UI states:

`{{The scheduler does not appear to be running. Last heartbeat was received 9 minutes ago.`}}

I've attached py-spy to the scheduler process to investigate. This is the output:

 
{code:java}
  %Own   %Total  OwnTime  TotalTime  Function (filename:line)
 100.00% 100.00%   38.00s    38.00s   recv_into (OpenSSL/SSL.py:1821)
  0.00% 100.00%   0.000s    38.00s   sync (airflow/contrib/executors/kubernetes_executor.py:795)
  0.00% 100.00%   0.000s    38.00s   request_encode_body (urllib3/request.py:150)
  0.00% 100.00%   0.000s    38.00s   begin (http/client.py:307)
  0.00% 100.00%   0.000s    38.00s   recv_into (urllib3/contrib/pyopenssl.py:304)
  0.00% 100.00%   0.000s    38.00s   wrapper (airflow/utils/cli.py:74)
  0.00% 100.00%   0.000s    38.00s   create_namespaced_pod_with_http_info (kubernetes/client/apis/core_v1_api.py:6206)
  0.00% 100.00%   0.000s    38.00s   _make_request (urllib3/connectionpool.py:383)
  0.00% 100.00%   0.000s    38.00s   _execute_helper (airflow/jobs/scheduler_job.py:1412){code}

 
 Next, I've added debug logging to `cli.py` on urlib3. This confirmed that the scheduler hangs at the point where no response comes from the kubernetes API:
{code:java}

(base) macbook:~ roland$ kubectl logs airflow-scheduler-7c5d6df8f4-w4tpn > logs.txt
(base) macbook:~ roland$ cat logs.txt | grep pod_launcher.py
[2019-08-20 14:40:10,471] {pod_launcher.py:60} DEBUG - Pod Creation Request:
[2019-08-20 14:40:10,652] {pod_launcher.py:63} DEBUG - Pod Creation Response: {'api_version': 'v1',
[2019-08-20 14:43:03,213] {pod_launcher.py:60} DEBUG - Pod Creation Request:
[2019-08-20 14:43:03,296] {pod_launcher.py:63} DEBUG - Pod Creation Response: {'api_version': 'v1',
[2019-08-20 14:44:38,078] {pod_launcher.py:60} DEBUG - Pod Creation Request:
[2019-08-20 14:44:38,409] {pod_launcher.py:63} DEBUG - Pod Creation Response: {'api_version': 'v1',
[2019-08-20 14:44:39,804] {pod_launcher.py:60} DEBUG - Pod Creation Request:
[2019-08-20 14:44:39,867] {pod_launcher.py:63} DEBUG - Pod Creation Response: {'api_version': 'v1',
[2019-08-20 14:45:21,785] {pod_launcher.py:60} DEBUG - Pod Creation Request:
[2019-08-20 14:45:22,040] {pod_launcher.py:63} DEBUG - Pod Creation Response: {'api_version': 'v1',
[2019-08-20 14:50:22,224] {pod_launcher.py:60} DEBUG - Pod Creation Request:
{code}
It appears that the timeout can already be specified in the `airflow.cfg`, but the default is infinite. In kubernetes section: `kube_client_request_args = {""_request_timeout"" : [60,60] }`

When I added the timeout, the scheduler will not hang anymore, but exit with an error (and be restarted by Kubernetes).

I propose a PR which:
 * Adds a default _request_timeout in airflow.cfg, so that default installations will not hang indefinitely
 * Catches the errors in the scheduler, and retries scheduling the task, in a similar fashion to how ApiExcptions are caught. (https://github.com/apache/airflow/blob/46885ccdbc8618f295b8f986da95c1abbb85bb02/airflow/executors/kubernetes_executor.py#L800-L803)

 

 ",rdeboo
AIRFLOW-5280,aws_default's region_name defaults to us-east-1 which causes major confusion for connections that do not use this region,"The `aws_default`  by default specifies the `region_name` to be
 `us-east-1` in its `extra` field. This causes trouble when the desired
 AWS account uses a different region as this default value has priority
 over the $AWS_REGION and $AWS_DEFAULT_REGION environment variables,
 gets passed directly to `botocore` and does not seem to be documented.

It seems that the best way of dealing with this situation would be to remove this default variable. The result of this would be that all aws_default connection parameters would have to manually define it which seems to be consistent with ""explicit better than implicit"" philosophy.",mrshu
AIRFLOW-5278,Add a mark failure button under mark success button in the tree view,"Currently there is no way to mark a task as failure after clicking on a task instance in the tree view. There should be a ""Mark Failure"" button under the ""Mark Success"" one.

Marking a task as successful might not be desired as it becomes hard to distinguish which tasks actually ran and which did not by viewing the UI.",smcqueen
AIRFLOW-5276,Remove unused helpers from airflow.utils.helpers,"At the very least {{is_in}} is not used in the code base any more, and it's behaviour has changed subtly in Python 3.7 so the tests fail there too.

So lets remove that helper, and it's associated tests, plus anything else not used any more.",houqp
AIRFLOW-5275,Add support for template parameters in DataprocWorkflowTemplateInstantiateOperator,"Implement passing parameter values when instantiating parametrized Dataproc workflow templates. While Dataproc API supports it ([https://cloud.google.com/dataproc/docs/reference/rest/v1/projects.locations.workflowTemplates/instantiate]) it's not implemented in DataprocWorkflowTemplateInstantiateOperator yet.

 

The implementation plan is to add a map of parameters to request body:
{code:java}
body={
      'requestId': str(uuid.uuid4()),
      'parameters': parameters
}
{code}
in [https://github.com/apache/airflow/blob/master/airflow/contrib/operators/dataproc_operator.py]",michal.brys
AIRFLOW-5274,dag.loading-duration metric name too long,"see conversation here - https://github.com/apache/airflow/pull/5350/files/5375a9af21db970651c17f947558b6fc180f0dd3#r315774306

problem:
when a single file generates a lot of dags, the metric name is huge

possible solution:
- is it not possible to append the name of the file where the dags are generated instead of appending {{'_'.join(dag_ids)}}  ?
- update the description in `docs/metrics.rst` to match the implementation
",taofeng
AIRFLOW-5270,JSONDecodeError in DockerOperator when force_pull,"Lines of the Docker API response usually end with ""\r\n"". We .strip() them to correct for this, but sometimes two lines are joined together, creating one line that is not valid JSON. The exact error we see is

```
json.decoder.JSONDecodeError: Extra data: line 2 column 1 (char 55)
```

And can be reproduced with the following code

```

pip install docker


from docker import APIClient
import json

cli = APIClient(base_url='unix://var/run/docker.sock')

for l in cli.pull('hello-world', stream=*True*):
    output = json.loads(l.decode('utf-8').strip()
```

which is exactly what this line of `DockerOperator` does which causes it to crash
[https://github.com/apache/airflow/blob/46885ccdbc8618f295b8f986da95c1abbb85bb02/airflow/operators/docker_operator.py#L259]

The solution is to replace ""\r\n"" with ""\n"" before json parsing",mattpetersen
AIRFLOW-5269,Reuse existing session of '/health' endpoint to get most recent Scheduler Job,,kaxilnaik
AIRFLOW-5268,Rectify DAG naming conventions,,basph
AIRFLOW-5266,AWS Athena Hook only returns first 1000 results,"When using the get_query_results function in the AWSAthenaHook, you will only get the first 1000 results for a given query execution id. See the [boto3 documentation|https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/athena.html?highlight=start_query_execution#Athena.Client.get_query_results]

I'll be creating a pr in the next few days.",lindsable
AIRFLOW-5262,Update timeout exception to include dag name,"This exception ([https://github.com/apache/airflow/blob/f06ae8fbe9a5c786f0299872abc85d9960b05886/airflow/sensors/base_sensor_operator.py#L114-L116]) could be made clearer for cursory log investigation (especially in third party paging systems) by including the DAG name with this timeout error message.

 ",jomeke
AIRFLOW-5260,Allow empty uri arguments in connection strings,Allow empty string arguments in the connection string. This is needed if you're connecting to RDS using IAM role instead of a password then we would want to pass the {{aws_conn_id}} argument as empty.,sb2nov
AIRFLOW-5258,"ElasticSearch log handler, has 2 times of hours (%H and %I) in _clean_execution_date instead of %H and %M",,a_soldatenko
AIRFLOW-5257,ElasticSearch log handler errors when attemping to close logs,"{code:python}
Traceback (most recent call last):
  File ""/usr/bin/airflow"", line 32, in <module>
    args.func(args)
  File ""/usr/lib/python3.7/site-packages/airflow/utils/cli.py"", line 74, in wrapper
    return f(*args, **kwargs)
  File ""/usr/lib/python3.7/site-packages/airflow/bin/cli.py"", line 523, in run
    logging.shutdown()
  File ""/usr/lib/python3.7/logging/__init__.py"", line 2039, in shutdown
    h.close()
  File ""/usr/lib/python3.7/site-packages/airflow/utils/log/es_task_handler.py"", line 253, in close
    self.writer.close()
AttributeError: 'ElasticsearchTaskHandler' object has no attribute 'writer'

{code}",a_soldatenko
AIRFLOW-5251,crash due to missing typing-extension dependency for python 3.7,Latest master now crashes with python 3.7 due to missing typing-extension dependency.,houqp
AIRFLOW-5250,dmypy still reports error for gcp hooks,"dmypy still reports the following error:
{code:java}
airflow/contrib/hooks/gcp_cloud_build_hook.py:51: error: Need type annotation for '_conn'
airflow/contrib/hooks/gcp_compute_hook.py:52: error: Need type annotation for '_conn'
airflow/contrib/hooks/gcp_sql_hook.py:739: error: Need type annotation for '_conn'
airflow/gcp/hooks/functions.py:43: error: Need type annotation for '_conn'{code}",houqp
AIRFLOW-5247,Getting all dependencies from NPM can be moved up in Dockerfile,Retrieving all NPM dependencies can happen before updating apt-get dependencies. It takes long time and it does not change that often so it is quite OK to move it before adding setup.py in dockerfile,higrys
AIRFLOW-5245,Tracks no tasks in the scheduler,more metrics are required to understand scheduler behavior,bolke
AIRFLOW-5244,Add all possible themes to default_webserver_config.py,Add full list of FAB themes from [https://github.com/dpgaspar/Flask-AppBuilder-Skeleton/blob/b27b73f79680bfe58fa75a0b15a73cea94db89a4/config.py#L98] to the default webserver config file.,rywalker
AIRFLOW-5243,Make airflow/configuration.py and airflow/exceptions.py Pylint compatible,"Fix all Pylint messages in airflow/configuration.py. To start; running scripts/ci/ci_pylint.sh on master should produce no messages. 
1. Remove the file airflow/configuration.py from the blacklist. 
2. Run scripts/ci/ci_pylint_main.sh to see all messages on the no longer blacklisted files. 
3. Fix all messages and create PR.",kik
AIRFLOW-5240,latest version of Kombu is breaking airflow,"I believe kombu 4.6.4 released on Aug 14th (2 days ago) is breaking CeleryExecutor

[https://github.com/celery/kombu/releases/tag/4.6.4]

I was able to resolve it by pinning kombu at 4.6.3

 

Refer to the stack trace below:
{code:java}
Traceback (most recent call last):
  File ""/usr/local/bin/airflow"", line 22, in <module>
    from airflow.bin.cli import CLIFactory
  File ""/usr/local/lib/python2.7/dist-packages/airflow/bin/cli.py"", line 55, in <module>
    from airflow import jobs, settings
  File ""/usr/local/lib/python2.7/dist-packages/airflow/jobs/__init__.py"", line 21, in <module>
    from airflow.jobs.base_job import BaseJob  # noqa: F401
  File ""/usr/local/lib/python2.7/dist-packages/airflow/jobs/base_job.py"", line 48, in <module>
    class BaseJob(Base, LoggingMixin):
  File ""/usr/local/lib/python2.7/dist-packages/airflow/jobs/base_job.py"", line 82, in BaseJob
    executor=executors.get_default_executor(),
  File ""/usr/local/lib/python2.7/dist-packages/airflow/executors/__init__.py"", line 48, in get_default_executor
    DEFAULT_EXECUTOR = _get_executor(executor_name)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/executors/__init__.py"", line 76, in _get_executor
    from airflow.executors.celery_executor import CeleryExecutor
  File ""/usr/local/lib/python2.7/dist-packages/airflow/executors/celery_executor.py"", line 27, in <module>
    from celery import Celery
  File ""/usr/local/lib/python2.7/dist-packages/celery/local.py"", line 509, in __getattr__
    module = __import__(self._object_origins[name], None, None, [name])
  File ""/usr/local/lib/python2.7/dist-packages/celery/app/__init__.py"", line 5, in <module>
    from celery import _state
  File ""/usr/local/lib/python2.7/dist-packages/celery/_state.py"", line 17, in <module>
    from celery.utils.threads import LocalStack
  File ""/usr/local/lib/python2.7/dist-packages/celery/utils/__init__.py"", line 9, in <module>
    from .nodenames import worker_direct, nodename, nodesplit
  File ""/usr/local/lib/python2.7/dist-packages/celery/utils/nodenames.py"", line 9, in <module>
    from kombu.entity import Exchange, Queue
  File ""/usr/local/lib/python2.7/dist-packages/kombu/entity.py"", line 9, in <module>
    from .serialization import prepare_accept_content
  File ""/usr/local/lib/python2.7/dist-packages/kombu/serialization.py"", line 456, in <module>
    for ep, args in entrypoints('kombu.serializers'):  # pragma: no cover
  File ""/usr/local/lib/python2.7/dist-packages/kombu/utils/compat.py"", line 89, in entrypoints
    for ep in importlib_metadata.entry_points().get(namespace, [])
  File ""/usr/local/lib/python2.7/dist-packages/importlib_metadata/__init__.py"", line 456, in entry_points
    ordered = sorted(eps, key=by_group)
  File ""/usr/local/lib/python2.7/dist-packages/importlib_metadata/__init__.py"", line 454, in <genexpr>
    dist.entry_points for dist in distributions())
  File ""/usr/local/lib/python2.7/dist-packages/importlib_metadata/__init__.py"", line 364, in <genexpr>
    cls._search_path(path, pattern)
  File ""/usr/local/lib/python2.7/dist-packages/importlib_metadata/__init__.py"", line 373, in _switch_path
    return pathlib.Path(path)
  File ""/usr/local/lib/python2.7/dist-packages/pathlib2/__init__.py"", line 1256, in __new__
    self = cls._from_parts(args, init=False)
  File ""/usr/local/lib/python2.7/dist-packages/pathlib2/__init__.py"", line 898, in _from_parts
    drv, root, parts = self._parse_args(args)
  File ""/usr/local/lib/python2.7/dist-packages/pathlib2/__init__.py"", line 891, in _parse_args
    return cls._flavour.parse_parts(parts)
  File ""/usr/local/lib/python2.7/dist-packages/pathlib2/__init__.py"", line 250, in parse_parts
    parsed.append(intern(x))
TypeError: can't intern subclass of string
{code}",kaxilnaik
AIRFLOW-5239,Small typo and incorrect tests in CONTRIBUTING.md,"There is a small typo in CONTRIBUTING.md under the ""Local virtualenv development environment"" heading:

bq. The tests in Airflow are a mixture of unit and integration tests and some of them require those components to be setup. Only real unit tests can be run bu default in local environment.

Additionally, the section about running tests in the Docker container [lists only {{./scripts/ci/in_container/run_pylint.sh}}|https://github.com/apache/airflow/blob/master/CONTRIBUTING.md#running-static-code-analysis-in-the-docker-compose-environment] to run pylint tests, but both {{./scripts/ci/in_container/run_pylint_main.sh}} and {{./scripts/ci/in_container/run_pylint_tests.sh}} are both present.",coopergillan
AIRFLOW-5236,No longer need to import all pip packages every time we deploy to kubernetes CI,"Currently, when we wish to build the docker image needed to launch the airflow k8s tests, we have to import all pip packages every time. This makes testing on k8s very tedious. These changes would ensure that pip packages would only be installed when there is a change in requirements.",dimberman
AIRFLOW-5235,Airflow CI kubernetes is not properly creating users,There is a bug in the k8s setup that prevents it from properly creating users. This makes the UI unusable.,dimberman
AIRFLOW-5221,Add host alias support to the KubernetesPodOperator,"[https://kubernetes.io/docs/concepts/services-networking/add-entries-to-pod-etc-hosts-with-host-aliases/]

The only wait to manage DNS entries for kubernetes pods is through hosts aliases ",d_mink99
AIRFLOW-5220,Easy form to create airflow dags,"The airflow usage threshold is higher and the user must write a Python dag file. However, many users don't write Python, they want to create dags directly from forms.",huangyan
AIRFLOW-5219,Alarm if the task is not executed within the expected time range.,"When using Airflow, user has an expected time range for the task. Beyond this range, the user expects to get an alert instead of performing the task directly. 

They may not want the task to be executed automatically, and then manually perform the task after analyzing the cause.",huangyan
AIRFLOW-5218,"AWS Batch Operator - status polling too often, esp. for high concurrency","The AWS Batch Operator attempts to use a boto3 feature that is not available and has not been merged in years, see
 - [https://github.com/boto/botocore/pull/1307]
 - see also [https://github.com/broadinstitute/cromwell/issues/4303]

This is a curious case of premature optimization. So, in the meantime, this means that the fallback is the exponential backoff routine for the status checks on the batch job. Unfortunately, when the concurrency of Airflow jobs is very high (100's of tasks), this fallback polling hits the AWS Batch API too hard and the AWS API throttle throws an error, which fails the Airflow task, simply because the status is polled too frequently.

Check the output from the retry algorithm, e.g. within the first 10 retries, the status of an AWS batch job is checked about 10 times at a rate that is approx 1 retry/sec. When an Airflow instance is running 10's or 100's of concurrent batch jobs, this hits the API too frequently and crashes the Airflow task (plus it occupies a worker in too much busy work).
{code:java}
In [4]: [1 + pow(retries * 0.1, 2) for retries in range(20)] 
 Out[4]: 
 [1.0,
 1.01,
 1.04,
 1.09,
 1.1600000000000001,
 1.25,
 1.36,
 1.4900000000000002,
 1.6400000000000001,
 1.81,
 2.0,
 2.21,
 2.4400000000000004,
 2.6900000000000004,
 2.9600000000000004,
 3.25,
 3.5600000000000005,
 3.8900000000000006,
 4.24,
 4.61]{code}
Possible solutions are to introduce an initial sleep (say 60 sec?) right after issuing the request, so that the batch job has some time to spin up. The job progresses through a through phases before it gets to RUNNING state and polling for each phase of that sequence might help. Since batch jobs tend to be long-running jobs (rather than near-real time jobs), it might help to issue less frequent polls when it's in the RUNNING state. Something on the order of 10's seconds might be reasonable for batch jobs? Maybe the class could expose a parameter for the rate of polling (or a callable)?

 

Another option is to use something like the sensor-poke approach, with rescheduling, e.g.

- [https://github.com/apache/airflow/blob/master/airflow/sensors/base_sensor_operator.py#L117]

 ",dazza
AIRFLOW-5217,Fix Pod docstring,{{Pod}} class docstring is currently out of date with regards to its {{__init__}} method's arguments.,pgag
AIRFLOW-5216,Add Azure File Share Sensor,Add sensor to check if a file exists on Azure File Share using the existing AzureFileShareHook.,mwyau
AIRFLOW-5215,Add sidecar container support to Pod object,Add sidecar container support to Pod object.,pgag
AIRFLOW-5213,"DockerOperator failing when the docker default logging drivers are other than 'journald','json-file'","Background:

Docker can be configured with multiple logging drivers.
 * syslog
 * local
 * json - file
 * journald
 * local
 * gelf
 * fluentd
 * awslogs
 * splunk
 * etwlogs
 * gcplogs
 * Logentries

But reading docker logs is supported only with drivers local , json-file , journald

Docker documentation: [https://docs.docker.com/config/containers/logging/configure/]

 

Description:

When a docker is configured with a logging driver other than local , json-file , jourmald , Airflow Tasks which are using DockerOperator are failing with an error

_docker.errors.APIError: 501 Server Error: Not Implemented (""configured logging driver does not support reading"")_

Issue exists in the below lines of the code when the operator is trying to read the logs by attaching the container.

```
{code:python}
line = ''
for line in self.cli.attach(container=self.container['Id'], stdout=True, stderr=True, stream=True):  
    line = line.strip()
    if hasattr(line, 'decode'):
       line = line.decode('utf-8')
    self.log.info(line)
{code}
```

 

 ",bonuvm
AIRFLOW-5211,Add pass_value to template_fields -- BigQueryValueCheckOperator,"There's use cases to fill *pass_value* from *XCom* when use *BigQueryValueCheckOperator*, so add pass_value to template_fields.",damon09273
AIRFLOW-5209,Fix Documentation build,"Currently, if you try to build on master or 1.10.4 it fails with the following error:


{noformat}
  File ""/home/docs/checkouts/readthedocs.org/user_builds/airflow/envs/latest/lib/python3.7/site-packages/docutils/statemachine.py"", line 460, in check_line
    return method(match, context, next_state)
  File ""/home/docs/checkouts/readthedocs.org/user_builds/airflow/envs/latest/lib/python3.7/site-packages/docutils/parsers/rst/states.py"", line 2753, in underline
    self.section(title, source, style, lineno - 1, messages)
  File ""/home/docs/checkouts/readthedocs.org/user_builds/airflow/envs/latest/lib/python3.7/site-packages/docutils/parsers/rst/states.py"", line 327, in section
    self.new_subsection(title, lineno, messages)
  File ""/home/docs/checkouts/readthedocs.org/user_builds/airflow/envs/latest/lib/python3.7/site-packages/docutils/parsers/rst/states.py"", line 395, in new_subsection
    node=section_node, match_titles=True)
  File ""/home/docs/checkouts/readthedocs.org/user_builds/airflow/envs/latest/lib/python3.7/site-packages/docutils/parsers/rst/states.py"", line 282, in nested_parse
    node=node, match_titles=match_titles)
  File ""/home/docs/checkouts/readthedocs.org/user_builds/airflow/envs/latest/lib/python3.7/site-packages/docutils/parsers/rst/states.py"", line 196, in run
    results = StateMachineWS.run(self, input_lines, input_offset)
  File ""/home/docs/checkouts/readthedocs.org/user_builds/airflow/envs/latest/lib/python3.7/site-packages/docutils/statemachine.py"", line 239, in run
    context, state, transitions)
  File ""/home/docs/checkouts/readthedocs.org/user_builds/airflow/envs/latest/lib/python3.7/site-packages/docutils/statemachine.py"", line 460, in check_line
    return method(match, context, next_state)
  File ""/home/docs/checkouts/readthedocs.org/user_builds/airflow/envs/latest/lib/python3.7/site-packages/docutils/parsers/rst/states.py"", line 2326, in explicit_markup
    nodelist, blank_finish = self.explicit_construct(match)
  File ""/home/docs/checkouts/readthedocs.org/user_builds/airflow/envs/latest/lib/python3.7/site-packages/docutils/parsers/rst/states.py"", line 2338, in explicit_construct
    return method(self, expmatch)
  File ""/home/docs/checkouts/readthedocs.org/user_builds/airflow/envs/latest/lib/python3.7/site-packages/docutils/parsers/rst/states.py"", line 2081, in directive
    directive_class, match, type_name, option_presets)
  File ""/home/docs/checkouts/readthedocs.org/user_builds/airflow/envs/latest/lib/python3.7/site-packages/docutils/parsers/rst/states.py"", line 2130, in run_directive
    result = directive_instance.run()
  File ""/home/docs/checkouts/readthedocs.org/user_builds/airflow/envs/latest/lib/python3.7/site-packages/sphinx/ext/autodoc/directive.py"", line 121, in run
    documenter_options = process_documenter_options(doccls, self.config, self.options)
  File ""/home/docs/checkouts/readthedocs.org/user_builds/airflow/envs/latest/lib/python3.7/site-packages/sphinx/ext/autodoc/directive.py"", line 73, in process_documenter_options
    return Options(assemble_option_dict(options.items(), documenter.option_spec))
  File ""/home/docs/checkouts/readthedocs.org/user_builds/airflow/envs/latest/lib/python3.7/site-packages/docutils/utils/__init__.py"", line 328, in assemble_option_dict
    options[name] = convertor(value)
  File ""/home/docs/checkouts/readthedocs.org/user_builds/airflow/envs/latest/lib/python3.7/site-packages/sphinx/ext/autodoc/__init__.py"", line 82, in members_option
    return [x.strip() for x in arg.split(',')]
AttributeError: 'bool' object has no attribute 'split'

Exception occurred:
  File ""/home/docs/checkouts/readthedocs.org/user_builds/airflow/envs/latest/lib/python3.7/site-packages/sphinx/ext/autodoc/__init__.py"", line 82, in members_option
    return [x.strip() for x in arg.split(',')]
AttributeError: 'bool' object has no attribute 'split'
{noformat}

Our doc build on RTD also fails with the same error: https://readthedocs.org/projects/airflow/builds/9511663/

This is caused where the version of Sphinx < 2

Using the latest Sphinx version solves this for us.",kaxilnaik
AIRFLOW-5208,Passed **kwargs to push_by_returning,"Without **kwargs push_by_returning was giving error, so added that parameter.

[https://github.com/apache/airflow/blob/master/airflow/example_dags/example_xcom.py]",gauravprachchhak
AIRFLOW-5207,Mark Success and Mark Failed views error out due to DAG reassignment,"When trying to clear a task after upgrading to 1.10.4, I get the following traceback:
{code:java}
File ""/usr/local/lib/python3.7/site-packages/airflow/www/views.py"", line 1451, in failed future, past, State.FAILED) File ""/usr/local/lib/python3.7/site-packages/airflow/www/views.py"", line 1396, in _mark_task_instance_state task.dag = dag File ""/usr/local/lib/python3.7/site-packages/airflow/models/baseoperator.py"", line 509, in dag ""The DAG assigned to {} can not be changed."".format(self)) airflow.exceptions.AirflowException: The DAG assigned to <Task(SubDagOperator): my-dag> can not be changed.{code}
This should be a simple fix by either dropping the offending line, or if it is required to keep things working, just set the private attribute instead:
{code:java}
task._dag = dag
{code}",marcusian
AIRFLOW-5182,"""KubernetesOperator"" isn't implemented","h2. Problem

I encountered the following error with `KubernetesOperator`.
{code:java}
Broken DAG: [/root/airflow/dags/sample_k8s.py] cannot import name KubernetesOperator
{code}
h2. Investigation

The following document is written a sample code to describe how to use Kubernetes Executor.
 [https://airflow.apache.org/kubernetes.html#kubernetes-operator]

There is a line `import KubernetesOperator`, but I think it isn't implemented on airflow and it isn't used in this script.
{code:java}
from airflow.contrib.operators import KubernetesOperator
{code}
I couldn't find `KubernetesOperator` in the following dirs.
 * [https://github.com/apache/airflow/tree/1.10.4/airflow/contrib/operators]
 * [https://github.com/apache/airflow/tree/1.10.4/airflow/operators]

Could you check it?",lcolium
AIRFLOW-5179,Top level __init__.py breaks imports,"The recent commit [3724c2aaf4cfee4a60f6c7231777bfb256090c7c|https://github.com/apache/airflow/commit/3724c2aaf4cfee4a60f6c7231777bfb256090c7c] to master introduced a {{__init__.py}} file in the project root folder, which basically breaks all imports in local development ({{pip install -e .}}) as it turns the project root into a package.",ashb
AIRFLOW-5176,Add integration with Azure Data Explorer,"Add a hook and an operator that allow sending queries to Azure Data Explorer (Kusto) cluster.

ADX (Azure Data Explorer) is relatively new but very promising analytics data store / data processing offering in Azure:

[https://azure.microsoft.com/en-us/services/data-explorer/]

 

PR: [https://github.com/apache/airflow/pull/5785]",spektom
AIRFLOW-5175,Update pagination symbols for dag run and task instances,"same as -AIRFLOW-5067-

(for DAGS page) but for dag runs and task instances pages",trker
AIRFLOW-5170,Add static checks consistent licences for python files,"Automated check for encoding pragma, consisten licence files can be added for python files.

Since we have pylint checks in pre-commits added we should also make sure to fix all pylint related changes however for all the changed python files.",higrys
AIRFLOW-5169,Pass GCP Project ID explicitly to StorageClient in GCSHook,"If Project_id is not passed to the StorageClient it tries to infer the Project_ID from the environment variables or Cloud SDK.

If neither env variables is set nor Cloud SDK is configured it will error with the following message:

{code}
google.auth.exceptions.DefaultCredentialsError: Could not automatically determine credentials. Please set GOOGLE_APPLICATION_CREDENTIALS or explicitly create credentials and re-run the application. For more information, please see https://cloud.google.com/docs/authentication/getting-started
{code}
",kaxilnaik
AIRFLOW-5168,Dataproc operators fail on 1.10.4,"We've found a problem with DataprocOperators introduced in 1.10.4.

There is a commit to master which adds label support, this has a change in the hook & base operator: https://github.com/apache/airflow/commit/4d58f36df3b533edfce873799dc161ee34571d63#diff-bc55ad8b749b7a136f51ffabfdbaf13dL684

Then in the 1.10.4 branch there is a massive commit for GCP DLP: https://github.com/apache/airflow/commit/222c6ac45de54ed0398645c1d456a592e194325b

This seems to have leaked the operator part of the label change only: https://github.com/apache/airflow/commit/222c6ac45de54ed0398645c1d456a592e194325b#diff-bc55ad8b749b7a136f51ffabfdbaf13dR693

This then manifests as this error when trying to execute a Dataproc job: {{[2019-08-09 14:32:02,971] \{taskinstance.py:1047} ERROR - _DataProcJobBuilder instance has no attribute 'add_labels'}}",kaxilnaik
AIRFLOW-5165,Make Dataproc highly available,"Currently the number of master nodes in dataproc create operator is set to 1.
There is no way to increase the number of master nodes in dataproc.

Inorder to have HA dataproc, Google Cloud suggests to have 3 master nodes
[https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/high-availability|https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/high-availability]

The number of masters should be a config in DataprocCreateOperator so that the created dataproc is HA",mauliksoneji
AIRFLOW-5158,Google Sheets hook,"Need a hook that can both read and write from a google sheet.

Planning to implement the  Reading & Writing Cell Values of the API:

[https://developers.google.com/sheets/api/guides/values]

In particular get / update / append both single range and batch",hagope
AIRFLOW-5153,Add option to force delete a non-empty Google BQ dataset,"h2. Problem

There is currently no option to force delete a Google BigQuery dataset that is not empty along with its tables.
h2. Proposed improvement

Add _delete_contents_ as an extra argument to the _BigQueryDeleteDatasetOperator_ to specify whether to force the deletion (even if the dataset is not empty) or not. Update the _BigQueryBaseCursor_ too.",mbanayosy
AIRFLOW-5152,Fix autodetect default value,"[https://github.com/apache/airflow/pull/3880/commits/eb4cf61ae7583f5a306aa0cd7faa4d01aef61c33#diff-ee06f8fcbc476ea65446a30160c2a2b2]

The PR above introduces a new field `autodetect` and sets the default value to be false. As a result, `schema_fields`/`schema_objects` is always required if `autodetect` is not set. This breaks downstream services which use `GoogleCloudStorageToBigQueryOperator` without being aware of the added `autodetect` field.",bingqinz
AIRFLOW-5148,Add Google Analytics to the Airflow doc website,"Asked by [~aizhamal] 

Note from her:


{noformat}
I've looked at Google Analytics for the Airflow site, and I noticed that:
-The https://airflow.readthedocs.io/en/latest/ site has the GA code set up.
- The https://airflow.apache.org site does NOT have the GA code set up.
So the data that we're getting on GA is not complete. 
It would be really helpful to fix it soon, before we start revamping the website to understand the changes user behavior (I am signing a contract with a vendor next week)
{noformat}
",aizhamal
AIRFLOW-5147,Annotations for k8s executors should support extended alphabet (like '/')) ,"The fix to introduce k8s annotations for executors ([https://github.com/apache/airflow/pull/4589] for https://issues.apache.org/jira/browse/AIRFLOW-3766) limited the character set allowed for the annotation key to [-._a-zA-Z0-9] set. However many annotations contain `/` in it, for example: 
{code:java}
injector.tumblr.com/request{code}
 or
{code:java}
iam.amazonaws.com/role{code}
Which would not be allowed in the current solution.

 

I believe original solution should be completely revisited. And instead of using a separate *kubernetes_annotations* section there should be a key which will contain a set of key:value annotations in some format. E.g. json:
{code:java}
[kubernetes]
annotations = { ""iam.amazonaws.com/role"": ""arn:aws:iam:::role/some-role-CKU5HL9BIPXG"", ""some-other-anno-key"": ""some/value"" }
{code}
 

Supported character set for annotations:

https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/#syntax-and-character-set",dimberman
AIRFLOW-5140,missing type annotation errors reported by dmypy,"dmypy is reporting the following error:

 
{code:bash}
$ dmypy run -- --follow-imports=error airflow tests
tests/core.py:2188: error: Need type annotation for 'HDFSHook'
tests/core.py:2189: error: Need type annotation for 'snakebite'
airflow/__init__.py:42: error: Need type annotation for 'login'
airflow/api/auth/backend/default.py:22: error: Need type annotation for 'CLIENT_AUTH'
airflow/api/auth/backend/deny_all.py:23: error: Need type annotation for 'CLIENT_AUTH'
airflow/contrib/example_dags/example_gcp_speech.py:60: error: Need type annotation for 'SOURCE_LANGUAGE'
airflow/contrib/hooks/gcp_cloud_build_hook.py:50: error: Need type annotation for '_conn'
airflow/contrib/hooks/gcp_compute_hook.py:50: error: Need type annotation for '_conn'
airflow/contrib/hooks/gcp_function_hook.py:41: error: Need type annotation for '_conn'
airflow/contrib/hooks/gcp_sql_hook.py:722: error: Need type annotation for '_conn'
airflow/contrib/hooks/gcp_translate_hook.py:35: error: Need type annotation for '_client'
airflow/contrib/hooks/gcs_hook.py:41: error: Need type annotation for '_conn'
airflow/contrib/operators/awsbatch_operator.py:62: error: Need type annotation for 'client'
airflow/contrib/operators/awsbatch_operator.py:63: error: Need type annotation for 'arn'
airflow/contrib/operators/ecs_operator.py:61: error: Need type annotation for 'client'
airflow/contrib/operators/ecs_operator.py:62: error: Need type annotation for 'arn'
airflow/contrib/operators/gcp_function_operator.py:229: error: Need type annotation for 'upload_function'
airflow/executors/__init__.py:28: error: Need type annotation for 'DEFAULT_EXECUTOR'
airflow/hooks/dbapi_hook.py:41: error: Need type annotation for 'connector'
airflow/models/crypto.py:49: error: Need type annotation for '_fernet'
airflow/security/kerberos.py:25: error: Need type annotation for 'NEED_KRB181_WORKAROUND'
airflow/settings.py:66: error: Need type annotation for 'SQL_ALCHEMY_CONN'
airflow/settings.py:67: error: Need type annotation for 'DAGS_FOLDER'
airflow/settings.py:68: error: Need type annotation for 'PLUGINS_FOLDER'
airflow/settings.py:69: error: Need type annotation for 'LOGGING_CLASS_PATH'
airflow/settings.py:71: error: Need type annotation for 'engine'
airflow/settings.py:72: error: Need type annotation for 'Session'
airflow/settings.py:312: error: Need type annotation for 'CONTEXT_MANAGER_DAG'
airflow/utils/state.py:28: error: Need type annotation for 'NONE'
airflow/www/app.py:39: error: Need type annotation for 'appbuilder'
tests/contrib/hooks/test_databricks_hook.py:72: error: Need type annotation for 'RESULT_STATE'
tests/contrib/hooks/test_gcp_cloud_build_hook.py:49: error: Need type annotation for 'hook'
tests/contrib/hooks/test_gcp_cloud_build_hook.py:122: error: Need type annotation for 'hook'
tests/contrib/hooks/test_gcp_cloud_build_hook.py:191: error: Need type annotation for 'hook'
tests/contrib/utils/gcp_authenticator.py:57: error: Need type annotation for 'original_account'
tests/test_utils/reset_warning_registry.py:45: error: Need type annotation for '_pattern'
tests/test_utils/reset_warning_registry.py:48: error: Need type annotation for '_backup'
{code}",houqp
AIRFLOW-5139,Allow custom ES configs,"In [the ES task-handler|[https://github.com/apache/airflow/blob/master/airflow/utils/log/es_task_handler.py#L70]], the ""ElasticSearch"" object only takes in ""host"" as an argument. This means that users do not have the option to turn on ssl, assign certs, or perform any other of the necessary actions to match their ES environment.",dimberman
AIRFLOW-5137,Add templated fields tests for all GCP operators,We should make sure that all gcp operators have tests for templated fields.,higrys
AIRFLOW-5136,Fix Bug with Incorrect template_fields in DataProc{*} Operators,"
{code:python}
Traceback (most recent call last):
File ""/usr/local/virtualenv/airflow/local/lib/python2.7/site-packages/airflow/models/dagbag.py"", line 389, in collect_dags safe_mode=safe_mode)
File ""/usr/local/virtualenv/airflow/local/lib/python2.7/site-packages/airflow/models/dagbag.py"", line 253, in process_file self.bag_dag(dag, parent_dag=dag, root_dag=dag)
File ""/usr/local/virtualenv/airflow/local/lib/python2.7/site-packages/airflow/models/dagbag.py"", line 339, in bag_dag self.bag_dag(subdag, parent_dag=dag, root_dag=root_dag)
File ""/usr/local/virtualenv/airflow/local/lib/python2.7/site-packages/airflow/models/dagbag.py"", line 326, in bag_dag dag.resolve_template_files()
File ""/usr/local/virtualenv/airflow/local/lib/python2.7/site-packages/airflow/models/dag.py"", line 706, in resolve_template_files t.resolve_template_files()
File ""/usr/local/virtualenv/airflow/local/lib/python2.7/site-packages/airflow/models/baseoperator.py"", line 699, in resolve_template_files content = getattr(self, attr)
AttributeError: 'DataProcPySparkOperator' object has no attribute 'dataproc_pyspark_jars'
{code}
",kaxilnaik
AIRFLOW-5135,Use gapic ClientInfo in GoogleCloudBaseHook,"*from google.api_core.gapic_v1.client_info import ClientInfo*

this object inherits from

*from google.api_core.client_info import ClientInfo*

and implements one additional method used by Python SDKs.

 ",urbaszek
AIRFLOW-5134,Get task instance logs REST API endpoint,Create a REST API endpoint to get the logs of a specific task instance.,pyrooka
AIRFLOW-5129,Add typehint to gcp_dlp_hook.py,,ryan.yuan
AIRFLOW-5118,Airflow DataprocClusterCreateOperator does not currently support setting optional components,"there need to be an option to install optional components via DataprocClusterCreateOperator . components such as zeppelin.

From the source code of the DataprocClusterCreateOperator[1], the only software configs that can be set are the imageVersion and the properties. As the Zeppelin component needs to be set through softwareConfig optionalComponents[2], the DataprocClusterCreateOperator does not currently support setting optional components. 

 

As a workaround for the time being, you could create your clusters by directly using the gcloud command rather than the DataprocClusterCreateOperator . Using the Airflow BashOperator[4], you can execute gcloud commands that create your Dataproc cluster with the required optional components. 

[1] [https://github.com/apache/airflow/blob/master/airflow/contrib/operators/dataproc_operator.py] 
 [2] [https://cloud.google.com/dataproc/docs/reference/rest/v1/ClusterConfig#softwareconfig] 

[3] [https://airflow.apache.org/howto/operator/bash.html] ",igor@buran.us
AIRFLOW-5117,Airflow K8S not working with EKS when not deployed in cluster,"When Airflow scheduler or worker is not deployed in K8S cluster, all K8S API call will fail with 401 error after running for about 14min.

This is due to EKS enforces a 15min timeout for all API tokens but the official k8S python client doesn't support token refresh.",houqp
AIRFLOW-5116,Provide useful error messages from S3DeleteObjectsOperator,"When an error occurs deleting an object from S3, the API provides a response containing a list of errored objects, providing for each its key, an error code, and a descriptive error message. All of this would be very useful for debugging, except for some unknown reason, the `S3DeleteObjectsOperator` only provides the list of keys and strips out the error message. This makes debugging extremely difficult, and there is no reason not to include this information.",tv4fun
AIRFLOW-5115,S3KeySensor template_fields for bucket_name & bucket_key do not support Jinja variables,"In all Airflow operators (which inherit form {{BaseOperator}}) there is a {{template_fields}} attribute defined as [""which fields will get jinjafied""|https://github.com/apache/airflow/blob/master/airflow/models/baseoperator.py#L218-L219]). For the {{S3KeySensor}} op in specific, these are {{template_fields = ('bucket_key', 'bucket_name')}}.

The {{bucket_key}} kwarg, however, has some input validation in that the {{bucket_key}} needs to begin with the S3 protocol {{s3://}}; this exception is thrown by the [constructor|https://github.com/apache/airflow/blob/master/airflow/sensors/s3_key_sensor.py#L71-L74], which makes it impossible to use Jinja strings as an arg to {{bucket_key}}, since these don't get rendered in the scope of the DAG {{*.py}} file itself. Below is an example; I'm using Airflow 1.9.0 with Python 3.5.3:

Given the below DAG code, where ""my_s3_key"" is {{s3://bucket/prefix/object.txt:}}
{code:java}
dag = DAG('sample_dag', start_date=datetime(2019, 8, 1, 12, 15))

s3_variable_sensor = S3KeySensor(
    task_id='s3_variable_sensor',
    bucket_key=Variable.get('my_s3_key'),
    dag=dag
)

s3_jinja_sensor = S3KeySensor(
    task_id='s3_jinja_sensor',
    bucket_key=""{{ var.value.my_s3_key }}"",
    dag=dag
)
{code}
Executing the first task will run just fine while the next task will throw the following exception:
{code:java}
airflow.exceptions.AirflowException: Please provide a bucket_name.
{code}
This ticket is to propose a code change that will move input validation out of the constructor to allow for Jinja-templated strings to be passed into both {{bucket_name}} and {{bucket_key}}.",dsynkov
AIRFLOW-5114,TypeError when running S3ToGoogleCloudStorageTransferOperator or GoogleCloudStorageToGoogleCloudStorageTransferOperator with default arguments,"When running `S3ToGoogleCloudStorageTransferOperator` or `GoogleCloudStorageToGoogleCloudStorageTransferOperator` with default arguments, you get the following `TypeError`:

 
{noformat}
[2019-08-05 04:13:19,873] {models.py:1796} ERROR - '>' not supported between instances of 'NoneType' and 'int'
Traceback (most recent call last)
  File ""/usr/local/lib/airflow/airflow/models.py"", line 1664, in _run_raw_tas
    result = task_copy.execute(context=context
  File ""/home/airflow/gcs/dags/dependencies/gcp_transfer_operator.py"", line 675, in execut
    hook.wait_for_transfer_job(job, timeout=self.timeout
  File ""/home/airflow/gcs/dags/dependencies/gcp_api_base_hook.py"", line 188, in wrapper_decorato
    return func(self, *args, **kwargs
  File ""/home/airflow/gcs/dags/dependencies/gcp_transfer_hook.py"", line 390, in wait_for_transfer_jo
    while timeout > 0
TypeError: '>' not supported between instances of 'NoneType' and 'int{noformat}
This is because both operators default `timeout` to `None`, and `wait_for_transfer_job` assumes `timeout` is an integer. I have a fix I can submit.

 ",tv4fun
AIRFLOW-5113,Support icon url in slack web hook,"the slack client has a feature that it can set an icon for its user webhook, unfortunately, it not implemented on the Slack webhook.",blcksrx
AIRFLOW-5111,Remove apt-get upgrade from the Dockerfile,,fokko
AIRFLOW-5107,Fix template_fields in GCS ACL operator,,urbaszek
AIRFLOW-5104,Set default schedule for GCP Transfer operators," The docstring for `airflow.contrib.operators.gcp_transfer_operator.S3ToGoogleCloudStorageTransferOperator` says of the `schedule` parameter:
{noformat}
Optional transfer service schedule;
 If not set, run transfer job once as soon as the operator runs{noformat}
This isn't the way the operator is actually implemented though. If `schedule` is unset, the operator provides to `schedule` parameter in the REST API call, which causes the API to return this error:
{noformat}
HttpError 400 when requesting https://storagetransfer.googleapis.com/v1/transferJobs?alt=json returned ""Schedule was not specified.""{noformat}
It's not well-described in [the official documentation|https://cloud.google.com/storage-transfer/docs/reference/rest/v1/transferJobs], but it seems the REST API actually does require a `schedule` parameter to be provided, and fails if it isn't.

 ",tv4fun
AIRFLOW-5103,GCS Prefix Sensor should make matched files available to downstream tasks,"Matching the style of the [PubSubSensor|[https://airflow.readthedocs.io/en/stable/_modules/airflow/contrib/sensors/pubsub_sensor.html]], we should make the GoogleCloudStoragePrefixSensor return the list of matches so that downstream tasks don't have to duplicate the Sensor's logic and can instead grab matching GCS objets via XCom.",jakebiesinger
AIRFLOW-5102,Workers fail to shutdown jobs after failed heartbeats,"If a LocalTaskJob fails to heartbeat for scheduler_zombie_task_threshold, it should shut itself down: [https://github.com/apache/airflow/blob/f34e13a/airflow/jobs/local_task_job.py#L109]

 

However, at some point, a change was made to catch exceptions inside the heartbeat: [https://github.com/apache/airflow/blob/f34e13a/airflow/jobs/base_job.py#L194]

LocalTaskJob now thinks heartbeats always succeed.

 

This effectively means that zombie tasks don't shut themselves down. When the scheduler reschedules the job, this means we could have two instances of the task running concurrently.",hauntsaninja
AIRFLOW-5101,Inconsistent owner value in examples,"By default, the owner of a dag is 'Airflow' (Capital A). Nevertheless, a large part (but not all) of the examples/tests/perf has 'airflow' as the owner (lowercase A). I think it is better to use 'Airflow' in examples. ",j535d165
AIRFLOW-5099,Implement Google Cloud AutoML operators,Implement Google Cloud AutoML operators.,urbaszek
AIRFLOW-5096,reduce the number of times the pickle is inserted into the database by modifying the hash field of Dag,"After the scheduler has the --do_pickle option turned on, the scheduler will insert all the file pickles into the database each time it scans the file, which will cause the database to swell rapidly.

In my opinion, the main reason is because the hash function that determines whether the dag is the same as the pickle version uses the last_loaded field, which changes every time it is read instead of modified. Therefore, airflow inserts a large amount of unchanging data into the database.

I created a commit in which the last modified time of the file was used instead of last_loaded as the hash field, which works fine on my computer. Please let me know if you have a better way.

English is not my native language; please excuse typing errors.",meik
AIRFLOW-5095,airflow.cfg merger,A CLI that can reads an old airflow.cfg and merge with the latest version of airflow.cfg,milton0825
AIRFLOW-5094,Make airflow conn prefix configurable,Currently the Airflow picks up connection strings from environment variable if the environment variable starts with `AIRFLOW_CONN_`. This ticket propose to make the prefix configurable.,milton0825
AIRFLOW-5092,Latest python image should be pulled locally in force_pull_and_build,Latest python image should also be pulled locally before the build as the local builds after pull will start using previously pulled image.,higrys
AIRFLOW-5090,Dependencies to GCP libraries are inconsistent,Dependencies to GCP libraries are inconsistent in a few places. We need to review and update them and test.,higrys
AIRFLOW-5088,To implement DAG JSON serialization and DB persistence for webserver scalability improvement,"Created this issue for starting to implement DAG serialization using JSON and persistence in DB. Serialized DAG will be used in webserver for solving the webserver scalability issue.

 

The implementation is based on AIP-24: [https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-24+DAG+Persistence+in+DB+using+JSON+for+Airflow+Webserver+and+%28optional%29+Scheduler]

 

 ",coufon
AIRFLOW-5087,Display task/dag run stats on UI so that users can debug more easily,"# display the current running dag runs count / max dag runs limit (max_active_runs_per_dag)
 # display the current running tasks count / max running task limit (dag_concurrency)
 # hover a task to also display `depends_on_past` and `wait_for_downstream` params for not scheduled tasks",ping.goblue
AIRFLOW-5086,Partial search in classic UI not working,,feluelle
AIRFLOW-5080,npm: not found with ./airflow/www_rbac/compile_assets.sh,"h2. Problem
When I run [scripts/ci/kubernetes/docker/build.sh|https://github.com/apache/airflow/blob/1.10.3/scripts/ci/kubernetes/docker/build.sh#L45] to build Docker Image, I encountered the following error.

{code}
running compile_assets
./airflow/www_rbac/compile_assets.sh: 26: ./airflow/www_rbac/compile_assets.sh: npm: not found
{code}

So, http://YOUR_HOSTNAME/static didn't create. I got `500 Internal Server Error` with it on a browser(Chrome).

h2. Proposal
I think it needs to install npm in [scripts/ci/kubernetes/docker/compile.sh|https://github.com/apache/airflow/blob/1.10.3/scripts/ci/kubernetes/docker/compile.sh#L30].

{code}
apt-get install -y --no-install-recommends git nodejs npm
{code}

It works for me.
Please consider my proposal.",esfahan
AIRFLOW-5075,HttpHook cannot handle connections with an empty host field,"{{HttpHook}} cannot handle connections with an empty {{host}} field:

 
{code:java}
Traceback (most recent call last):
  File ""my_operator.py"", line 152, in <module>
    - datetime.timedelta(days=1)
  File ""my_operator.py"", line 50, in execute
    sess = self.sfdc_hook.get_conn()
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/airflow/hooks/http_hook.py"", line 63, in get_conn
    if ""://"" in conn.host:
TypeError: argument of type 'NoneType' is not iterable
{code}
 ",pgag
AIRFLOW-5072,gcs_hook causes out-of-memory error when downloading huge files,"Possibly there is an ""else"" missing here, but the gcs_hook's `download` method *always* downloads a blob as a string even when a filename was supplied. This causes the method to take twice as long when a filename is supplied and for huge blobs it can even cause out-of-memory errors.

I think that there is an else missing?

[https://github.com/apache/airflow/blob/05c01a97497e992c7d8b05a39a7855343dee1603/airflow/contrib/hooks/gcs_hook.py#L176]",james-woods
AIRFLOW-5062,Allow ACL header in S3 Hook,"Definition of Done
 * Add ACL field in S3Hook when uploading a file",lesterkim
AIRFLOW-5060,Add support of CatalogId to AwsGlueCatalogHook,"h2. Use Case

Imagine that you stream data into S3 bucket of an *account A* and update AWS Glue datacatalog on a daily basis, so that you can query new data with AWS Athena. Now let's assume that you provided access to this S3 bucket for an external *account B* who wants to use its' own AWS Athena to query your data in an exactly the same way. Unfortunately, an *account B* would need to have exactly the same table definitions in its AWS Glue Datacatalog, because AWS Athena cannot run against external glue datacatalog. However, AWS Glue service supports [cross-account datacatalog access|[https://docs.aws.amazon.com/glue/latest/dg/cross-account-access.html]], which means that *account B* can simply copy/sync meta information about database, tables, partitions etc from glue data catalog of an *account A*, provided additional permissions have been granted. Thus, all methods in *AwsGlueCatalogHook* should an use ""CatalogId"", i.e. ID of the Data Catalog from which to retrieve/create/delete.
h2.  
h2. How it fits into Airflow

Assume that you have an AWSAthenaOperator, which queries data once a day, then result is retrieved, visualised locally and then uploaded to some server/website. Then before this happens, you simply need to create an operator (even PythonOperator would do) which has two hooks, one to source catalog and another to destination catalog. At run time, it would use source hook retrieve information from *account A*, for example [get_partitions()|[https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/glue.html#Glue.Client.get_partitions], then parse response and remove unnseccary keys and finally use destination hook to update *account B* datacatalog with [batch_create_partitions()|[https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/glue.html#Glue.Client.batch_create_partition]]

 
h2. Proposal
 * Add a parameter *catalog_id* to AwsGlueCatalogHook, which then will be used in all its methods, regardless of this hook associated with source or destination datacatalog. 
 * In order not to break exsisting implementation, we set *catalog_id=None.* But we add method *fallback_catalog_id(),* which uses AWS STS to infer Catalog ID associated with used *aws_conn_id.* Obtained value * *would be used if *catalog_id* hasn't been provided during hook creation.
 * Extend available methods of *AwsGlueCatalogHook* in a similar way to already exsisting once, for convenience of the workflow described above. Note: all new methods should strictly adhere AWS Glue Client Request Syntax and do it in transparent manner. This means, that input information shouldn't be modified within a method. When such actions are required, they should be performed outside of the AwsGlueCatalogHook.

h2. Implementation
 * I am happy to contribute to airflow if this feature request gets approved.

h2. Other considerations
 * At the moment an existing method *get_partitions* doesn't not provide you with all metainformation about partitions available from glue client, whereas *get_table* does. Don't know the best way around it, but imho it should be refactored to *get_partitions_values* or something like that. In this way, we would be able to stay inline with boto3 glue client.

 ",ilya kisil
AIRFLOW-5059,Use .mypy_cache in CI builds to speed-up static checks,,higrys
AIRFLOW-5058,Add support for dmypy (Mypy daemon) to Breeze environment,Per discussion in [https://github.com/apache/airflow/pull/5664] we might use dmypy for local development speedups.,higrys
AIRFLOW-5057,Provide bucket name to functions in S3 Hook when none is specified,,feluelle
AIRFLOW-5056,Add mail_filter argument to ImapHook when searching for attachments,"By making use of the _criterion_ argument in the [search|https://docs.python.org/3.6/library/imaplib.html#imaplib.IMAP4.search] function we can filter only specific mails we want to search in for attachments.

*+ adding support to the ImapAttachmentSensor and ImapAttachmentToS3Operator*",feluelle
AIRFLOW-5055,kubernetes_environment_variables section cannot be set using `AIRFLOW_` prefixed environment variables,"When you set environment variables like `AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES____AIRFLOW__HOME`, the scheduler will pass `airflow_home` as environment variable key to workers instead, which gets ignored by the workers.",houqp
AIRFLOW-5052,Add the include_deleted parameter to Salesforce Hook,"At present, the salesforce hook uses the make_query() method to perform query via Salesforce api. The make_query() accepts a query and and uses a wrapper function which calls the query_all() method from simple salesforce to perform the query.

The deficiency with the use of the query_all() method is that it does not factor the inclusion of deleted records which is an optional parameter in the query_all() method of simple salesforce as shown below:

```

#query_all method in the salesforce hook

query_all(query)

#This should be changed to reflect what the simple salesforce query_all() does in the. background as follows

{color:#ffc66d}query_all{color}({color:#94558d}self{color}{color:#cc7832}, {color}query{color:#cc7832}, {color}include_deleted={color:#cc7832}False, {color}**kwargs):

```

The objective of this ticket is add these parameters to the salesforce hook so that the hook also has the option of including the deleted records from Salesforce in the query.",wmorris75
AIRFLOW-5051,Coverage is not properly reported in the new CI system,,higrys
AIRFLOW-5049,Wrong src_fmt_configs in bigquery hook are silently ignored instead of an error being thrown,For the bigquery hook if a wrong config parameter is given for src_fmt_configs it is silently ignored. Instead it would be better if an error is thrown.,dpgrev
AIRFLOW-5048,Improve display of Kubernetes resources,At the moment just a serialized class is displayed in task instance view,ms32035
AIRFLOW-5045,Add ability to create Google Dataproc cluster with custom image from a different project,"Custom image support has been added for Dataproc by https://issues.apache.org/jira/browse/AIRFLOW-2797.

Unfortunately the code assumes that the images come from the same project.

It would be useful to add a new parameter 'custom_image_project_id' to specify the source project of the image.",igor@buran.us
AIRFLOW-5044,Fix copy/pasta in k8s request factory extract resources,,a_soldatenko
AIRFLOW-5041,CI scripts force you to python 3.6 on the host,,higrys
AIRFLOW-5040,Fix typos,Opening this to submit a PR as Work in Progress to fix several typos.,serkef
AIRFLOW-5038,Pod deleted message is shown incorrectly when pod deletion is disabled,"When `delete_worker_pods` is set to False, scheduler still logs this line: `self.log.info('Deleted pod: %s', str(key))`, which is incorrect and confusing.",houqp
AIRFLOW-5035,Dag parsing process can leave orphan processes under heavy load,"As reported by James Meickle on the mailing list, under certain cases it is possible that the multiprocessing.Manager process can end up orphaned when the scheduler shuts down.

This is relating to a new merge in 1.10.4 (i.e. 1.10.3 wasn't affected)

The ""orphan"" process is massively exasperated but having a {{--run-duration 600}}, but we should try and fix this if we can.",ashb
AIRFLOW-5034,CLI errors should exit with non-zero codes,"Some CLI errors raise an exception and return a non-zero exit code, but others (like deleting a user that doesn't exist) print an error message and return a zero exit code. To improve consistency, we should update the CLI so that all errors raise exceptions and return non-zero exit codes.",jmcarp
AIRFLOW-5032,build is broken due to version conflict between tzlocal and pendulum,"Lastest master build is broken due to dependency version conflict, here is the error: 
{code:java}
Traceback (most recent call last):
File ""/usr/local/bin/airflow"", line 4, in <module>
__import__('pkg_resources').require('apache-airflow==2.0.0.dev0')
File ""/usr/local/lib/python3.7/site-packages/pkg_resources/__init__.py"", line 3241, in <module>
@_call_aside
File ""/usr/local/lib/python3.7/site-packages/pkg_resources/__init__.py"", line 3225, in _call_aside
f(*args, **kwargs)
File ""/usr/local/lib/python3.7/site-packages/pkg_resources/__init__.py"", line 3254, in _initialize_master_working_set
working_set = WorkingSet._build_master()
File ""/usr/local/lib/python3.7/site-packages/pkg_resources/__init__.py"", line 585, in _build_master
return cls._build_from_requirements(__requires__)
File ""/usr/local/lib/python3.7/site-packages/pkg_resources/__init__.py"", line 598, in _build_from_requirements
dists = ws.resolve(reqs, Environment())
File ""/usr/local/lib/python3.7/site-packages/pkg_resources/__init__.py"", line 791, in resolve
raise VersionConflict(dist, req).with_context(dependent_req)
pkg_resources.ContextualVersionConflict: (tzlocal 2.0.0 (/usr/local/lib/python3.7/site-packages), Requirement.parse('tzlocal<2.0.0.0,>=1.5.0.0'), {'pendulum'})
{code}
 ",houqp
AIRFLOW-5031,Tzlocal 2.0.0 has been released today and it breaks the build,,7yl4r
AIRFLOW-5030,config key name with __ in it is getting skipped in AirflowConfigParser.as_dict method,"When a key name contains one or more ""__"", it will get ignored by AirflowConfigParser.as_dict method. This makes it impossible to set worker environment variables using `KUBERNETES_ENVIRONMENT_VARIABLES` section from scheduler.",houqp
AIRFLOW-5028,Add ephemeral-storage to airflow pod resources,"Right now we are not setting {{ephemeral-storage}} requests/limits on any of the Airflow pods. 

[https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/#local-ephemeral-storage]",a_soldatenko
AIRFLOW-5027,Generalized CloudWatch log grabbing for ECS and SageMaker operators,"It's very useful to have the CloudWatch task logs of an ECS/SageMaker job available within Airflow. Currently, this is not possible for ECS tasks.

The SageMaker operator starts a job on AWS SageMaker. It already supports grabbing the CloudWatch logs of the (finished) job to the Airflow instance.

The ECS operator is very similar in that it starts a job on AWS ECS. It doesn't, however, support grabbing CloudWatch logs at the end of a job.

I've separated the existing log grabbing of the SageMaker operator to a hook and both the SageMaker and ECS operators now call that hook at the end of a job.",stevenreitsma
AIRFLOW-5021,gitpyhon should not be included as runtime dependency,gitpython is included as `install_requires` but is only used in setup.py. It's better to move it to `setup_requires` so we can reduce package size.,houqp
AIRFLOW-5015,Make AWS Operators Pylint compatible,Make AWS Operators Pylint compatible.,gto
AIRFLOW-5013,Add GCP Data Catalog Hook and Operators,"Add GCP Data Catalog services to Airflow

[https://cloud.google.com/data-catalog/docs/reference]",ryan.yuan
AIRFLOW-5012,Add typehints for gcp_*_hook.py,,kamil.bregula
AIRFLOW-5007,Python version is overridden in CI tests to 3.6,Python version should be forced to 3.6 only in static checks but it was accidentally forced to 3.6 in running the tests as well. It should be fixed quickly,higrys
AIRFLOW-5004,Stable/test image should be selectable via TRAVIS_BRANCH environment variable,"When building CI images you need to know whether you use v1-10-test or v1-10-stable branch (and use corresponding image).

Currently the default branch is read from _default_branch.sh . This is not a problem for master/v1-10-test distinction because those are in different ""physical"" branches, but v1-10-stable and v1-10-test are actually following one-another (v1-10-test moves first and when several commits pass, then stable branch follows).

Therefore we need to utilise TRAVIS_BRANCH variable - this variable works in two modes - for PRs it is the branch that the PR is going to be merged to, whereas for triggered builds it's the branch from which the build was triggered. This is perfect for us to determine which image should be used during build.",higrys
AIRFLOW-5003,Make AWS hooks Pylint compatible,Make AWS hooks Pylint compatible.,gto
AIRFLOW-5000,Remove task instance duplicate end_date and reorder template context,,zhongjiajie
AIRFLOW-4994,Bare version of integration test environment,We need a slim version of environment that should only start airflow's docker image and let it use sqlite rather than start all external dependencies,higrys
AIRFLOW-4992,Replace backports.configparser in favour of Python 3 native configparser,,basph
AIRFLOW-4984,Improve SalesforceHook to Query Deleted Records,"The current salesforce_hook does not allow you to query deleted records. This feature is already enabled in simple-salesforce, which the salesforce_hook implements.

 

From [https://simple-salesforce.readthedocs.io/en/latest/simple_salesforce.api.html]:

{{query_all}}(_query_, _include_deleted=False_, _**kwargs_)

Returns the full set of results for the ??query??. This is a convenience wrapper around ??query(...)?? and ??query_more(...)??.

The returned dict is the decoded JSON payload from the final call to Salesforce, but with the ??totalSize?? field representing the full number of results retrieved and the ??records?? list representing the full list of records retrieved.

Arguments
 * query - the SOQL query to send to Salesforce, e.g.SELECT Id FROM Lead WHERE Email = ""[waldo@somewhere.com|mailto:waldo%40somewhere.com]""
 * include_deleted - True if the query should include deleted records.

 

 

Going to include the keyword argument *include_deleted=False* in the salesforce_hook *make_query* method.",dan-ladd
AIRFLOW-4983,DataflowPythonOperator should be able to submit pipelines with python3,"Currently the DataflowHook hard codes python2 interpreter.

Apache Beam is beginning to support python3 interpreter and we should support submitting those pipelines.

I've we should add a `py_interpreter` arg to the operator and hook that defaults to 'python2' (to not be interface breaking.",data-runner0
AIRFLOW-4981,execution_date in macro is not a pendulum DateTime for fixed cron schedules,"When using a fixed cron schedule_interval such as ""0 12 * * *"", the execution_date (as well as next_execution_date) available in macros is a python datetime object, not a pendulum DateTime as indicated in the macro docs. For my purposes, this means that methods such as in_timezone are not available.",arlo
AIRFLOW-4965,Handling throttling in GCP AI operators,"Polidea develops Apache Airflow operators for following Google Cloud AI services:
 * Cloud Translate
 * Cloud Vision
 * Cloud Text-To-Speech
 * Cloud Speech-To-Text
 * Cloud Translate Speech
 * Cloud Natural Language
 * Cloud Video Intelligence

Those API implement quota verification and throttle requests that exceed the quota. Here are the relevant links describing it:

[https://cloud.google.com/translate/quotas] 

[https://cloud.google.com/vision/quotas]

[https://cloud.google.com/speech-to-text/quotas]

[https://cloud.google.com/text-to-speech/quotas]

[https://cloud.google.com/natural-language/quotas]

[https://cloud.google.com/video-intelligence/quotas]

There are several types of quotas and limits:

 

*Translate:*
 * characters per day [403 - error  “Daily Limit Exceeded”]
 * characters per 100 seconds (per project or per project/user) [403 error “User Rate Limit Exceeded”] [TEMPORARY]

 

*Vision:*
 * image file size
 * requests per minute [TEMPORARY]
 * images per feature per month 

 

*Text to speech:*
 * total characters per request
 * requests per minute [TEMPORARY]
 * characters per minute [TEMPORARY]

 

*Speech to text*
 * limits of the content size
 * limits of the phrases/characters per request for context
 * requests per 60 seconds [TEMPORARY]
 * processing per day

*Natural Language*
 * Text Content size 

 * Token quota and Entity mentions (ignored?)
 * requests per 100 seconds [TEMPORARY]
 * requests per day 

*Video Intelligence*
 * video size
 * requests per minute [TEMPORARY]
 * backend time in seconds per minute [TEMPORARY]

 

In all Cloud AI operators we are using Python Client API. Most methods are using built-in [Retry|https://googleapis.github.io/google-cloud-python/latest/core/retry.html#google.api_core.retry.Retry] object and Retry mechanism. The assumption is that for functions that use the mechanism, it is implemented correctly and by default “retriable” errors only are retried. User can configure behaviour of the Retry object - exponential back-off factor, delays, etc. In the current API Retry object can be provided by the user creating the DAG and using the operator:
  

The APIS that use Retry object are:
 * *Cloud Vision Product Search*

 * *Cloud Vision Extra*

 * *Cloud Vision Detect*

 * *Cloud Natural Language*

 * *Cloud Speech*

 * *Cloud Video Intelligence*

**

The Retry mechanism provided by the Client API should be enough to handle temporary bursts of requests. User can control exponential back-off rate and will be able to adjust it to their own needs. They are also able to manually restart failed jobs using standard Airflow mechanisms in case their configuration is not well adjusted to their limits.

 

The only case where Retry is not used in the API is Translate operator - specifically [translate|https://googleapis.github.io/google-cloud-python/latest/translate/client.html#google.cloud.translate_v2.client.Client.translate] API. 

 

In case of Translate API, the proposal is to use Retry decorator in our own hook and perform retries only in case of *“User Rate Limit Exceeded”* error, all other errors (size limit and Daily Limit Exceeded) should be treated as non-retriable. In those cases users will be able to manually restart failed jobs. 

 
h1. Implementation

We analyzed two solutions:
 # extension of the built-in mechanism from google-cloud-python library - Retry
 # external library - tenacity

 

The use of the first solution seems natural, but it is problematic. Each method creates a retry object by default from a configuration based on a private file with configuration.
 Reference: 
 [https://github.com/googleapis/google-cloud-python/blob/b718d2d9bb32b0e7934ae90d57dc80c81ce0fb73/vision/google/cloud/vision_v1/gapic/image_annotator_client.py#L162-L168
 ] [https://github.com/googleapis/google-cloud-python/blob/b718d2d9bb32b0e7934ae90d57dc80c81ce0fb73/vision/google/cloud/vision_v1/gapic/image_annotator_client.py#L296-L304
 ]If we would like to extend this mechanism, we would have to copy the logic to this configuration. The google-cloud-python library does not allow us to easily change only part of the configuration of retry object.

 

The retry mechanism is not supported by all services (See: [Current approach|https://docs.google.com/document/d/1QYwTy6r7bbLK3cmE9x1VShelHQ4B9SLHZDVjp6ja1S4/edit#heading=h.y76gaxcevnym]), so there is a need to create a separate mechanism. The new mechanism based on the external library will work with all services. This will provide a more predictable developer experience.

 

The tenacity library provides a code retry mechanism based on the decorator. It use wait strategy that applies exponential backoff. All hook methods that are covered by quota restrictions will get a new decorator.

 

Sample implementation:

{{@tenacity.retry(}}
 {{    wait=tenacity.wait_exponential(min=1, max=100),}}
 {{    retry=retry_if_temporary_quota(),}}
 {{)}}
 {{def fetch():}}
 {{    response = client.translate(TEXT, target_language=""PL"")['translatedText']}}
 {{    return response|}}

   

 

_retry_if_temporary_quota_ is a factory method that creates a predicate to check if the exception concerns the quota restriction.",dzakus13
AIRFLOW-4964,Add BigQuery Data Transfer Hook and Operator,"Add BigQuery Data Transfer Hook and Operator to allow users to transfer data from partner SaaS applications to Google BigQuery on a scheduled, managed basis.",kamil.bregula
AIRFLOW-4962,Fix Werkzeug v0.15 deprecation notice for DispatcherMiddleware import,"In 1.10.4rc3 tag, after werkzeug upgrade
{code}
/usr/local/lib/python3.6/site-packages/airflow/www_rbac/app.py:231: DeprecationWarning: 'werkzeug.wsgi.DispatcherMiddleware' has moved to 'werkzeug.middleware.dispatcher.DispatcherMiddleware'. This import is deprecated as of version 0.15 and will be removed in version 1.0.
{code}",nritholtz
AIRFLOW-4959,Add hql template support for DataProcHiveOperator,,fokko
AIRFLOW-4956,LocalTaskJob heartbeat can create spamming log,"When DB is unavailable and heartbeat() fails, LocalTaskJob will keep trying to heartbeat--causing DB load and spamming log.",yrqls21
AIRFLOW-4951,Use new style classes,,dzakus13
AIRFLOW-4949,Use OSError exception,,dzakus13
AIRFLOW-4948,"Use items, values method instead six package",,dzakus13
AIRFLOW-4947,Remove six types,,dzakus13
AIRFLOW-4946,Use yield from syntax,,dzakus13
AIRFLOW-4945,Use super() syntax,,dzakus13
AIRFLOW-4944,Use new types syntax,,dzakus13
AIRFLOW-4943,Replace six assertion method with native,,dzakus13
AIRFLOW-4942,Drop six.next,,dzakus13
AIRFLOW-4941,default_args not applied when dag is assigned to task through setter,"When the DAG is set to the task via [setter|https://github.com/apache/airflow/blob/526c65a57204022596fb69e9478c5515ad0b880e/airflow/models/baseoperator.py#L501], `default_args` won't be applied to the task. Adding a warning message to let user know about that.",ashb
AIRFLOW-4940,DynamoDB to S3 backup operator,Add an Airflow operator that back up DynamoDB table to S3.,milton0825
AIRFLOW-4939,Ability to specify default retries for all tasks,There should be a config which sets the default retry count for all the tasks. ,msumit
AIRFLOW-4936,Tests should clean up configuration changes,Several unit tests modify settings in `configuration.conf` but don't clean up those changes. This can cause tests to change the results of other tests that run later. All tests should clean up changes they make to airflow configuration.,jmcarp
AIRFLOW-4935,Add method in the bigquery hook to list tables in a dataset,A method to list list all tables in datasets. A prefix could be specified.,benjamin.grenier
AIRFLOW-4934,ProxyFix not working as expected since Werkzeug v0.15,"Since Werkzeug 0.15, _enable_proxy_fix_ doesn't work as expected as per [this Github issue|https://github.com/pallets/werkzeug/issues/1484].

We should fix it to at least respect what was previous behavior. We can eventually also offer a configuration to customize the new behavior, however this is breaking old behavior, and is required within the next/current release due to the Flask pin.",nritholtz
AIRFLOW-4932,key_file of hook is overwritten by SSHHook connection,"Currently in SSHHook key_file parameter which provided to hook is being overwritten by key_file defined as extra parameter in the connection.

This should be the other way around. The hook should overwrite the connection.",eladk
AIRFLOW-4931,Add KMS Encryption Configuration to BigQuery Hook and Operators,"One of the clients requires adding KMS encryption on BigQuery tables. 

Reference:

[https://cloud.google.com/bigquery/docs/reference/rest/v2/tables#ExternalDataConfiguration]",ryan.yuan
AIRFLOW-4930,"Druid hook sends json ingestion spec string using json= parameter of requests, not body=","Hi all, I recently tried to add druid ingestion task to my airflow DAG,

and found that druid hook for airflow is sending json string as `json=`, which makes druid coordinator impossible to parse the request body. [https://github.com/apache/airflow/blob/master/airflow/hooks/druid_hook.py#L71]

I changed it to send `json_index_spec` using `data=`, and my ingestion job worked as expected. I'm not sure whether this is a bug or environment specific issue. ",zxzl
AIRFLOW-4928,Remove redundant config parsing in DagBag ,"In DagBag, configuration file is parsed multiple times while this can be avoided. We can save the values we read in the object namespace and refer it within the instance.",serkef
AIRFLOW-4926,Fix example dags where its start_date is datetime.utcnow(),"Example dags with datetime.utcnow() start_date do not run tasks.

The solution is to change the start_date to a date where a period (execution_date + schedule_interval) can end.",feluelle
AIRFLOW-4925,Improve css style for Variables Input File Field in RBAC UI,,feluelle
AIRFLOW-4924,Loading DAGs asynchronously in Airflow webserver,"h2. Scalability Issue in Webserver

Airflow webserver uses gunicorn workers to serve HTTP requests. It loads all DAGs from DAG files before serving requests. If there are many DAGs (e.g., > 1,000), loading all DAGs can take a significant amount of time.

Airflow webserver also relies on restarting gunicorn workers to refresh all DAGs. This refreshing interval is set by webserver-worker_refresh_interval, default to 30s. As a result, if loading all DAGs takes >30s, the webserver will never be ready for HTTP requests.

The current solution is to skip loading DAGs by using env var SKIP_DAGS_PARSING. It makes the webserver work, but there is no DAG on the UI.
h2. Asynchronously DAG Loading

The solution here is to load DAGs asynchronously in the background. It creates a background process to load DAGs, stringifies DAGs, and sends DAGs to gunicorn worker process. The stringifying step is needed because some fields can not be pickled, e.g., locally defined functions and user defined modules. It aggressively transform all fields of DAG and task to be string-compatible.

This feature is enabled by webserver-async_dagbag_loader=True. The background process sends DAGs to gunicorn worker gradually (every webserver-dagbag_sync_interval). DAG refreshing interval is controlled by webserver-collect_dags_interval.

Asynchronous DAG loading has been released in Google Cloud Composer as an Alpha feature:
 [https://cloud.google.com/composer/docs/release-notes]
 [https://cloud.google.com/composer/docs/how-to/accessing/airflow-web-interface]

This issue is created to merge the feature to Airflow upstream.

 

 ",coufon
AIRFLOW-4923,Databricks hook leaks API secret in logs,"The databricks operator logs API keys during task instance run. The databricks operator implementation encourages users to put their API key in the connection ""extra"" field ([link to docstring|https://github.com/apache/airflow/blob/1.10.3/airflow/contrib/operators/databricks_operator.py#L201-L204]), and its accompanying databricks hook invokes BaseHook.get_connection(), which logs that ""extra"" field in plaintext via the [models.Connection.debug_info method|https://github.com/apache/airflow/blob/1.10.3/airflow/models/connection.py#L271-L280].

Links:
 * [BaseHook.get_connection|https://github.com/apache/airflow/blob/1.10.3/airflow/hooks/base_hook.py#L69-L84]
 * [DatabricksHook constructor invoking get_connection|https://github.com/apache/airflow/blob/1.10.3/airflow/contrib/hooks/databricks_hook.py#L65]
 * [BaseHook.debug_info|https://github.com/apache/airflow/blob/1.10.3/airflow/models/connection.py#L271-L280]

One potential fix would be to allow the operator to emit bearer token headers if the token is saved to the password field and/or a flag is set in ""extra"".

As an immediate workaround, users can just specify the username and API token as the password and the API appears to work.",neilp90
AIRFLOW-4920,cgi.escape used in QueryView was removed in Python 3.8,"QueryView uses AceEditorWidget form which uses cgi.escape. cgi.escape was deprecated since Python 3.3 and removed in 3.8. It's recommended to use html.escape. cgi.escape can still be available for Python 2 compatibility.

Sample warning in Python 3.7 :
{code:java}
/home/karthi/airflow/airflow/www/utils.py:442: DeprecationWarning: cgi.escape is deprecated, use html.escape instead
  contents=escape(text_type(field._value())),
{code}
CPython issue : [https://bugs.python.org/issue33843]

I will create a PR for this issue.",xtreak
AIRFLOW-4919,Add 'dataproc_properties' parameter to templated fields of DataProcJobBaseOperator child classes,"Properties in Dataproc jobs have a need for templating in cases of default values and other labeling properties, such as macro dates. 

dataproc_pig_properties in DataProcPigOperator

dataproc_hive_properties in DataProcHiveOperator

dataproc_spark_properties in DataProcSparkSqlOperator

dataproc_hadoop_properties in DataProcHadoopOperator

Additionally, should add generic dataproc_properties to each as templated_field. 

Update py docs to include that it is templated.",eladk
AIRFLOW-4916,delete dag from experimental API,"Currently, I have to use web interface if I want ot delete a DAG or use a hack like this :
{code:java}
curl -X POST airflow:8080/admin/airflow/delete?dag_id=<dag_id>
{code}
 

Expected :

I want to be able to delete a dag like this :
{code:java}
curl -X DELETE airflow:8080/api/experimental/dags/<DAG_ID>{code}",fcussac
AIRFLOW-4915,Support backfill through Airflow UI,Support a way to backfill DAGs through Airflow UI.,milton0825
AIRFLOW-4914,Submit backfill request through REST endpoint,Support submitting backfill request through REST endpoint.,milton0825
AIRFLOW-4913,Backfill through scheduler,"Currently Airflow backfill has its own scheduling logic other than the core scheduler. Since the backfill process might not be long running, often times we may find out later that the backfill process failed and exited in the middle. However, core scheduler is guaranteed (?) to be long running and would be great if there is a way to backfill through core scheduler.

Later we can support user submit backfill request through REST endpoint and then through Airflow UI.",milton0825
AIRFLOW-4911,Silence the FORBIDDEN errors from the KubernetesExecutor,"Right now the {{KubernetesExecutor}} throws an error when it hits a quota limit. It is gracefully handled, but we should also stop showing these errors since they are show front-and-center in the log view now. This will probably confuse folks and lead to an awkward user experience.",a_soldatenko
AIRFLOW-4910,KuberenetesExecutor - KubernetesJobWatcher can silently fail,"After not monitoring Airflow for a while, I noticed that tasks had not been running for several days.

My setup: Scheduler and web-server running in one pod, with KubernetesExecutor. 4 different DAGs, none of them very large: 1 running once per day, 2 every 30 mins and 1 every 2 minutes.

Airflow had log messages such as these:
{code:java}
{{jobs.py:1144}} INFO - Figuring out tasks to run in Pool(name=None) with 128 open slots and 179 task instances in queue{code}
{code:java}
{{jobs.py:1210}} DEBUG - Not handling task ('example_python_operator', 'print_the_context', datetime.datetime(2019, 6, 7, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) as the executor reports it is running{code}
... and a bit further down:
{code:java}
{{base_executor.py:124}} DEBUG - 32 running task instances{code}
In the Kubernetes cluster, there were no pods created by Airflow (they'd all finished and been deleted).

After digging into the logs around the time at which jobs stopped progressing, I noticed that at this point in time the KubernetesJobWatcher stopped logging the state changes of pods - even though I could see log messages for new pods being created.

It's hard to tell why this happened - if the subprocess running the job watcher died it should have been detected in the [heartbeat|https://github.com/apache/airflow/blob/1.10.3/airflow/contrib/executors/kubernetes_executor.py#L442]. If the [Watch threw an exception|https://github.com/apache/airflow/blob/1.10.3/airflow/contrib/executors/kubernetes_executor.py#L295], there should have been logs (which there weren't) and then it should have restarted.

I have a few theories as to what might have happened:
 # The Watch hung indefinitely - although I can't see any issues against the Kubernetes python client that suggest other people have had this issue
 # The KubernetesJobWatcher died, but the heartbeat was not functioning correctly
 # The Watcher experienced a large gap between watch requests meaning some relevant events were ""lost"" leaving the respective tasks in the ""running"" state

Unfortunately I dont have the answers, so I'm posting this in the hope someone has some additional insight.

As a side note - Im using Kubernetes Client version 9.0.0

My only suggestion for a fix is to periodically check what Pods are actually running, and reconcile that against the ""running"" queue in the executor and maybe force-restart the job watcher if the state has diverged).",dimberman
AIRFLOW-4908,"Implement BigQuery Hooks/Operators for update_dataset, patch_dataset and get_dataset","To create a BigQuery sink for GCP Stackdriver Logging, I have to assign `WRITER` access to group `cloud-logs@google.com` to access BQ dataset. However, current BigQueryHook doesn't support updating/patching dataset.

Reference: [https://googleapis.github.io/google-cloud-python/latest/logging/usage.html#export-to-bigquery]

Implement GCP Stackdriver Logging: https://issues.apache.org/jira/browse/AIRFLOW-4779

While it is missing update_dataset and patch_dataset, BigQueryHook has get_dataset but it doesn't have operator for it.

 

Features to be implemented:
BigQueryBaseCursor.patch_dataset
BigQueryBaseCursor.update_dataset
BigQueryPatchDatasetOperator
BigQueryUpdateDatasetOperator
BigQueryGetDatasetOperator",ryan.yuan
AIRFLOW-4907,AWSAthenaHook - using None connection in a method,"In the AWSAthenaHook [https://github.com/apache/airflow/blob/master/airflow/contrib/hooks/aws_athena_hook.py], the get_conn() method initializes `conn` if it's None. But the `get_state_change_reason()` method uses `conn` which is initialized to None - [https://github.com/apache/airflow/blob/master/airflow/contrib/hooks/aws_athena_hook.py#L98]

 

 

Solution:

Replace the call to `self.conn` with `self.get_conn()` in order to ensure initialization. ",bhavicka
AIRFLOW-4906,Improve debugging for the SparkSubmitHook,"Currently, the output of the spark-submit command is not being sent to the logs. This makes debugging of the k8s jobs rather hard. For example, if you make a typo, you only will get the exit code which is non-descriptive.",fokko
AIRFLOW-4905,Add colours to flake8 output,"Example screenshot of the coloured flake output attached.

Note that it should only be merged after we merge the new CI image and have a standard way of running flake8 using the slim CI docker image.",higrys
AIRFLOW-4904,Retrieve test config from $AIRFLOW_TEST_CONFIG,"There's an option to specify a specific location for the main config file via setting `$AIRFLOW_CONFIG`, but `unittests.cfg` is always appended (in an OS-unsafe way) to `$AIRFLOW_HOME` ([https://github.com/apache/airflow/blob/master/airflow/configuration.py#L505).]

 

The `$AIRFLOW_TEST_CONFIG` var should allow for dynamic placement similar to how `$AIRFLOW_CONFIG` does.",mattrasto
AIRFLOW-4899,Fix get_dataset_list from bigquery hook to return next pages,"Linked with AIRFLOW-3055

The next method must return ""full list of BigQuery datasets in the current project"" such as is documented, but return only the 50 first datasets.
{code:java}
def get_datasets_list(self, project_id=None){code}",benjamin.grenier
AIRFLOW-4896,KubernetesExecutorConfig labels default argument is mutable,KubernetesExecutorConfig's {{labels}} default argument is mutable (empty dict).,pgag
AIRFLOW-4895,DeprecationWarning in Python 3.7 while running tests,"There is DeprecationWarning raised when running tests locally. This was introduced as part of AIRFLOW-3958 and commit 75bec88f04705884de58facaac97338a5b5b439a. This is similar to AIRFLOW-3009. Since the fix is simple enough I would like to work on this as my first contribution.

DeprecationWarning in test

{code:bash}
/home/karthi/airflow/airflow/utils/helpers.py:26: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working
  from collections import Iterable
{code}

Thanks",xtreak
AIRFLOW-4894,Add hook and operator for GCP Data Loss Prevention API,"Add a hook and operator to manipulate and use Google Cloud Data Loss Prevention(DLP) API. DLP API allow users to inspect or redact sensitive data in text contents or GCP storage locations.

The hook includes the following APIs, implemented with Google service discovery API:
 * inspect/deidentify/reidentify for content: [https://cloud.google.com/dlp/docs/reference/rest/v2/projects.content]
 * create/delete/get/list/patch for inspectTemplates: [https://cloud.google.com/dlp/docs/reference/rest/v2/organizations.inspectTemplates], [https://cloud.google.com/dlp/docs/reference/rest/v2/projects.inspectTemplates]
 * create/delete/get/list/patch for storedInfoTypes: [https://cloud.google.com/dlp/docs/reference/rest/v2/organizations.storedInfoTypes], [https://cloud.google.com/dlp/docs/reference/rest/v2/projects.storedInfoTypes]
 * create/list/get/delete/cancel for dlpJobs: [https://cloud.google.com/dlp/docs/reference/rest/v2/projects.dlpJobs]

The operator creates a long-running dlp job (for storage inspection or risk analysis), keeps polling its status and waits for it to be done or canceled/deleted.

Apart from unit tests, also tested locally in DAG level(not included in PR).",zhuzhengliang
AIRFLOW-4892,Creation of Connections via UI fails in 1.10.4rc2,"Via classic UI: When trying to create/save a new connection. The form only reloads without saving the connection.

With RBAC: Error when rendering the connections form:

{noformat}
jinja2.exceptions.UndefinedError: 'airflow.www_rbac.forms.ConnectionForm object' has no attribute 'extra__grpc__credentials_pem_file'
{noformat}",ashb
AIRFLOW-4890,Fix Log link in TaskInstance's View for Non-RBAC,,feluelle
AIRFLOW-4887,Add docker image build operator,I believe that operators who build Docker Image can create more flexible workflows.,trsnium
AIRFLOW-4886,Add routines methods to BigQuery Hooks and Operators,"Add routines methods to BigQueryHook

Add related operators",ryan.yuan
AIRFLOW-4884,Roll up import_errors in UI,"When a DAG suffers from an import_error it populates each error at the top of the Airflow UI in big red flash boxes. For Airflow instances that are shared across multiple teams, this can be annoying when another team's import_errors crowd the top of the screen and cannot be suppressed by users of other teams. I would like to roll up the errors into a collective dropdown so that the information is available and loud without being intrusive.",chris.mclennon
AIRFLOW-4883,Scheduler does not kill hung dag file process managers,"The scheduler does not restart hung dag file process managers. While there is a timeout in the process created to parse a file, there is none on the process itself. We have seen hangs in this process. We need to kill processes that hang.",aoen
AIRFLOW-4880,Allow for value based failure and success criteria in SqlSensor,"Currently SqlSensor has a very specific requirements around what should be returned, and can only signal existence of a particular row. There is no way to fail the sensor based on the values returned, something like CheckOperator needs to be used. 

It would be great to have a way of doing some basic fail/pass logic in the SqlSensor.

It would also be great to fail if no results are returned as opposed to current infinite retries.

 ",drazen
AIRFLOW-4879,Add poll_interval and schema to PrestoHook,PR will follow,eladk
AIRFLOW-4876,Make CoreTest test cases rerunnable,"Contribution to any project is easier when you can quickly iterate on tests. 

We have spotted that tests in CoreTest are not cleaning properly their state in the DB while performing tear down. 

Steps to reproduce: run test in CoreTest twice and some will fail 

We have some code which solves this issue will provide PR.",exploy
AIRFLOW-4873,Improve BROKEN DAG error message by specifying dag_id ,"In some cases there is a broken DAG message appear in the UI:
 
{code:java}
Broken DAG: [/home/airflow/dags/my_dag.py] No module named 'HelperClass'  {code}
 
The problem here is that the error indicates only the file name but it doesn't specify the dag_id.
The error could be improved to (or similar):

 
{code:java}
Broken DAG: DAG_ID : [/home/airflow/dags/my_dag.py] No module named 'HelperClass' {code}
 ",omkarbdesai
AIRFLOW-4871,Unable to create DAG Runs via the (RBAC) UI,"It is not possible to create DagRuns under the new RBAC UI - this is the only way via the web interface to create a DAG run with a specific execution_date (the ""Trigger"" button only gives ""now()"" as the execution_date).",ashb
AIRFLOW-4869,Refactor sql to gcs operators,"Operators for copying data from mysql, postgresql, and mssql share most of their code but also behave slightly differently. We should factor the shared code into a base class and standardize arguments and behaviors across operators.",jmcarp
AIRFLOW-4868,Unable to make example work > Airflow on Kubernetes (Part 1): A Different Kind of Operator ,"Following [https://kubernetes.io/blog/2018/06/28/airflow-on-kubernetes-part-1-a-different-kind-of-operator/] gives the attached issues.
 * minikubes.
 * ./scripts/ci/kubernetes/kube/deploy.sh -d persistent_mode.
 * login with: airflow airflow.
 * All the rest was done following link instructions.

Tried the following but didn't work:

Running:
 * airflow initdb 

{code:java}
root@airflow-6c7c9db768-ttbj9:/# airflow initdb 
/usr/local/lib/python3.6/site-packages/airflow/models/dagbag.py:21: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses import imp 
[2019-06-29 18:04:42,510] {settings.py:173} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=461 
[2019-06-29 18:04:43,002] {__init__.py:51} INFO - Using executor KubernetesExecutor DB: postgresql+psycopg2://root:***@postgres-airflow:5432/airflow 
[2019-06-29 18:04:43,393] {db.py:320} INFO - Creating tables INFO [alembic.runtime.migration] Context impl PostgresqlImpl. INFO [alembic.runtime.migration] Will assume transactional DDL. WARNI [airflow.utils.log.logging_mixin.LoggingMixin] empty cryptography key - values will not be stored encrypted. Done. 
root@airflow-6c7c9db768-ttbj9:/#{code}
Configuring:
 * [webserver] rbac = True

same error

 ",dimberman
AIRFLOW-4864,Don't call load_test_configuration multiple times,"We have a pattern that many test files have cargo-culted from each other to call {{load_test_configuration}}, either at the module level, or inside the {{setUp}} of the test classes.

This isn't needed - we already load the test config from the environment variable we set in travis, so this just causes a (tiny) delay.",ashb
AIRFLOW-4862,Allow directly using IP address as hostname in airflow.utils.net.get_hostname(),"In airflow.utils.net.get_hostname(), the default function used to get hostname for nodes (like worker) is *socket.getfqdn()*, which will return fully qualified domain name.

In some cases, we do need to ensure that hostnames can resolved so that nodes can talk to each other. One example is: if I use S3 for remote logging, then the log will only be pushed to S3 after the job is finished (either success or failure); when the job is still running, webserver will first check if the log is available in its own volume, if not, webserver will fetch log from worker.

If workers' hostnames are something like ""airflow-worker-53-4bp8v"" (e.g., when running on OpenShift or K8S), it's possible that the hostname can't be resolved, then we will observe errors like below
{code:java}
*** Log file does not exist: /opt/app-root/airflow/logs/example_python_operator/sleep_for_3/2019-06-18T08:14:15.313472+00:00/2.log
*** Fetching from: http://airflow-worker-57-n69vb:8793/log/example_python_operator/sleep_for_3/2019-06-18T08:14:15.313472+00:00/2.log
*** Failed to fetch log file from worker. HTTPConnectionPool(host='airflow-worker-57-n69vb', port=8793): Max retries exceeded with url: /log/example_python_operator/sleep_for_3/2019-06-18T08:14:15.313472+00:00/2.log (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fec3266a6d8>: Failed to establish a new connection: [Errno -2] Name or service not known',))
{code}
 

This may be addressed by properly setting service discovery, but the users may not always have the privilege to do that (like myself in my organization).

Another solution is to change ""hostname_callable"" in Airflow's configuration ([core] section). But it will only work if you have a function which can return a resolvable hostname and it can't take any argument (due to the existing implementation [https://github.com/apache/airflow/blob/dd08ae3469a50a145f9ae7f819ed1840fe2a5bd6/airflow/utils/net.py#L41-L45).]

 

The change I would like to propose is: allow users to use IP address directly as hostname.

 ",xd-deng
AIRFLOW-4861,KubernetesPodOperator to print pod event logs when reaching a completion state,"Currently, when KubernetesPodOperators fail to launch the pod due to an error on the Kubernetes side (e.g. Secret does not exist), it fails the pod and Airflow logs report back that the pod failed to launch without further detail. For Airflow users that do not have Kubernetes access or knowledge, this becomes difficult to debug.

I'd like to have KubernetesPodOperators print the pod events when the operator reaches a completion state. This will expose any errors on the Kubernetes side before the pod is able to launch.",chris.mclennon
AIRFLOW-4860,Remove Redundant Information in Example Dags,`airflow/example_dags/example_subdag_operator.py` contains redundant `default_args=args` in all tasks. ,kaxilnaik
AIRFLOW-4858,Conf historical convenience functions are not deprecated properly,"In conf we want to deprecate ""historical convenience functions"" get, getint, getboolean, etc. However the current code doesn't issue a deprecation warning if any of these function is called.",haoliang
AIRFLOW-4857,Add templated fields to SlackWebhookOperator,"The `SlackWebhookOperator` has no templated fields, hence we can't pass Airflow Macros in the message field.",kaxilnaik
AIRFLOW-4855,Task remains in queued state,,anand.g14
AIRFLOW-4854,Env Var passed to K8 worker pod should be case sensitive,"We are using K8Executor and there is a use case where users might provide env variable in lower case which is read by their custom operator in K8 Pod. But currently it seems that in K8 executor env variables are transformed in to upper case [https://github.com/apache/airflow/blob/f520d02cc1f41f9861f479f984bb52bda3860d30/airflow/kubernetes/secret.py#L43].

These should be passed in case sensitive manner",anand.g14
AIRFLOW-4852,XCOM - Hide sensitive data value,"Actually, to hide a XCOM in the web UI, you need to push it with a key name starting with ""_"" and Airflow will hide the key/pair in the web UI.

What I want to implement is another way to hide the XCOM value by replacing it with a string like ""sensitive data"" so I already coded it and so I just need to push it.",n4rk0o
AIRFLOW-4851,Refactor K8S related code to use vendored k8s library,,davlum
AIRFLOW-4850,Config _cmd options should interpolate environment variables,"When specifying a configuration like sql_alchemy_conn_cmd, it should be able to interpolate environment variables from the environment that airflow is running from.  This is really useful when you mount secrets as environment variables and want to use them in this type of configuration option.  For example, this currently does not work:

 

`sql_alchemy_conn_cmd = echo ${POSTGRES_HOST}:${[POSTGRES_PASSWORD}@postgres.com|mailto:POSTGRES_PASSWORD%7D@postgres.com]`

 

The change is really simple, and I opened a PR for it.  [https://github.com/apache/airflow/pull/5479]",gmmotto
AIRFLOW-4848,"MySQL warnings about aborted connections, missing engine disposal","I am not referring to airflow logs in filesystem. I am referrring to logs in the MySQL db itself. This affects airflow 1.10.3, mysql rds 5.7.25

 

ie

 

2019-06-25T09:55:25.126187Z 54996343 [Note] Aborted connection 54996343 to db: 'airflowdb' user: 'f' host: 'host' (Got an error reading communication packets)
 2019-06-25T09:55:25.392705Z 54996375 [Note] Aborted connection 54996375 to db: 'airflowdb' user: 'f' host: 'host' (Got an error reading communication packets)
 2019-06-25T09:55:25.450276Z 54996240 [Note] Aborted connection 54996240 to db: 'airflowdb' user: 'f' host: 'host' (Got an error reading communication packets)
 2019-06-25T09:55:25.592741Z 54996391 [Note] Aborted connection 54996391 to db: 'airflowdb' user: 'f' host: 'host' (Got an error reading communication packets)",dxhuang
AIRFLOW-4847,Update config template to reflect supporting different Celery pool implementation,"In https://issues.apache.org/jira/browse/AIRFLOW-3502 ([https://github.com/apache/airflow/pull/4308)] , the patch to support choosing different Celery pool implementation has been merged. However, this change was not reflected in ""default_airflow.cfg"" while it's the main ""portal"" for most users to know what config they can adjust/change.",xd-deng
AIRFLOW-4846,Allow specification of an existing secret containing git credentials for init containers,"I would like to specify username/password git credentials as a secret so that I don't have plaintext username/passwords stored in my airflow configuration file.  We can assume the given secret has GIT_USERNAME and GIT_PASSWORD fields.

 

Here's my PR [https://github.com/apache/airflow/pull/5475]",gmmotto
AIRFLOW-4845,Kubernetes Executor Run As User 0 does not work,"If I set run_as_user in the kubernetes section of airflow.cfg to 0, that does not get applied.  The problem is this code

```
 if self.kube_config.worker_run_as_user:
     security_context['runAsUser'] = self.kube_config.worker_run_as_user
 ```

if self.kube_config.worker_run_as_user is an int 0, that get implied as a False and thus the security context never gets set.  This also happens to fsGroup.  A small change needs to be made to make this check against a """" (empty string) as that's what it's defaulted to in the default airflow config file.

 

Here is my pr [https://github.com/apache/airflow/pull/5474]",gmmotto
AIRFLOW-4844,Adding option to DAG arguments to specify is_paused flag upon creation of a DAG.,"There is a need in some applications to set the is_paused flag when creating a DAG for the first time. These applications are mostly the ones where there is no need for DAG owner to turn them on manually once the dag is initiated. This is particularly useful for dynamic DAGs that generate an arbitrary number of DAGs based on calls to some upstream data source. Since the DAGs start as paused by default, it requires users to know when a new DAG has been created, and to enable it.

Please note that this feature is configuring is_paused flag per dag object and is different than the existing global config coming from default_airflow.cfg file.",sina1367
AIRFLOW-4843,Allow orchestration of tasks with Docker Swarm aka `SwarmOperator`,"Currently, Airflow supports spawning Docker containers for running tasks via the {color:#707070}_DockerOperator_{color} but these containers are run on the same node as the scheduler.

It would be helpful for our use-case to be able to spawn these tasks wherever resources are available in our Docker Swarm cluster.

 

This can be achieved by creating a Docker swarm service, waiting for its run and removing it after it has completed execution.

This approach has been suggested/discussed at various places (and implemented in Golang for Swarm-cronjob):

[https://blog.alexellis.io/containers-on-swarm/]

[https://forums.docker.com/t/running-one-off-commands-in-swarm-containers/42436/3]

[https://gist.github.com/alexellis/e11321b8fbfc595c208ea3e74bf5e54b]

 ",akkidx
AIRFLOW-4842,Issue deprecation notice on Py2 on 1.10 releases.,,ashb
AIRFLOW-4840,Fix pylint errors regarding logging-format-interpolation,,dzakus13
AIRFLOW-4839,Fix pylint errors regarding superfluous-parens,,dzakus13
AIRFLOW-4838,Surface Athena errors in AWSAthenaOperator,"Currently when the AWSAthenaOperator raises an error it only includes the query state (failed or cancelled) and the query_execution_id. While more information about any failed queries is available in the returned boto3 response, it is not surfaces in the failure exception.

The error message should be included whenever the AWSAthenaOperator raises an exception because of a failure state.",cjsekl
AIRFLOW-4837,Fix pylint errors regarding ungrouped imports,,dzakus13
AIRFLOW-4836,Fixing pylint errors regarding file opening,,kamil.bregula
AIRFLOW-4833,Jinja templating removes newlines,"When using an operator that has Jinja templating enabled for a field, if the field value ends with a newline then the newline is removed, regardless of whether there was a template in the string.

 

This came up when attempting to send data to Prometheus pushgateway using the SimpleHttpOperator. Pushgateway requires a newline at the end of every entry, so the removal of the newline at the end of the data parameter causes the request to fail in a way that is difficult to debug.

 

This can be gotten around by including a space after the newline character, though this is not a great solution. The space is ignored by pushgateway.",galak75
AIRFLOW-4831,conf.has_option raises an error if the given section is missing instead of returning false,"Currently, conf.has_option raises an error if the given section is missing. I think it should return false if either option or section is missing, which is also the behavior of ConfigParser.has_option.",haoliang
AIRFLOW-4829,EMR job flow and step sensor should provide a reason why the job failed,"Currently, when using the EmrJobFlowSensor and the EmrStepSensor there is an exception raised when a cluster fails. But no information is provided on what the failure was. The sensor that raises the exception already has the info in the boto3 response from the EMR api and this could be easily provided by extending the exception message with more details. This could be made a boolean or just always provided.",jzucker
AIRFLOW-4828,Remove param python_version in PythonVirtualenvOperator,Remove param python_version in PythonVirtualenvOperator,zhongjiajie
AIRFLOW-4827,Remove compatible test for python 2,Remove compatible test for python 2,zhongjiajie
AIRFLOW-4826,Resetdb command throws warning in new version of alembic,"Resetdb still works but connection should be updated
{code:java}
$ airflow resetdb
{...}
[2019-06-20 09:52:58,165] \{__init__.py:51} INFO - Using executor CeleryExecutor
DB: postgres+psycopg2://USER:PASS@SERVER/DBNAME
This will drop existing tables if they exist. Proceed? (y/n)y
[2019-06-20 09:53:00,869] \{db.py:370} INFO - Dropping tables that exist
/path/to/ve/lib64/python3.6/site-packages/alembic/util/messaging.py:69: UserWarning: 'connection' argument to configure() is expected to be a sqlalchemy.engine.Connection instance, got Engine(postgres+psycopg2://USER:PASS@SERVER/DBNAME)
 warnings.warn(msg)
[2019-06-20 09:53:01,010] \{migration.py:117} INFO - Context impl PostgresqlImpl.

{...}
{code}",trker
AIRFLOW-4822,"When clearing task instance of subdag, irrelevant tasks within parent dag might be cleared as well.","Dag file:
{code}
import datetime

from airflow.models import DAG
from airflow.operators.bash_operator import BashOperator
from airflow.operators.subdag_operator import SubDagOperator

def create_subdag_opt(main_dag):
    subdag_name = ""daily_job""
    subdag = DAG(
        dag_id='.'.join([dag_name, subdag_name]),
        start_date=start_date,
        schedule_interval=None,
        concurrency=2,
    )
    subdag_task = BashOperator(
        bash_command=""echo 1"",
        task_id=""daily_job_subdag_task"",
        dag=subdag
    )
    return SubDagOperator(
        task_id=subdag_name,
        subdag=subdag,
        dag=main_dag,
    )

dag_name = ""daily_job_parent""

start_date = datetime.datetime(2019, 6, 18)

dag = DAG(
    dag_id=dag_name,
    concurrency=3,
    start_date=start_date,
    schedule_interval=""45 18 * * *""
)

daily_job_irrelevant = BashOperator(
    bash_command=""echo 1"",
    task_id=""daily_job_irrelevant"",
    dag=dag,
)
daily_job = create_subdag_opt(main_dag=dag)
{code}
 Graph view:

!image-2019-06-20-15-20-39-565.png!

 

!image-2019-06-20-15-21-08-614.png!

When clearing dagrun of task: daily_job_subdag_task,

  !image-2019-06-20-15-21-28-744.png!

irrelevant task daily_job_parent.daily_job_irrelevant is listed in the to-clear list, which is not expected.",djfffff
AIRFLOW-4820,Fix unnecessary-pass errors in pylint,,kamil.bregula
AIRFLOW-4819,Fix singleton-comparison errors in pylint,,kamil.bregula
AIRFLOW-4818,Remove valid file from todo list,,kamil.bregula
AIRFLOW-4817,Remove deprecated methods from tests,"This is a bug detected by Pylint.

 ",kamil.bregula
AIRFLOW-4816,add MySQLToS3Operator,"Airflow doesn't have MySQLToS3Operator 

would be nice if someone can add it",wkhudgins
AIRFLOW-4811,Implement GCP DLP' Hook and Operators,Implement GCP DLP' Hook and Operators,ryan.yuan
AIRFLOW-4809,s3_delete_objects_operator fails on empty list of keys,"When s3_delete_objects_operator is used in a dynamic way (for example list of keys comes from s3_list_operator via XCom) there might be a case when the list of keys is empty. In my case it happens when chained operators are removing old files from S3 and there are no old files yet (because this is very first run of DAG).

In case of empty `keys` hook raises an exception (via boto3):
{noformat}
[2019-06-18 13:23:53,790] {{base_task_runner.py:101}} INFO - Job 115: Subtask delete_old_files [2019-06-18 13:23:53,790] {{cli.py:517}} INFO - Running <TaskInstance: xxxxxx.delete_old_files 2019-06-17T00:00:00+00:00 [running]> on host 82f571f444f5
[2019-06-18 13:23:56,199] {{__init__.py:1580}} ERROR - An error occurred (MalformedXML) when calling the DeleteObjects operation: The XML you provided was not well-formed or did not validate against our published schema
Traceback (most recent call last):
  File ""/usr/local/lib/python3.6/site-packages/airflow/models/__init__.py"", line 1441, in _run_raw_task
    result = task_copy.execute(context=context)
  File ""/usr/local/lib/python3.6/site-packages/airflow/contrib/operators/s3_delete_objects_operator.py"", line 80, in execute
    response = s3_hook.delete_objects(bucket=self.bucket, keys=self.keys)
  File ""/usr/local/lib/python3.6/site-packages/airflow/hooks/S3_hook.py"", line 542, in delete_objects
    Delete=delete_dict)
  File ""/usr/local/lib/python3.6/site-packages/botocore/client.py"", line 357, in _api_call
    return self._make_api_call(operation_name, kwargs)
  File ""/usr/local/lib/python3.6/site-packages/botocore/client.py"", line 661, in _make_api_call
    raise error_class(parsed_response, operation_name)
botocore.exceptions.ClientError: An error occurred (MalformedXML) when calling the DeleteObjects operation: The XML you provided was not well-formed or did not validate against our published schema
{noformat}

I already have a patch that checks if there is anything to delete and if not, just returns from the operator.",szczeles
AIRFLOW-4808,Error when getting an already deleted dataproc cluster status ,"When a dataproc cluster is manually deleted, ` it will throw an NoneType is not iterable error when *DataprocClusterCreateOperator* operator tries to get cluster status, ",london_su
AIRFLOW-4807,Make GCS operators Pylint compatible,,urbaszek
AIRFLOW-4803,Support more flexible auth for HTTP Hook/Operator/Sensor,"Current situation:
 * Currently HTTP Hook/Operator/Sensor support authentication by specifying ""login"" and ""password"" in connection definitions
 * ""login"" and ""password"" can't be empty string, otherwise it will be considered as ""None""

(current code: [https://github.com/apache/airflow/blob/8644a6f94c345b10dce46b9c71a02c2c1285bc85/airflow/hooks/http_hook.py#L72])

 

However, there are a few limitations:
 * in some cases, either 'login' or ""password"" or both can be empty string (not ""None"") (for example, [https://www.cloudera.com/documentation/data-science-workbench/latest/topics/cdsw_rest_apis.html#start_job_python])
 * ""requests"" library actually supports much more flexible authentication [https://2.python-requests.org/en/master/user/authentication/]

 ",xd-deng
AIRFLOW-4800,GKE operators call GKEClusterHook with wrong (old) arguments,"[#4364|https://github.com/apache/airflow/pull/4364/files#diff-6c81e7d223790753a57708ed860b51d1R39] changed GKEClusterHook's ctor arguments, but not call-sites in [GKEClusterDeleteOperator|https://github.com/apache/airflow/blob/2d18f0738f775183b06d19f0b6703285c8bdcb32/airflow/contrib/operators/gcp_container_operator.py#L89] and [GKEClusterCreateOperator|https://github.com/apache/airflow/blob/2d18f0738f775183b06d19f0b6703285c8bdcb32/airflow/contrib/operators/gcp_container_operator.py#L176].

[Relevant tests|https://github.com/apache/airflow/blob/2d18f0738f775183b06d19f0b6703285c8bdcb32/tests/contrib/operators/test_gcp_container_operator.py#L57-L58] still assert that the old arguments are passed.

I'd have thought that the {{location}} arg being passed to {{delegate_to}} would cause real usages of the GKE operators to break, but perhaps it doesn't and they are working. I'm not sure what real-world use of these operators is like.

I think the fix is trivial and will PR it shortly.",rdub
AIRFLOW-4799,tests using bash operator fail flakily because jinja2 rendering of environment variables,"In test_retry_delay in task instance tests we see this:

{code:python}
        ti = TI(
            task=task, execution_date=timezone.utcnow())

        self.assertEqual(ti.try_number, 1)
        # first run -- up for retry
        run_with_error(ti)
        self.assertEqual(ti.state, State.UP_FOR_RETRY)
        self.assertEqual(ti.try_number, 2)

        # second run -- still up for retry because retry_delay hasn't expired
        run_with_error(ti)
        self.assertEqual(ti.state, State.UP_FOR_RETRY)

        # third run -- failed
        time.sleep(3)
        run_with_error(ti)
        self.assertEqual(ti.state, State.FAILED)
{code}

The same TI is re-run multiple times.

The problem is that in the execute method of BashOperator, the {{env}} attribute is modified, and updated to include a copy of the parent bash env.  Then when this TI is executed another time, it again tries to render {{env}}, because it is a templated parameter.  And it will attempt to load every {{.sh}} or {{.bash}} file found in `env` as a template.  If one file does not exist, test will fail with template not found error.

The modifications of env in execute method should be local to execute method. ",dstandish
AIRFLOW-4798,tests may reference dagbag objects that do not yet exist,For example TaskInstance test `test_depends_on_past` tries to grab a dag 'test_depends_on_past' but it is not there.  It is created in `test_scheduler_job.py` so if you don't run that before task instance tests you will get failure.,dstandish
AIRFLOW-4797,Zombie detection and killing is not deterministic,"Zombie detection and killing is done within the DAG file processing loop. Within one iteration only a subset of the DAG files are processed (config scheduler.max_threads). The loop sleeps for the rest of the second, until the next iteration runs which processes the next subset of DAG files. The function to get zombie task instancs only returns zombies once within 10 seconds, otherwise an empty list is returned.

That means only in every 10th iteration of the DAG file processing loop zombies are detected. And only if the zombie task belong to one of the DAG files of the current iteration they are killed.

We run into the worst case scenario with max_threads=2 and 20 DAGs. In such a scenario only zombies of the same 2 DAGs are killed. (as loop iterations are not exactly 1s it shifts slowly and eventually the zomies are killed, but in one example it took 33 minutes).
",seelmann
AIRFLOW-4795,Upgrade alembic,Use recent versions of alembic to pick up some performance and deprecation fixes. See the changelog at [https://alembic.sqlalchemy.org/en/latest/changelog.html] for details.,jmcarp
AIRFLOW-4794,Allow access to airflow context in Kubernetes Pods Operator,Change the Kubernetes Pod Operator so that it has access to context at runtime.  This could be done by passing a callable into the constructor the callable would take the context and the existing env_vars as parameters and allow the developer to manipulate to the env_vars at run time before they are used to create the pod object. ,whi
AIRFLOW-4793,The mlengine_operator should support the signature_name field,`signature_name` is an optional field for PredictionInput (reference: [https://cloud.google.com/ml-engine/reference/rest/v1/projects.jobs#Job]) and the operator should support it.,dossett
AIRFLOW-4792,Include Sentry for Error Tracking Contrib,"This hook integration lets you easily use the Sentry SDK in Airflow.

It provides:
 * the sentry sdk for DAG specific runs.",tiopi
AIRFLOW-4790,dag processing statsd metrics are implemented as gauges but should be timers,"These two metrics were implemented as gauges but should be timers to follow the convention of other metrics that measure how long something takes to run.
dag_processing.last_runtime.<dag_file>
dag_processing.last_run.seconds_ago.<dag_file>
",andrewhharmon
AIRFLOW-4784,Make GCP operators Pylint compatible,,higrys
AIRFLOW-4782,Make GCP hooks Pylint compatible,,urbaszek
AIRFLOW-4781,Added the ability to specify ports in kubernetesOperator,"In kubernetes you have the ability to specify which ports to open to the container.

 
{code:java}
containers:
    - name: task-pv-container
      image: nginx
      ports:
        - containerPort: 80
          name: ""http-server""
{code}
In this issue we want to add that support to the kubernetesOperator and `PodRequestFactory`

With the support in the PodRequestFactory we can add functionality to build an operator that start a container in kubernetes and interact over the opened port.

 ",rweverwijk
AIRFLOW-4780,CLI connections add option -i for JSON file,"We have a ton's of connections and we would like to register them with a json file , like variables and pools 

today we have to call the cli for each connection , wich take more than 4 minutes.

So It would be nice to have a -i option to import a JSON file",raphaelauv
AIRFLOW-4779,Implement GCP Stackdriver' Hook and Operators,"Implement GCP Stackdriver Logging' Hook and Operators.

 - Read, write, delete logs

 - Export logs to GCS, BQ or Pub/Sub",ryan.yuan
AIRFLOW-4778,Improve get_logs for KubernetesPodOperator,"Current KubernetesPodOperator's get_logs option has following weaknesses:

- It gets only 10 lines of logs from the end.
- Because it reads logs when the pod starts, you may not get the last log lines.

The following improvements fixes these weaknesses:

- Add `tail_lines` parameter to set the number of log lines.
- Modify to read the logs when the pod finishes.",yoichiwo7
AIRFLOW-4777,Simplify the Python require in setup.py,,fokko
AIRFLOW-4776,Add type hints to models package,Add type hints to models package,pgag
AIRFLOW-4774,MLEngineHook should use the built-in Google Discovery API method for retry requests,"Hello,

MLEngineHook should use the built-in Google Discovery API method for retry requests. In other words, It should pass the num_retry parameter to the execute method.

Currently, the implementation uses its own mechanism implemented in the _poll_with_exponential_delay function.

 

Thanks

 ",urbaszek
AIRFLOW-4768,Timeout parameter in example_gcp_video_intelligence,The example uses a timeout parameter that does not exist in operators.,urbaszek
AIRFLOW-4767,Errors in the documentation of Dataproc Operator,"Hello,

Operator documentation contains many errors. We should think about it deeper.

 

Example 1:

DataProcPigOperator accepts the query_uri parameter. 

Airflow documentation describes this parameter as:
{quote}The uri of a pig script on Cloud Storage.
{quote}
[https://cloud.google.com/dataproc/docs/reference/rest/v1beta2/PigJob]

Google documentation describes this parameters as:
{quote}The HCFS URI of the script that contains the Pig queries.
{quote}
[https://cloud.google.com/dataproc/docs/reference/rest/v1beta2/PigJob]

 

I am afraid that there are similar mistakes in other places, so we should check the entire documentation.

 ",urbaszek
AIRFLOW-4766,Add autoscaling option for DataprocClusterCreateOperator,Add autoscaling param for scaling Dataproc cluster.,urbaszek
AIRFLOW-4765,Recursion error in DataProcPigOperator,"Recursion error in DataProcPigOperator in execute method:

 
{code:java}
class DataProcPigOperator(DataProcJobBaseOperator):
    ...
    def execute(self, context):
        self.create_job_template()
        if self.query is None:
            self.job_template.add_query_uri(self.query_uri)
        else:
            self.job_template.add_query(self.query)
            self.job_template.add_variables(self.variables)

        self.execute(context) -> super().execute(context){code}
 ",urbaszek
AIRFLOW-4763,DockerOperator command parameter does not allow lists,"According to the docstring `command` parameter allows `list` as input ([https://github.com/apache/airflow/blob/master/airflow/operators/docker_operator.py#L56])

But the `strip` function being called here indicates that `command` is assumed to be a string ([https://github.com/apache/airflow/blob/master/airflow/operators/docker_operator.py#L250])

 

 

It would be very nice to allow `list` inputs as well. I think the fix would be to add this before the line calling `.strip()`:

{code:python}
if type(command) == list:

    commands = command
{code}
",akkidx
AIRFLOW-4762,"Test against Python 3.6, 3.7 and 3.8b1","Airflow is currently tested against python 3.5 only. This may be insufficient as users may wish to run airflow on newer versions, which may introduce breaking changes.",pgag
AIRFLOW-4760,Packaged DAGs disappear from the DagBag,"AIRFLOW-2900 introduced a change that broke Airflow whenever it tries to reprocess a packaged DAG. Since that change, the {{fileloc}} column in the database for packaged DAGs isn't an actual filepath. It looks something like: {{/usr/local/airflow/dags/package.zip/my_dag.py}} -- *notice that the path to the DAG inside the zip is appended to the zip file's path*. Then, when the {{DagBag#process_file}} method tries to load that filepath, it bails because that filepath is invalid.

 

Steps to reproduce:

 
 # Add a packaged DAG to your DAGS_FOLDER
 # From the main /admin UI, click on the DAG to open it's detail view
 # Click the ""Refresh"" button

 

What should happen:

The detail view should refresh and display a ""flash"" message saying that the DAG is ""fresh as a daisy.""

 

What actually happens:

The app redirects to the main /admin UI and displays a ""flash"" error message in addition to the ""fresh as a daisy"" message.

 

See animated GIF, attached.",danmactough
AIRFLOW-4759,Improve performance of /success endpoint,"Setting a dagrun to success or failure calls `set_state` for each task in the dag, running multiple database queries for each one. We can reduce the number of queries, and improve performance for the associated endpoints, by setting the states of all relevant tasks in the same query.",jmcarp
AIRFLOW-4757,Selectively disable some pylint checks for tests,"Maybe it is a good idea to selectively disable missing-docstrings for tests ? This seems to be a huge overhead having to document all test modules/classes and especially methods. And it seems easy to do with the pylint_ci script we have.

 

After a bit of discussion with [~ash] , [~kamil.bregula] and [~BasPH]  we decided to exclude those checks for testS:
 * missing-docstrings
 * no-self-use
 * too-many-public-methods
 * protected-access

 ",higrys
AIRFLOW-4756,gantt chart view fails after clearing failed task in current run,"To repro: 
Make some dag with a number of decently long-running tasks (so you have time to do this).

Get the dag running

Make sure one of the tasks fails.

Before the others complete, clear the failing task.

View gantt chart.

Observer error like so:

{code}
Traceback (most recent call last):
  File ""/Users/someperson/.virtualenvs/scratch/lib/python3.7/site-packages/flask/app.py"", line 2292, in wsgi_app
    response = self.full_dispatch_request()
  File ""/Users/someperson/.virtualenvs/scratch/lib/python3.7/site-packages/flask/app.py"", line 1815, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File ""/Users/someperson/.virtualenvs/scratch/lib/python3.7/site-packages/flask/app.py"", line 1718, in handle_user_exception
    reraise(exc_type, exc_value, tb)
  File ""/Users/someperson/.virtualenvs/scratch/lib/python3.7/site-packages/flask/_compat.py"", line 35, in reraise
    raise value
  File ""/Users/someperson/.virtualenvs/scratch/lib/python3.7/site-packages/flask/app.py"", line 1813, in full_dispatch_request
    rv = self.dispatch_request()
  File ""/Users/someperson/.virtualenvs/scratch/lib/python3.7/site-packages/flask/app.py"", line 1799, in dispatch_request
    return self.view_functions[rule.endpoint](**req.view_args)
  File ""/Users/someperson/.virtualenvs/scratch/lib/python3.7/site-packages/flask_admin/base.py"", line 69, in inner
    return self._run_view(f, *args, **kwargs)
  File ""/Users/someperson/.virtualenvs/scratch/lib/python3.7/site-packages/flask_admin/base.py"", line 368, in _run_view
    return fn(self, *args, **kwargs)
  File ""/Users/someperson/.virtualenvs/scratch/lib/python3.7/site-packages/flask_login/utils.py"", line 258, in decorated_view
    return func(*args, **kwargs)
  File ""/Users/someperson/.virtualenvs/scratch/lib/python3.7/site-packages/airflow/www/utils.py"", line 275, in wrapper
    return f(*args, **kwargs)
  File ""/Users/someperson/.virtualenvs/scratch/lib/python3.7/site-packages/airflow/utils/db.py"", line 73, in wrapper
    return func(*args, **kwargs)
  File ""/Users/someperson/.virtualenvs/scratch/lib/python3.7/site-packages/airflow/www/views.py"", line 2015, in gantt
    root=root,
  File ""/Users/someperson/.virtualenvs/scratch/lib/python3.7/site-packages/flask_admin/base.py"", line 308, in render
    return render_template(template, **kwargs)
  File ""/Users/someperson/.virtualenvs/scratch/lib/python3.7/site-packages/flask/templating.py"", line 135, in render_template
    context, ctx.app)
  File ""/Users/someperson/.virtualenvs/scratch/lib/python3.7/site-packages/flask/templating.py"", line 117, in _render
    rv = template.render(context)
  File ""/Users/someperson/.virtualenvs/scratch/lib/python3.7/site-packages/jinja2/asyncsupport.py"", line 76, in render
    return original_render(self, *args, **kwargs)
  File ""/Users/someperson/.virtualenvs/scratch/lib/python3.7/site-packages/jinja2/environment.py"", line 1008, in render
    return self.environment.handle_exception(exc_info, True)
  File ""/Users/someperson/.virtualenvs/scratch/lib/python3.7/site-packages/jinja2/environment.py"", line 780, in handle_exception
    reraise(exc_type, exc_value, tb)
  File ""/Users/someperson/.virtualenvs/scratch/lib/python3.7/site-packages/jinja2/_compat.py"", line 37, in reraise
    raise value.with_traceback(tb)
  File ""/Users/someperson/.virtualenvs/scratch/lib/python3.7/site-packages/airflow/www/templates/airflow/gantt.html"", line 18, in top-level template code
    {% extends ""airflow/dag.html"" %}
  File ""/Users/someperson/.virtualenvs/scratch/lib/python3.7/site-packages/airflow/www/templates/airflow/dag.html"", line 19, in top-level template code
    {% import 'admin/lib.html' as lib with context %}
  File ""/Users/someperson/.virtualenvs/scratch/lib/python3.7/site-packages/airflow/www/templates/airflow/master.html"", line 18, in top-level template code
    {% extends ""admin/master.html"" %}
  File ""/Users/someperson/.virtualenvs/scratch/lib/python3.7/site-packages/airflow/www/templates/admin/master.html"", line 18, in top-level template code
    {% extends 'admin/base.html' %}
  File ""/Users/someperson/.virtualenvs/scratch/lib/python3.7/site-packages/flask_admin/templates/bootstrap3/admin/base.html"", line 94, in top-level template code
    {% block tail %}
  File ""/Users/someperson/.virtualenvs/scratch/lib/python3.7/site-packages/airflow/www/templates/airflow/gantt.html"", line 58, in block ""tail""
    data = {{ data |tojson|safe }};
  File ""/Users/someperson/.virtualenvs/scratch/lib/python3.7/site-packages/flask/json/__init__.py"", line 327, in tojson_filter
    return Markup(htmlsafe_dumps(obj, **kwargs))
  File ""/Users/someperson/.virtualenvs/scratch/lib/python3.7/site-packages/flask/json/__init__.py"", line 242, in htmlsafe_dumps
    rv = dumps(obj, **kwargs) \
  File ""/Users/someperson/.virtualenvs/scratch/lib/python3.7/site-packages/flask/json/__init__.py"", line 179, in dumps
    rv = _json.dumps(obj, **kwargs)
  File ""/usr/local/Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/lib/python3.7/json/__init__.py"", line 238, in dumps
    **kw).encode(obj)
  File ""/usr/local/Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/lib/python3.7/json/encoder.py"", line 199, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/usr/local/Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/lib/python3.7/json/encoder.py"", line 257, in iterencode
    return _iterencode(o, 0)
TypeError: '<' not supported between instances of 'NoneType' and 'str'
{code}",dstandish
AIRFLOW-4755,"Dockerhub user should be ""apache"" not airflow",The hooks/build script default DOCKERHUB_USER should be apache not airflow :),higrys
AIRFLOW-4754,Exception is thrown when .git folder is not found by setup.py (for example in DockerHub),"{code:java}
Complete output from command /usr/local/bin/python -c ""import setuptools, tokenize;__file__='/opt/airflow/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" develop --no-deps:
Traceback (most recent call last):
File ""<string>"", line 1, in <module>
File ""/opt/airflow/setup.py"", line 458, in <module>
do_setup()
File ""/opt/airflow/setup.py"", line 319, in do_setup
write_version()
File ""/opt/airflow/setup.py"", line 152, in write_version
text = ""{}"".format(git_version(version))
File ""/opt/airflow/setup.py"", line 132, in git_version
repo = git.Repo('.git')
File ""/usr/local/lib/python3.6/site-packages/git/repo/base.py"", line 131, in __init__
raise NoSuchPathError(epath)
git.exc.NoSuchPathError: /opt/airflow/.git{code}",higrys
AIRFLOW-4753,Pylint Json log formater ,,higrys
AIRFLOW-4752,Pylint CI Script has missing star in build directory exclusion and misses generated webserver_config.py ,,higrys
AIRFLOW-4750,Add log to specify which zombie task instances are killed,While troubleshooting task instances that got killed I wanted to see if the task instance got killed by the process that finds zombies and added a log to print which task instances were identified as zombies. I think this will be useful to share.,chris.mclennon
AIRFLOW-4748,dagbag exception catching does not print a stack trace,"In the dagbag `process_file` method, if an exception is caught during processing only the error message is logged.  A full stack trace can be very helpful in these situations.",shuwen
AIRFLOW-4746,Implement GCP Cloud Tasks' Hook and Operators,Implement hook and operators for GCP Cloud Tasks.,ryan.yuan
AIRFLOW-4745,failed filtering when accessing DAG Runs page from DAG information page,"When click ""{{schedule: X day, hh:mm:ss}}"" on DAG information page such as Tree View, we can jump to DAG Runs page, but not filtering.

This is easily implement so I'll create this PR.",hkak03key
AIRFLOW-4743,Add environment variables support to SSHOperator,,ms32035
AIRFLOW-4741,Include Sentry for Error Tracking in Core functionality,"This core integration lets you easily use the Sentry SDK in Airflow.

It provides:
 * a set of tags for every TaskInstance, along with its respective DAG
 * breadcrumbs of the tasks that were successful before any crash

*Note: This feature is completely optional to users, if a connection or the sentry sdk is not found the hook is not used.",tiopi
AIRFLOW-4739,Kubernetes Executor:  Allow per task labels on task pods,"I want to be able to add arbitrary labels to an airflow worker pod.  For example, I want to specify this in the task definition:

```
 run_this = PythonOperator(
 task_id='print_the_context',
 provide_context=True,
 python_callable=my_sleeping_function,
 executor_config={""KubernetesExecutor"": {""labels"":

{""test"": ""label""}

}},
 dag=dag,
 )
 ```

And have my worker pod have the label `test:label`. 

 

My main use case for this is for auditing.  We audit our kubernetes cluster by tags, attributing cost based on how much resources the pod uses.  

 

I will make a PR.",gmmotto
AIRFLOW-4737,Column queue on task_instance table limit of 50 chars is too low,"I tried to use an SQS queue named ""airtest-AirflowStack-FW7GGJK59W9-Tasks-BL0L2W6F5OKF"" as message broker, but the Scheduler fails to work because the queue name is 51 characters long.

Why this huge queue name? SQS queue names are automatically generated by CloudFormation, and it's best practice to let it do so. Also, when using nested stacks, the inner stack also has its name automatically generated and appended to the master stack.

So in short, my master stack is called ""airtest"" and the queue resource ""Tasks"" (which I believe are quite reasonably sized), but that huge name is what you end up in the end. Therefore the use case for longer queue names is there.",villasv
AIRFLOW-4736,S3 Operator - Recursive Copy Functionality,"The current s3 copy operator does not allow for recursive copy (ex: `aws s3 cp s3://bucket/folder/ – recursive `).

 

Anyone have thoughts on whether there should be a recursive copy operator, or whether the current operator should be extended?  ",brucearctor
AIRFLOW-4735,S3 Operator - Sync Functionality,"Would love for there to be the equivalent of `aws s3 sync <source>  <dest>`, that would be in operator form.  

 

 ",brucearctor
AIRFLOW-4734,Upsert functionality for PostgresHook.insert_rows(),"PostgresHook's parent class, DbApiHook, implements upsert in its insert_rows() method with the replace=True flag. However, the underlying generated SQL is specific to MySQL's ""REPLACE INTO"" syntax and is not applicable to Postgres.

I'd like to override this method in PostgresHook to implement the ""INSERT ... ON CONFLICT DO UPDATE"" syntax (new since Postgres 9.5)",oxymor0n
AIRFLOW-4726,Duplicate operator task ids raise inappropriate error message,"When there are duplicate operator task ids, it raises `_Broken DAG: Cycle detected in DAG. Faulty task: foo to foo_`.

This error message is ambiguous, so I suggest that `_Broken DAG: duplicate task ids. ~~~_` is better.

 

I examined this case on my local environment (Airflow version: 1.10.3).

DAG script and error messages are on Attachment.

(I changed [https://airflow.apache.org/tutorial.html] a little to make this DAG script.)",dama_yu
AIRFLOW-4725,Make setup.py run without errors/warnings,"Currently running ""python setup.py --help-commands"" (on master) throws a warning + error.",basph
AIRFLOW-4723,Remove the deprecated pass-through conf methods,"This has been deprecated since 2016 (airflow/configuration.py):
{code}
# Historical convenience functions to access config entries

load_test_config = conf.load_test_config
get = conf.get
getboolean = conf.getboolean
getfloat = conf.getfloat
getint = conf.getint
getsection = conf.getsection
has_option = conf.has_option
remove_option = conf.remove_option
as_dict = conf.as_dict
set = conf.set # noqa

for func in [load_test_config, get, getboolean, getfloat, getint, has_option,
             remove_option, as_dict, set]:
    deprecated(
        func,
        ""Accessing configuration method '{f.__name__}' directly from ""
        ""the configuration module is deprecated. Please access the ""
        ""configuration from the 'configuration.conf' object via ""
        ""'conf.{f.__name__}'"".format(f=func)){code}
 Time to remove it and unify the way to access conf.",basph
AIRFLOW-4721,Remove all import builtins imports,There's lots of from builtins import ... imports in the codebase. These are all the result of Python 2/3 compatibility and should be removed.,basph
AIRFLOW-4720,Allow comments in .airflowignore files,It would useful to allow comments in .airflowignore files to explain why a pattern has been ignored. I propose using # as the comment character and skipping all characters on a line following a #.,jmcarp
AIRFLOW-4717,The spark_binary arg to SparkSubmitOperator has no effect when connection exists.,"Apache spark depending on the desto, the spark binary name may be different. (ex. spark2-submit)
For this reason, the spark_binary option has been added to sparkSubmitOperator

(Reference : [https://github.com/apache/airflow/pull/4360/files])

 

However, this option does not work.

This is because there is logic to hard-code and override the spark-binary option value in spark_submit_hook.py

(Full path : airflow/contrib/hooks/spark_submit_hook.py)

 
{code:java}
...
conn_data['spark_binary'] = extra.get('spark-binary', ""spark-submit"")
...{code}
It is necessary to delete the corresponding line.",yslee
AIRFLOW-4716,Instrument dag loading time duration,Sometimes it is useful to instrument the dag loading time and see which dag takes the longest the load. This is helpful for debugging once the UI becomes a bit unstable(e.g gunicorn timeout etc),taofeng
AIRFLOW-4712,Make tests/ti_deps Pylint compatible,Fix all Pylint messages in tests/ti_deps. To start; running scripts/ci/ci_pylint.sh on master should produce no messages. (1) Remove the files mentioned in your issue from the blacklist. (2) Run scripts/ci/ci_pylint.sh to see all messages on the no longer blacklisted files. (3) Fix all messages and create PR.,basph
AIRFLOW-4690,Make tests/api Pylint compatible,Fix all Pylint messages in tests/api. To start; running scripts/ci/ci_pylint.sh on master should produce no messages. (1) Remove the files mentioned in your issue from the blacklist. (2) Run scripts/ci/ci_pylint.sh to see all messages on the no longer blacklisted files. (3) Fix all messages and create PR.,basph
AIRFLOW-4689,Make setup.py Pylint compatible,Fix all Pylint messages in setup.py. To start; running scripts/ci/ci_pylint.sh on master should produce no messages. (1) Remove the files mentioned in your issue from the blacklist. (2) Run scripts/ci/ci_pylint.sh to see all messages on the no longer blacklisted files. (3) Fix all messages and create PR.,basph
AIRFLOW-4686,Make dags Pylint compatible,Fix all Pylint messages in dags. To start; running scripts/ci/ci_pylint.sh on master should produce no messages. (1) Remove the files mentioned in your issue from the blacklist. (2) Run scripts/ci/ci_pylint.sh to see all messages on the no longer blacklisted files. (3) Fix all messages and create PR.,feluelle
AIRFLOW-4684,Make airflow/utils Pylint compatible,Fix all Pylint messages in airflow/utils. To start; running scripts/ci/ci_pylint.sh on master should produce no messages. (1) Remove the files mentioned in your issue from the blacklist. (2) Run scripts/ci/ci_pylint.sh to see all messages on the no longer blacklisted files. (3) Fix all messages and create PR.,kik
AIRFLOW-4683,Make airflow/ti_deps Pylint compatible,Fix all Pylint messages in airflow/ti_deps. To start; running scripts/ci/ci_pylint.sh on master should produce no messages. (1) Remove the files mentioned in your issue from the blacklist. (2) Run scripts/ci/ci_pylint.sh to see all messages on the no longer blacklisted files. (3) Fix all messages and create PR.,serkef
AIRFLOW-4681,Make airflow/sensors Pylint compatible,Fix all Pylint messages in airflow/sensors. To start; running scripts/ci/ci_pylint.sh on master should produce no messages. (1) Remove the files mentioned in your issue from the blacklist. (2) Run scripts/ci/ci_pylint.sh to see all messages on the no longer blacklisted files. (3) Fix all messages and create PR.,kamil.bregula
AIRFLOW-4678,Make airflow/models Pylint compatible,Fix all Pylint messages in airflow/models. To start; running scripts/ci/ci_pylint.sh on master should produce no messages. (1) Remove the files mentioned in your issue from the blacklist. (2) Run scripts/ci/ci_pylint.sh to see all messages on the no longer blacklisted files. (3) Fix all messages and create PR.,basph
AIRFLOW-4673,Make airflow/jobs Pylint compatible,Fix all Pylint messages in airflow/jobs. To start; running scripts/ci/ci_pylint.sh on master should produce no messages. (1) Remove the files mentioned in your issue from the blacklist. (2) Run scripts/ci/ci_pylint.sh to see all messages on the no longer blacklisted files. (3) Fix all messages and create PR.,basph
AIRFLOW-4672,Make airflow/hooks Pylint compatible,Fix all Pylint messages in airflow/hooks. To start; running scripts/ci/ci_pylint.sh on master should produce no messages. (1) Remove the files mentioned in your issue from the blacklist. (2) Run scripts/ci/ci_pylint.sh to see all messages on the no longer blacklisted files. (3) Fix all messages and create PR.,omkarbdesai
AIRFLOW-4671,Make airflow/executors Pylint compatible,Fix all Pylint messages in airflow/executors. To start; running scripts/ci/ci_pylint.sh on master should produce no messages. (1) Remove the files mentioned in your issue from the blacklist. (2) Run scripts/ci/ci_pylint.sh to see all messages on the no longer blacklisted files. (3) Fix all messages and create PR.,basph
AIRFLOW-4670,Make airflow/example_dags Pylint compatible,Fix all Pylint messages in airflow/example_dags. To start; running scripts/ci/ci_pylint.sh on master should produce no messages. (1) Remove the files mentioned in your issue from the blacklist. (2) Run scripts/ci/ci_pylint.sh to see all messages on the no longer blacklisted files. (3) Fix all messages and create PR.,basph
AIRFLOW-4669,Make airflow/dag Pylint compatible,Fix all Pylint messages in airflow/dag. To start; running scripts/ci/ci_pylint.sh on master should produce no messages. (1) Remove the files mentioned in your issue from the blacklist. (2) Run scripts/ci/ci_pylint.sh to see all messages on the no longer blacklisted files. (3) Fix all messages and create PR.,basph
AIRFLOW-4668,Make airflow/contrib/utils Pylint compatible,Fix all Pylint messages in airflow/contrib/utils. To start; running scripts/ci/ci_pylint.sh on master should produce no messages. (1) Remove the files mentioned in your issue from the blacklist. (2) Run scripts/ci/ci_pylint.sh to see all messages on the no longer blacklisted files. (3) Fix all messages and create PR.,omkarbdesai
AIRFLOW-4667,Make airflow/contrib/task_runner Pylint compatible,Fix all Pylint messages in airflow/contrib/task_runner. To start; running scripts/ci/ci_pylint.sh on master should produce no messages. (1) Remove the files mentioned in your issue from the blacklist. (2) Run scripts/ci/ci_pylint.sh to see all messages on the no longer blacklisted files. (3) Fix all messages and create PR.,omkarbdesai
AIRFLOW-4665,Make airflow/contrib/plugins Pylint compatible,Fix all Pylint messages in airflow/contrib/plugins. To start; running scripts/ci/ci_pylint.sh on master should produce no messages. (1) Remove the files mentioned in your issue from the blacklist. (2) Run scripts/ci/ci_pylint.sh to see all messages on the no longer blacklisted files. (3) Fix all messages and create PR.,rinsableorc94
AIRFLOW-4659,Make airflow/api Pylint compatible,Fix all Pylint messages in airflow/api. To start; running scripts/ci/ci_pylint.sh on master should produce no messages. (1) Remove the files mentioned in your issue from the blacklist. (2) Run scripts/ci/ci_pylint.sh to see all messages on the no longer blacklisted files. (3) Fix all messages and create PR.,higrys
AIRFLOW-4658,Make tests/[a-zA-Z_]*.py Pylint compatible,Fix all Pylint messages in tests/[a-zA-Z_]*.py. To start; running scripts/ci/ci_pylint.sh on master should produce no messages. (1) Remove the files mentioned in your issue from the blacklist. (2) Run scripts/ci/ci_pylint.sh to see all messages on the no longer blacklisted files. (3) Fix all messages and create PR.,a_soldatenko
AIRFLOW-4657,Make airflow/[a-zA-Z_]*.py Pylint compatible,Fix all Pylint messages in airflow/[a-zA-Z_]*.py. To start; running scripts/ci/ci_pylint.sh on master should produce no messages. (1) Remove the files mentioned in your issue from the blacklist. (2) Run scripts/ci/ci_pylint.sh to see all messages on the no longer blacklisted files. (3) Fix all messages and create PR.,basph
AIRFLOW-4598,Task retries are not exhausted for K8s executor,,anand.g14
AIRFLOW-4596,Capture stats for dagrun schedule delay for adhoc/nonscheduled dag,"With respect to the metric we are capturing in airflow version v1-10 - [https://github.com/apache/airflow/blob/v1-10-stable/docs/metrics.rst]. The Timer merics `dagrun.schedule_delay.<dag_id>` `Seconds of delay between the scheduled DagRun start date and the actual DagRun start date`
 we are capturig the delay of only schduled Dags.

But we should also capture the delay for the adhoc/non-scheduled dags by capturing the difference between `start_date` in `dag_run` table and `start_date` in `task_instance` table keeping the `dag_id` and `excecution` date as the cardinality for performing join.",himagarg
AIRFLOW-4595,Need index on task_instance table on column start_date,Need index on *task_instance* table on column *start_date*,himagarg
AIRFLOW-4594,Kubernetes watcher updating Task state based on Pod state,"Currently KubernetesJobWatcher updates the task state based on the pod state inside 

process_status function. If it gets the pod state as failed it marks the task state to failed which is not the correct behaviour because task might move to retry state or reschedule state.

Pod might be failed because of various reasons like machine/network issue, it might crash. But its state might  not reflect the task state",ramandumcs
AIRFLOW-4592,add the translations,"Now the system can not choose language translation, now add translation",fengxuanmo
AIRFLOW-4591,Tag tasks with default pool,"Currently the number of running tasks without a pool specified will be limited by `non_pooled_task_slot_count`. It limits the number of tasks launched per scheduler loop but does not limit the number of tasks running in parallel.

This ticket proposes that we assign tasks without a pool specified to default pool which limits the number of running tasks in parallel.",milton0825
AIRFLOW-4590,winrm hook/operator - Suppress -WARNING - Failed to parse headers,"When using the winrm hook/operator, which uses pywinrm and subsequently urllib3, often this warning happens so often, it makes reading the task logs difficult. This card is to suppress warnings from urllib3 that cause this.

see: 

[https://github.com/urllib3/urllib3/issues/800]

[https://github.com/python/cpython/pull/12214]

 
{code:java}
WARNING - Failed to parse headers (url=http://tn019wvda005.docutap.local:5985/wsman): [StartBoundaryNotFoundDefect(), MultipartInvariantViolationDefect()], unparsed data: {code}
 ",lbodeen
AIRFLOW-4588,Add GoogleDiscoveryApiHook and GoogleApiToS3Transfer,"This PR adds a hook based on Google's Discovery API to communicate to any Google Services.
And also a implementation of this hook in an operator that transfers data from Google to S3.

- add documentation to integration.rst
The hook provides:
- a get_conn function to authenticate to the Google API via an airflow connection
- a query function to dynamically query all data available for a specific endpoint and given parameters. (You are able to either retrieve one page of data or all data)
The transfer operator provides:
- basic transfer between google api and s3
- passing an xcom variable to dynamically set the endpoint params for a request
- exposing the response data to xcom, but raises exception when it exceeds MAX_XCOM_SIZE
",feluelle
AIRFLOW-4586,Task getting stuck in Queued State,"We are observing intermittently that Tasks get stuck in queued state and never get executed by Airflow. On debugging it we found that one of the queued dependency was not met due to which task did not move from queued to running state. So task remained in queued state. (are_dependencies_met function returned false for QUEUE_DEPS inside _check_and_change_state_before_execution).

By looking into scheduler code it seems that scheduler does not reschedule the queued state tasks due to which task never got added to executor queue again and remained stuck in queued state. There is a logic inside _check_and_change_state_before_execution function to move the task from queued to None state(which gets picked by scheduler for rescheduling) if RUN_DEPS are not met but this logic seems to be missing for QUEUE_DEPS. It seems that task should be moved to None state even if QUEUE_DEPS are not met.",ramandumcs
AIRFLOW-4585,Implement pod mutation hook for KubernetesExecutor and KubernetesPodOperator,"This is a proposal to add a mutation hook similar to the {{policy}} function in {{airflow_local_settings}} to allow cluster administrators to mutate worker pods with more complex behavior than what is currently possible with the ini-based configuration.

This function could be called in {{PodLauncher.run_pod\{_async}}} and apply cluster-wide mutations to pods. This could for instance allow users to add complex sidecar or init containers to pods launched by their Airflow instance.",pgag
AIRFLOW-4584,Error when using ssh operateur to execute a sh script from an remote server,"hello guys;

i need yr help please, i'm new in apache airflow, and i'm trying to ssh operateur to execute a shell script from a remote server, my code looks like this:

 

t4 = SSHOperator(
    ssh_conn_id='test_ssh',
    task_id= 'Execute_transfert',
    command=""""""sh 'scripts/jwi/test.sh'"""""",
    dag=dag )

 

the only thing is inside my script (test.sh) i called a pentaho job (.kjb extention), the line command gives:

LOGFILE=""/xxx2/xxx3/logs/migxxx__`date ""+%Y-%m-%d-%H%M""`.log""
JOBFILE=""/xxx2/xxx3/xxx4/migxxx/avxxx.kjb""
PDI_LEVEL=Detailed

/folder1/folder2/kitchen.sh -file:$JOBFILE -level:$PDI_LEVEL -logfile:$LOGFILE

when running and afeter establishing connection to the remote server, the execution faild, a snapshot of the log:

{{[2019-05-27 20:02:02,651] \{logging_mixin.py:95} INFO - [2019-05-27 20:02:02,651] \{transport.py:1746} INFO - Connected (version 2.0, client OpenSSH_4.3) }}

{{[2019-05-27 20:02:05,877] \{logging_mixin.py:95} INFO - [2019-05-27 20:02:05,877] \{transport.py:1746} INFO - Authentication (publickey) failed. }}

{{[2019-05-27 20:02:05,897] \{logging_mixin.py:95} INFO - [2019-05-27 20:02:05,897] \{transport.py:1746} INFO - Authentication (password) successful! }}

{{[2019-05-27 20:02:06,640] \{ssh_operator.py:133} INFO - INFO 27-05 18:22:07,371 - Using ""/tmp/vfs_cache"" as temporary files store. }}

{{[2019-05-27 20:02:06,777] \{models.py:1788} ERROR - SSH operator error: 'utf8' codec can't decode byte 0xe9 in position 63: invalid continuation byte }}

{{Traceback (most recent call last): }}

{{File ""/usr/lib/python2.7/site-packages/airflow/models.py"", line 1657, in _run_raw_task }}

{{result = task_copy.execute(context=context)}}

{{ File ""/usr/lib/python2.7/site-packages/airflow/contrib/operators/ssh_operator.py"", line 167, in execute}}

{{ raise AirflowException(""SSH operator error: \{0}"".format(str(e)))}}

{{ AirflowException: SSH operator error: 'utf8' codec can't decode byte 0xe9 in position 63: invalid continuation byte }}

{{[2019-05-27 20:02:06,780] \{models.py:1817} INFO - All retries failed; marking task as FAILED [2019-05-27 20:02:06,795] \{base_task_runner.py:101} INFO - Job 1146: Subtask Execute_transfert Traceback (most recent call last): }}

{{[2019-05-27 20:02:06,796] \{base_task_runner.py:101} INFO - Job 1146: Subtask Execute_transfert File ""/usr/bin/airflow"", line 32, in <module> [2019-05-27 20:02:06,796] \{base_task_runner.py:101} INFO - Job 1146: Subtask Execute_transfert args.func(args) }}

{{[2019-05-27 20:02:06,796] \{base_task_runner.py:101} INFO - Job 1146: Subtask Execute_transfert File ""/usr/lib/python2.7/site-packages/airflow/utils/cli.py"", line 74, in wrapper [2019-05-27 20:02:06,797] \{base_task_runner.py:101} INFO - Job 1146: Subtask Execute_transfert return f(*args, **kwargs) }}

{{[2019-05-27 20:02:06,797] \{base_task_runner.py:101} INFO - Job 1146: Subtask Execute_transfert File ""/usr/lib/python2.7/site-packages/airflow/bin/cli.py"", line 526, in run [2019-05-27 20:02:06,798] \{base_task_runner.py:101} INFO - Job 1146: Subtask Execute_transfert _run(args, dag, ti) }}

{{[2019-05-27 20:02:06,798] \{base_task_runner.py:101} INFO - Job 1146: Subtask Execute_transfert File ""/usr/lib/python2.7/site-packages/airflow/bin/cli.py"", line 445, in _run [2019-05-27 20:02:06,798] \{base_task_runner.py:101} INFO - Job 1146: Subtask Execute_transfert pool=args.pool, }}

{{[2019-05-27 20:02:06,799] \{base_task_runner.py:101} INFO - Job 1146: Subtask Execute_transfert File ""/usr/lib/python2.7/site-packages/airflow/utils/db.py"", line 73, in wrapper [2019-05-27 20:02:06,799] \{base_task_runner.py:101} INFO - Job 1146: Subtask Execute_transfert return func(*args, **kwargs) }}

{{[2019-05-27 20:02:06,799] \{base_task_runner.py:101} INFO - Job 1146: Subtask Execute_transfert File ""/usr/lib/python2.7/site-packages/airflow/models.py"", line 1657, in _run_raw_task [2019-05-27 20:02:06,799] \{base_task_runner.py:101} INFO - Job 1146: Subtask Execute_transfert result = task_copy.execute(context=context) }}

{{[2019-05-27 20:02:06,800] \{base_task_runner.py:101} INFO - Job 1146: Subtask Execute_transfert File ""/usr/lib/python2.7/site-packages/airflow/contrib/operators/ssh_operator.py"", line 167, in execute [2019-05-27 20:02:06,800] \{base_task_runner.py:101} INFO - Job 1146: Subtask Execute_transfert raise AirflowException(""SSH operator error: \{0}"".format(str(e))) }}

{{[2019-05-27 20:02:06,800] \{base_task_runner.py:101} INFO - Job 1146: Subtask Execute_transfert airflow.exceptions.AirflowException: SSH operator error: 'utf8' codec can't decode byte 0xe9 in position 63: invalid continuation byte}}

please help how can resolve this issue.

 

Many thanks

 ",ashb
AIRFLOW-4582,"Set KubernetesExecutor worker pod limits and requests based on ""resources"" task attribute","At this time the {{KubernetesExecutor}} does not honor resource limits or requests set in the {{resources}} task attribute.

It is possible to set limits and requests by passing an {{executor_config}} dict, however this method seems sub-optimal, considering that a generic built-in task attribute with essentially the same purpose is already available.

I think we should implement this functionality, but gate it behind an opt-in configuration parameter to ensure backwards-compatibility.",pgag
AIRFLOW-4578,Set Container for Logs to Azure Blob Storage,"Currently it is possible to set the container that the logs to azure are written to when following the instructions.

However, there remain two issues:
 # The instructions do not mention the lines that need to be changed in the code to effect using a different container.
 # The instructions are completely unnecessary to use blob storage. 

I suggest an update that, when setting {color:#707070}remote_base_log_folder{color} in airflow.cfg, sets this as the Container in Azure. (The current setup forces the user to setup a container called airflow_logs and creates a folder in airflow_logs with the name set in {color:#707070}remote_base_log_folder{color}. This does not appear to be the likely behavior.


Secondly, I suggest updating the instructions such that they resemble the logging setup for both s3 and gcs. Setting three items in {color:#707070}airflow.cfg{color} allows logs to be sent to azure, but it currently requires a container labelled {color:#707070}airflow-logs{color} in the storage account. These three items are:


{color:#707070}remote_logging=True
{color}

{color:#707070}remote_base_log_folder=wasb-<choose a name here>{color}

{color:#a9b7c6}{color:#707070}remote_log_conn_id= <wasb-default>{color}
{color}

 ",dannylee12
AIRFLOW-4577,Azure Container Instances Operator: Volumes,"Hi There,

in my DAG File with an Azure Container Instances Operator i got an error after execution:

 
{code:java}
[2019-05-27 14:42:04,184] {{logging_mixin.py:95}} INFO - [2019-05-27 14:42:04,184] {{log.py:114}} INFO - ebf40d8d-89ca-4fda-8d21-116918844e82 - TokenRequest:Getting token with client credentials.
[2019-05-27 14:42:04,306] {{__init__.py:1580}} ERROR - __init__() takes 1 positional argument but 4 were given
Traceback (most recent call last):
  File ""/usr/local/lib/python3.6/site-packages/airflow/models/__init__.py"", line 1441, in _run_raw_task
    result = task_copy.execute(context=context)
  File ""/usr/local/lib/python3.6/site-packages/airflow/contrib/operators/azure_container_instances_operator.py"", line 157, in execute
    volume_mounts.append(VolumeMount(mount_name, mount_path, read_only))
TypeError: __init__() takes 1 positional argument but 4 were given
{code}
 

 

My DAG File looks like:

 
{code:java}
aci_run = AzureContainerInstancesOperator(
    task_id='aci_run',
    ci_conn_id='azure_container_instances_default',
    registry_conn_id=None,
    resource_group='man-ano',
    name='aci-test-{{ ds }}',
    image='alfpark/batch-shipyard:latest-cli',
    region='Westeurope',
    environment_variables={},
    volumes=[('wasb_default', 'modelv19141447271', 'airflow-azure-batch-shipyard', '/mnt/azfile/', False),],
    memory_in_gb=2.0,
    cpu=1.0,
    command=['ls', '/mnt/azfile/config-gpu'],
    # command=['/opt/batch-shipyard/shipyard.py', '--version'],
    dag=dag
)
{code}
 

 

I also fixed the Bug. I´ve seen that in file 'airflow/contrib/operators/azure_container_instances_operator.py' in line 157 something is missing:

Instead of:

 
{code:java}
volume_mounts.append(VolumeMount(mount_name, mount_path, read_only))
{code}
I think the line has to be following:
{code:java}
volume_mounts.append(VolumeMount(name=mount_name, mount_path=mount_path, read_only=read_only))
{code}
After i changed the line everything works as expected

 

BG

 ",trollgeir
AIRFLOW-4574,Add SSHHook private key parameter pkey,"The SSHHook only supports key_file parameter for specifying the path to a private key on disk. This means that private keys for connections that use ssh hooks must be stored on the disk of the worker instead of in the connection database. Maintaining the relationship between the worker's disk state and the connection makes deploying connection changes unnecessarily complicated.

Paramiko, which SSHHook is built on, has support for accepting private keys as an input parameter (pkey)

[https://github.com/paramiko/paramiko/blob/53095107625a1303bd9fcfcc7c2c20b9819ee79f/paramiko/client.py#L224]

The work involved in doing this should only be to add pkey as a parameter to the SSHHook constructor, and test that SSHConnection passes pkey to SSHHook which then passes it to Paramiko.",frediy
AIRFLOW-4573,airflow_local_settings is imported before sys.path is populated,"{{settings.py}} imports the contents of {{airflow_local_settings.py}} (which is expected to live under {{$AIRFLOW_HOME/config}}) before it is added to \{{sys.path}}. Thus, unless \{{airflow_local_settings.py}} is added manually to {{PYTHONPATH}} before execution, the module will not be found.

This is caused by the \{{airflow_local_settings}} module getting imported before {{prepare_classpath()}} is called, which is the function responsible for adding \{{DAGS_FOLDER}}, \{{PLUGINS_FOLDER}} and {{$AIRFLOW_HOME/config}} to {{sys.path}}.",pgag
AIRFLOW-4572,Rename prepare_classpath() to prepare_syspath() in settings.py,"{{settings.py}} currently defines a {{prepare_classpath()}} function that ensures that certain subfolders in {{$AIRFLOW_HOME}} are added to {{sys.path}}. The function's naming seems borrowed from Java, and thus could be improved by renaming it to {{prepare_syspath()}} which is the equivalent Python concept.

This appears to be an internal API that is only used in {{settings.py}} itself, so the change shouldn't have any impact.",pgag
AIRFLOW-4571,Add headers field to templated field for SimpleHttpOperator,"To let users use Jinja templates in headers, it is necessary to add it to template fields",kaxilnaik
AIRFLOW-4570,Removal all future library usage,The future library enabled compatibility between py2 and py3. This should be removed now that we are supporting python 3 only.,basph
AIRFLOW-4566,Missing documentation around sla and sla_miss_callback,The documentation/information around the `sla` and `sla_miss_callback` task parameters are limited and could use some explanation or examples.,billderose
AIRFLOW-4565,CeleryExecutor does not gauge executor stats,# of open slots + running/queued tasks was instrumented in [https://github.com/apache/airflow/pull/4928] but CeleryExecutor overwrites heartbeat method and misses the states,youngyjd
AIRFLOW-4564,Azure Container Instance bugfixes and improvements,"The Azure Container Instance Operator (and hook) have certain issues:
 * Azure polling errors before the container is running
 * Azure polling spam in log from TokenRequest every second
 * Environmental variables cannot be secret
 * No status if container fails provisioning
 * Max timer for container-run is currently 12 hours
 * Docs say 'command' parameter takes a string but really takes a list [AIRFLOW-4523|https://issues.apache.org/jira/browse/AIRFLOW-4523]
 * VolumeMount doesn't work properly [AIRFLOW-4577|https://issues.apache.org/jira/browse/AIRFLOW-4577]

Room for improvement:
 * Environmental variables should be able to use hooks and xcom to fetch values a execution time (less stress on DAG-loading)
 * Name of container has special requirements that should be checked upon Operator construction to avoid errors downstream.",trollgeir
AIRFLOW-4562,Wrong log filename in task failure notification,"Task failure email renders wrong log file name

 
{code:java}
Try 4 out of 4
Exception:
Jira operator error: <omitted>
Log: <omitted>
Host: <omitted>
Log file: /Users/airflow/logs/user_onboarding/real_jira_call/2019-05-23T07:39:20.877933+00:00.log
Mark success: <omitted>

{code}
in this example Log file is wrong and should be 
{code:java}
2019-05-23T07:39:20.877933+00:00/4.log{code}
 

 ",t
AIRFLOW-4557,The get_sqlproxy_runner() method of the CloudSqlDatabaseHook requires you to use the google_cloud_default_connection,"The get_sqlproxy_runner method of the CloudSqlDatabaseHook returns an instance of CloudSqlProxyRunner. This class allows you to specify a connection, but the method isn't passing in that parameter. Therefore the default value `google_cloud_default` always gets used for the CloudSqlProxyRunner. 

the desired action should be that the CloudSqlProxyRunner uses the connection specified in the CloudSqlDatabaseHook",andrewhharmon
AIRFLOW-4554,run_unit_tests.sh uses sudo unconditionally,"The run_unit_tests.sh script includes a sudo command:

 

[https://github.com/apache/airflow/blob/master/run_unit_tests.sh#L65]

However, one of the recommended methods for running unit tests locally is to create a docker container and that container lacks the sudo command. The script should only sudo if sudo exists.",dossett
AIRFLOW-4553,Scheduler only schedules one task at a time with CeleryExecutor,"I've recently upgraded from Airflow 1.9.0 to Airflow 1.10.3. There has been a change in how the scheduler put tasks up for execution that have affected some of our DAGs. We use the CeleryExecutor.

On Airflow 1.9.0 the scheduler would set as `scheduled` as many tasks as it could, For example in a DAG with 5 independent tasks and enough concurrency set, the scheduler would queue the 5 tasks for execution and all 5 tasks could potentially start running.

On Airflow 1.10.3 the scheduler would set as `scheduled` only one task at a time, For example in a DAG with 5 independent tasks and enough concurrency set, the scheduler would queue the 1 task for execution and the other 4 would not stay with a None state.",alrolorojas
AIRFLOW-4549,wait_for_downstream does not respect skipped tasks,See [http://mail-archives.apache.org/mod_mbox/airflow-dev/201609.mbox/%3CCAHEEp7UTGPjVKgww9_9N5FUpNU+PSKF3RmBvXUGK5dxb6bhjbw@mail.gmail.com%3E],dimakamalov
AIRFLOW-4546,Fix version conflict in gcp_api extras,"The `gcp_api` extras group pins `google-cloud-bigtable` to version 0.31.0, which pins `google-cloud-core<0.29dev,>=0.28.0`. This is incompatible with several other `gcp_api` requirements, which require `google-cloud-core>=1.0`. This discrepancy has been resolved in `google-cloud-bigtable` version 0.33.0. We should upgrade to fix.",jmcarp
AIRFLOW-4545,Github Enterprise OAuth auto registration fails due to bug in Flask-AppBuilder Lib.,"Registration of users via Github Enterprise fails due to a mishandling of the userinfo in the Security manager 
See [https://github.com/dpgaspar/Flask-AppBuilder/blob/v1.12.3/flask_appbuilder/security/manager.py#L841]

It's fixed with v.1.12.4 
See: 
[https://github.com/dpgaspar/Flask-AppBuilder/blob/v1.12.4/flask_appbuilder/security/manager.py#L841]

I'd like to upgrade this if at all possible. ",skyscanner.stewartw
AIRFLOW-4543,Update slack operator to support slackclient v2,"Official [Slack API for python|https://pypi.org/project/slackclient/] has recently released [v.2|https://github.com/slackapi/python-slackclient/wiki/Migrating-to-2.x0]

Among others some important points:
 * Async IO
 * SSL and Proxy
 * Dropping 2.7 support

Opening this ticket to work on the upgrade. Current functionalities will be migrated and will try to extend functionalities, if possible.",serkef
AIRFLOW-4541,Replace mkdirs usage with pathlib,_makedirs is used in 'airlfow.utils.file.mkdirs'  - it could be replaced with pathlib now with python3.5+_,basph
AIRFLOW-4537,Cleanup the mkdir_p function,"configuration.mkdir_p seems to come straight from [https://stackoverflow.com/a/600612.]

Now with Python 3.5, we have this functionality native. From the same post:

{{pathlib.Path(""/tmp/path/to/desired/directory"").mkdir(parents=True, exist_ok=True)}}

Remove the custom mkdir_p function and replace by pathlib. Will also resolve some imports to configuration, just for mkdir_p.",basph
AIRFLOW-4536,Split jobs module into package,,basph
AIRFLOW-4535,Breaks job.py into multiple files,,milton0825
AIRFLOW-4533,Give DAGs the ability to specify that they should be created paused/unpaused,"We have use cases with Dynamic DAGs where we want to create them unpaused, but we want most DAGs to be created paused as normal

Passing the pause/unpause state to the DAG constructor would be a good way to manage this. ",mpayton
AIRFLOW-4531,UI Mark success and Mark failed bug,"In 1.10.3 in the graph and tree pages, in the task on click menu, the mark success or mark failure in the GUI will now pick up the options set in Clear i.e. Past, Future, Downstream or Upstream.

Other users reporting the same issue in slack.

The work around is to set the options in the Clear settings

I believe the issue was introduced in https://issues.apache.org/jira/browse/AIRFLOW-4240. In the airflow/www/templates/dag.html file the method was changed to POST and unique variables were set rather than all the buttons passing the same variables in the get to different functions in [airflow/www/views.py|https://github.com/apache/airflow/commit/dbed51e702bf8177800183d2c4f595073aa2339d#diff-948e87b4f8f644b3ad8c7950958df033].




 ",jimbaldwin
AIRFLOW-4528,DataProcOperators do not stop underlying jobs on timeout,"When using DataProcOperators (like DataProcSparkOperator) with a specified `execution_timeout`, the DAG task is actually marked as failed and retry after the timeout, but the underlying job is not stopped.",alexisbrenon_ayl
AIRFLOW-4527,Connection error while calling refreshfromdb() makes the task stuck in running state,"{{I have setup airflow with mysql as metastore. When there is a network issue and task fails with a network connection reset exception, airflow tries to refresh status from db and gets a connection error - This results in task getting stuck in running.}}

{{There is no retry for mysql connection error and it never handles the exception}}

If worker nodes are unable to reach mysql to update task status, scheduler node should handle this scenario and mark those tasks failed. Tasks shouldn't be stuck in running state for ever.

 

 Scheduler heartbeat got an exception: (MySQLdb._exceptions.OperationalError) (2013, ""Lost connection to MySQL server at 'reading authorization packet', system error: 104"") (Background on this error at: [http://sqlalche.me/e/e3q8])

{base_task_runner.py:101}

INFO - Job 989226: Subtask count_cust_shipped_data Traceback (most recent call last):

{base_task_runner.py:101}

INFO - Job 989226: Subtask count_cust_shipped_data File ""/usr/local/bin/airflow"", line 32, in <module>

{base_task_runner.py:101}

INFO - Job 989226: Subtask count_cust_shipped_data args.func(args)

{base_task_runner.py:101}

INFO - Job 989226: Subtask count_cust_shipped_data File ""/usr/local/lib/python2.7/site-packages/airflow/utils/cli.py"", line 74, in wrapper

{base_task_runner.py:101}

INFO - Job 989226: Subtask count_cust_shipped_data return f(*args, **kwargs)

{base_task_runner.py:101}

INFO - Job 989226: Subtask count_cust_shipped_data File ""/usr/local/lib/python2.7/site-packages/airflow/bin/cli.py"", line 526, in run

{base_task_runner.py:101}

INFO - Job 989226: Subtask count_cust_shipped_data _run(args, dag, ti)

{base_task_runner.py:101}

INFO - Job 989226: Subtask count_cust_shipped_data File ""/usr/local/lib/python2.7/site-packages/airflow/bin/cli.py"", line 445, in _run

{base_task_runner.py:101}

INFO - Job 989226: Subtask count_cust_shipped_data pool=args.pool,

{base_task_runner.py:101}

INFO - Job 989226: Subtask count_cust_shipped_data File ""/usr/local/lib/python2.7/site-packages/airflow/utils/db.py"", line 73, in wrapper

{base_task_runner.py:101}

INFO - Job 989226: Subtask count_cust_shipped_data return func(*args, **kwargs)

{base_task_runner.py:101}

INFO - Job 989226: Subtask count_cust_shipped_data File ""/usr/local/lib/python2.7/site-packages/airflow/models.py"", line 1692, in _run_raw_task

{base_task_runner.py:101}

INFO - Job 989226: Subtask count_cust_shipped_data self.refresh_from_db()

{base_task_runner.py:101}

INFO - Job 989226: Subtask count_cust_shipped_data File ""/usr/local/lib/python2.7/site-packages/airflow/utils/db.py"", line 73, in wrapper

{base_task_runner.py:101}

INFO - Job 989226: Subtask count_cust_shipped_data return func(*args, **kwargs)

{base_task_runner.py:101}

INFO - Job 989226: Subtask count_cust_shipped_data File ""/usr/local/lib/python2.7/site-packages/airflow/models.py"", line 1218, in refresh_from_db

{base_task_runner.py:101}

INFO - Job 989226: Subtask count_cust_shipped_data ti = qry.first()

 ",bharathpalaksha
AIRFLOW-4524,"""Ignore All Deps"" and ""Ignore Task Deps"" doesn't ignore task dependencies","After an upgrade from Airflow 1.10.1->1.10.3, we're seeing this behavior when trying to ""Run"" a task in the UI with ""Ignore All Deps"" and ""Ignore Task Deps"":
{code:java}
""Could not queue task instance for execution, dependencies not met: Trigger Rule: Task's trigger rule 'all_success' requires all upstream tasks to have succeeded, but found 1 non-success(es). upstream_tasks_state={'successes': 1, 'skipped': 0, 'upstream_failed': 0, 'failed': 0, 'done': 1, 'total': 2}, upstream_task_ids={'XXX', 'XXX'}""
{code}
I suspected this was due to the checkbox not getting applied so I investigated this in Chrome. Here's a 1.10.3 POST (broken):
{code:java}
dag_id: MY_DAG_ID
task_id: MY_TASK_ID
execution_date: 2019-05-15T00:00:00+00:00
origin: MY_AIRFLOW_SERVER
ignore_all_deps: 
ignore_ti_state: 
ignore_task_deps: 
ignore_all_deps: true
ignore_task_deps: true
downstream: true
recursive: true
{code}
And here's a 1.10.1 POST (working):
{code:java}
task_id: MY_TASK_ID
dag_id: MY_DAG_ID
ignore_all_deps: true
ignore_task_deps: true
ignore_ti_state: false
execution_date: 2019-05-16T11:00:00+00:00
origin: MY_AIRFLOW_SERVER
{code}
It looks like the JS here is doing something wonky and sending blank values for unchecked boxes, as well as checkboxes that don't apply to the current action, which is presumably breaking parsing when the server receives a json object with multiple conflicting keys.",ashb
AIRFLOW-4523,Command Parameter in Azure Container Instances Operator,"Hi, i´ve got a Problem with Azure Container Instances Operator: In my DAG the 'command' Parameter will be ignored..

I´m using Airflow 1.10.3

Here my Operator in my DAG:


{code:java}
chunk_data = AzureContainerInstancesOperator(

    ci_conn_id='azure_container_instances_default',

    registry_conn_id='azure_registry',

    resource_group='man-ano',

    name='aci-echo',

    image='mancr.azurecr.io/echo-docker:latest',

    region='westeurope',

    environment_variables={},

    memory_in_gb=4.0,

    cpu=1.0,

    command='world',

    task_id='aci-echo',

    dag=dag
)
{code}
 

 

And here the Error:
{code:java}
[2019-05-16 07:26:57,805] {{azure_container_instances_operator.py:162}} INFO - Starting container group with 1.0 cpu 4.0 mem

[2019-05-16 07:26:57,806] {{azure_container_instances_operator.py:201}} INFO - Deleting container group

[2019-05-16 07:26:57,807] {{logging_mixin.py:95}} INFO - [2019-05-16 07:26:57,806] {{log.py:114}} INFO - 2e336521-8ee7-4f3f-af7f-cb7e380a06af - TokenRequest:Getting token with client credentials.

[2019-05-16 07:26:57,902] {{__init__.py:1580}} ERROR - Refuse str type as a valid iter type.

Traceback (most recent call last):

 File ""/usr/local/lib/python3.6/site-packages/airflow/models/__init__.py"", line 1441, in _run_raw_task

   result = task_copy.execute(context=context)

 File ""/usr/local/lib/python3.6/site-packages/airflow/contrib/operators/azure_container_instances_operator.py"", line 184, in execute

   ci_hook.create_or_update(self.resource_group, self.name, container_group)

 File ""/usr/local/lib/python3.6/site-packages/airflow/contrib/hooks/azure_container_instance_hook.py"", line 88, in create_or_update

   container_group)

 File ""/usr/local/lib/python3.6/site-packages/azure/mgmt/containerinstance/operations/container_groups_operations.py"", line 337, in create_or_update

   **operation_config

 File ""/usr/local/lib/python3.6/site-packages/azure/mgmt/containerinstance/operations/container_groups_operations.py"", line 280, in _create_or_update_initial

   body_content = self._serialize.body(container_group, 'ContainerGroup')

 File ""/usr/local/lib/python3.6/site-packages/msrest/serialization.py"", line 580, in body

   return self._serialize(data, data_type, **kwargs)

 File ""/usr/local/lib/python3.6/site-packages/msrest/serialization.py"", line 452, in _serialize

   target_obj, data_type, **kwargs)

 File ""/usr/local/lib/python3.6/site-packages/msrest/serialization.py"", line 715, in serialize_data

   return self._serialize(data, **kwargs)

 File ""/usr/local/lib/python3.6/site-packages/msrest/serialization.py"", line 489, in _serialize

   new_attr = self.serialize_data(orig_attr, attr_desc['type'], **kwargs)

 File ""/usr/local/lib/python3.6/site-packages/msrest/serialization.py"", line 708, in serialize_data

   data, data_type[1:-1], **kwargs)

 File ""/usr/local/lib/python3.6/site-packages/msrest/serialization.py"", line 783, in serialize_iter

   serialized.append(self.serialize_data(d, iter_type, **kwargs))

 File ""/usr/local/lib/python3.6/site-packages/msrest/serialization.py"", line 715, in serialize_data

   return self._serialize(data, **kwargs)

 File ""/usr/local/lib/python3.6/site-packages/msrest/serialization.py"", line 489, in _serialize

   new_attr = self.serialize_data(orig_attr, attr_desc['type'], **kwargs)

 File ""/usr/local/lib/python3.6/site-packages/msrest/serialization.py"", line 708, in serialize_data

   data, data_type[1:-1], **kwargs)

 File ""/usr/local/lib/python3.6/site-packages/msrest/serialization.py"", line 776, in serialize_iter

   raise SerializationError(""Refuse str type as a valid iter type."")

msrest.exceptions.SerializationError: Refuse str type as a valid iter type.

[2019-05-16 07:26:57,907] {{__init__.py:1611}} INFO - Marking task as FAILED.
{code}

Does anyone has an explanation for my problem ?
Thanks!",trollgeir
AIRFLOW-4521,Pause dag also pause subdags,Currently pause dag only pause the parent DAG but not pause the subdags.,milton0825
AIRFLOW-4515,KubernetesExecutor hangs in the subdag context,"The bug is that when using the kubernetes executor in a k8s cluster and running a subdag with `executor=KubernetesExecutor()` the subdag never returns. Just runs infinitely. Here is an example repo https://github.com/slocke716/test_subdags

This is occurring on 1.10.3 1.10.3post1 and 1.10.3b2 at least

Not passing `executor=KubernetesExecutor()` into the subdag operator is not an option since it defaults to the SequentialExecutor and changing to the LocalExecutor causes thread errors because we require quite a bit of concurrency",dimberman
AIRFLOW-4512,New trigger rule for tasks - At least one successful,"{{New trigger rule - *atleast_one_success*}}

{{Trigger rules help in defining better dependencies on up stream tasks. There are good number of rules currently defined which are as below}}
 * {{all_success}}: (default) all parents have succeeded

 * {{all_failed}}: all parents are in a {{failed}} or {{upstream_failed}} state

 * {{all_done}}: all parents are done with their execution

 * {{one_failed}}: fires as soon as at least one parent has failed, it does not wait for all parents to be done

 * {{one_success}}: fires as soon as at least one parent succeeds, it does not wait for all parents to be done

 * {{none_failed}}: all parents have not failed ({{failed}} or {{upstream_failed}}) i.e. all parents have succeeded or been skipped

 * {{none_skipped}}: no parent is in a {{skipped}} state, i.e. all parents are in a {{success}}, {{failed}}, or {{upstream_failed}} state

 * {{dummy}}: dependencies are just for show, trigger at will

 

There can be another rule added here which is *atleast_one_success* - This waits for all parent tasks to be complete and checks if at least one parent is successful and triggers current task. It differs from one_success as it waits for all parents to be done. 

Consider a very common scenario in data pipelines where you have a number of parallel tasks generating some data. As a downstream task to all these generate tasks, you have a task to collate all data into one collection which has to run if any of the upstream generate is successful and also has to wait for all of them to be done. one_success can't be used as it doesn't wait for other tasks.",bharathpalaksha
AIRFLOW-4511,Travis CI builds fail randomly - usually when pulling Docker images or pip install,"Travis experiences some randome build failures recently and they opened an incident for this:

[https://www.traviscistatus.com/incidents/pcqjnb3v1p57]

They suggested a temporary solution to check if that fixes our problem:

 

Here's the snippet we would like you to add to your .travis.yml file:

 
{code:java}
before_install:
  - echo ""options timeout:5"" | sudo tee -a /etc/resolvconf/resolv.conf.d/tail
  - echo ""options attempts:5"" | sudo tee -a /etc/resolvconf/resolv.conf.d/tail
  - sudo service resolvconf restart
{code}
 ",higrys
AIRFLOW-4510,Timezone set incorrectly if multiple DAGs defined in the same file,"If multiple DAGs are defined in the same file and they share the same default_args, then the subsequent DAGs have an incorrect timezone.

 

Steps to reproduce:

 

Set the default_timezone to be non-UTC in airflow.cfg

 
{noformat}
default_timezone = America/New_York{noformat}
 

DAG definition which has multiple DAGs in the same file:

 

 
{code:java}
from airflow import DAG
from airflow.operators.bash_operator import BashOperator
from datetime import datetime, timedelta

default_args = {
 'owner': 'airflow',
 'depends_on_past': False,
 'start_date': datetime(2019, 5, 11),
}
def make_dynamic_dag(schedule_interval, dag_name):

 dag = DAG(f""tutorial_{dag_name}"", default_args=default_args, schedule_interval=schedule_interval)
 t1 = BashOperator(task_id='print_date', bash_command='date', dag=dag)
 return dag
test_dag_1 = make_dynamic_dag(""00 15 * * *"", “1”)
test_dag_2 = make_dynamic_dag(""00 18 * * *"", “2”)
{code}
 

 

test_dag_1 is expected to run at 15:00 EST or 19:00 UTC and test_dag_2 is expected to run at 18:00 EST or 22:00 UTC.

 

However, test_dag_2 runs at 18:00 UTC which seems to point at it losing timezone information:

!Screen Shot 2019-05-13 at 2.41.54 PM.png!

 

I added some logging in the Airflow code around the default_args initialization and it confirmed the hypothesis that the default_args were being mutated:

 
{noformat}
[2019-05-13 18:40:10,409] {__init__.py:3045} INFO - default_args for DAG tutorial_1: {'owner': 'airflow', 'start_date': datetime.datetime(2019, 5, 11, 0, 0)}
[2019-05-13 18:40:10,410] {__init__.py:3045} INFO - default_args for DAG tutorial_2: {'owner': 'airflow', 'start_date': <Pendulum [2019-05-11T04:00:00+00:00]>}
{noformat}
 

 

As a simple fix, I changed the DAG definition to:
{noformat}
dag = DAG(f""tutorial_{dag_name}"", default_args=default_args, schedule_interval=schedule_interval){noformat}
and this seems to fix the problem:

 
{noformat}
[2019-05-13 18:44:44,674] {__init__.py:3045} INFO - default_args for DAG tutorial_1: {'owner': 'airflow', 'start_date': datetime.datetime(2019, 5, 11, 0, 0)}
[2019-05-13 18:44:44,676] {__init__.py:3045} INFO - default_args for DAG tutorial_2: {'owner': 'airflow', 'start_date': datetime.datetime(2019, 5, 11, 0, 0)}
{noformat}
 

!Screen Shot 2019-05-13 at 6.45.25 PM.png!

I want to add a fix to create a deep-copy of default_args here: [https://github.com/apache/airflow/blob/master/airflow/models/dag.py#L197]",abhishekray
AIRFLOW-4509,SubDagOperator using scheduler instead of backfill,Make SubDagOperator use Airflow scheduler instead of backfill to schedule tasks.,milton0825
AIRFLOW-4506,Make GIT_SYNC_DEPTH and GIT_SYNC_REV configurable,"Hey there,

I would love to see these two variables for *git-sync* init container configurable as currently this is limiting git-sync to only sync dag repository's branch tip with only the last commit. I am more than happy to implement it myself.

 

Thanks for consideration and responses!

Cheers",andrej.baran
AIRFLOW-4505,Setup.py correct tag ALL for PY3,"Currently I need (like probably a lot of people only PY3 packages) , and for that I need to use

 
{code:java}
apache-airflow[devel_ci]==1.10.3 {code}
 

But it do not explicit my needs , I would prefer

 
{code:java}
apache-airflow[all]==1.10.3 {code}
I propose to correct the tag ALL in a PY3 context",raphaelauv
AIRFLOW-4504,Remove unnecessary join in run_command(),,basph
AIRFLOW-4497,KubernetesExecutor fails to launch pods when using run_as_user or fs_group options,"If you set the run_as_user or the fs_group options in the airflow.cfg file when using the KubernetesExecutor the pods will fail to launch with the following error message:

 
{noformat}
[2019-05-10 05:36:09,207] {pod_launcher.py:60} ERROR - Exception when attempting to create Namespaced Pod.
Traceback (most recent call last):
File ""/usr/local/lib/python3.6/site-packages/airflow/contrib/kubernetes/pod_launcher.py"", line 57, in run_pod_async
resp = self._client.create_namespaced_pod(body=req, namespace=pod.namespace)
File ""/usr/local/lib/python3.6/site-packages/kubernetes/client/apis/core_v1_api.py"", line 6115, in create_namespaced_pod
(data) = self.create_namespaced_pod_with_http_info(namespace, body, **kwargs)
File ""/usr/local/lib/python3.6/site-packages/kubernetes/client/apis/core_v1_api.py"", line 6206, in create_namespaced_pod_with_http_info
collection_formats=collection_formats)
File ""/usr/local/lib/python3.6/site-packages/kubernetes/client/api_client.py"", line 334, in call_api
_return_http_data_only, collection_formats, _preload_content, _request_timeout)
File ""/usr/local/lib/python3.6/site-packages/kubernetes/client/api_client.py"", line 168, in __call_api
_request_timeout=_request_timeout)
File ""/usr/local/lib/python3.6/site-packages/kubernetes/client/api_client.py"", line 377, in request
body=body)
File ""/usr/local/lib/python3.6/site-packages/kubernetes/client/rest.py"", line 266, in POST
body=body)
File ""/usr/local/lib/python3.6/site-packages/kubernetes/client/rest.py"", line 222, in request
raise ApiException(http_resp=r)
kubernetes.client.rest.ApiException: (400)
Reason: Bad Request
HTTP response headers: HTTPHeaderDict({'Content-Type': 'application/json', 'Date': 'Fri, 10 May 2019 05:36:09 GMT', 'Content-Length': '428'})
HTTP response body: {""kind"":""Status"",""apiVersion"":""v1"",""metadata"":{},""status"":""Failure"",""message"":""Pod in version \""v1\"" cannot be handled as a Pod: v1.Pod.Spec: v1.PodSpec.SecurityContext: v1.PodSecurityContext.FSGroup: readUint64: unexpected character: \ufffd, error found in #10 byte of ...|sGroup\"": \""65533\""}}}|..., bigger context ...|, \""affinity\"": {}, \""securityContext\"": {\""fsGroup\"": \""65533\""}}}|..."",""reason"":""BadRequest"",""code"":400}{noformat}
 

It seems like those two options are getting put into the pod manifest as strings instead of integers?

 

My airflow.cfg options:

 
{noformat}
[kubernetes]
run_as_user = 1000
fs_group = 65533{noformat}
 ",pgag
AIRFLOW-4495,allow externally triggered dags to run for future 'Execution date',"1. 
 useful to handle future date for externally triggered batch process where ingesting 'forecast' data where filename date is in the future

2.
 this error is just in the scheduler log and not propagated up, so the dag stays in 'running' state forever (or for 1 year waiting for the time to pass :) )
 ERROR - Execution date is in future: 2020-01-01 00:00:00+00:00

 

 

fix below works if u only have externally triggered DAGs:

 

commenting below ti_deps\deps\runnable_exec_date_dep.py

        #if ti.execution_date > cur_date:

        #    yield self._failing_status(

        #        reason=""Execution date \{0} is in the future (the current ""

        #               ""date is \{1})."".format(ti.execution_date.isoformat(),

        #                                      cur_date.isoformat()))

 

commenting below jobs.py

            # don't consider runs that are executed in the future

            #if run.execution_date > timezone.utcnow():

            #    self.log.error(

            #        ""Execution date is in future: %s"",

            #        run.execution_date

            #    )

            #    continue

 ",toopt4
AIRFLOW-4492,Dataproc operator does not capture error from create cluster failure,"This line [1] captures the detail message as exception when cluster enters the ERROR state, however, Dataproc does not populate the detail field if cluster creation fails.

The correct behavior is to poll on Operation and report the error from the Operation, instead of polling on the cluster.

[[1] https://github.com/apache/airflow/blob/master/airflow/contrib/operators/dataproc_operator.py#L252|https://github.com/apache/airflow/blob/master/airflow/contrib/operators/dataproc_operator.py#L252]",dansedov
AIRFLOW-4490,dag_run.conf should be an empty dictionary by default,"Currently, if no conf is provided when running a dag, the `dag_run.conf` instance is None. 

When a DAG might not always be triggered with a configuration, this leads to needing to handle an ""empty"" configuration like this :

{code}
{{ if dag_run.conf and dag_run.conf.get('value') and ... }}
{code}

If dag_run.conf was an empty dictionary by default, this could simply become:

{code}
{{ if dag_run.conf.get('key') and ....}}
{code}

 ",yampelo
AIRFLOW-4487,Move k8s executor from contrib to executors,"Considering that the k8s executor is now supported by core
committers, we should move it from contrib to the primary executor
directory.",dimberman
AIRFLOW-4486,Enhance MySQL hook to allow for IAM authentication,"Enhance to current MySqlHook to allow for connecting to AWS RDS AWS IAM authentication.

The hook will use an AWSHook to generate a temporary token via boto3. This token will be passed in as the password.

To specify IAM authentication, the following extras can be added to the MySql connection

{ ""iam"":true, ""region"":""us-west-1"", ""aws_conn_id"":""my_aws_conn"" }",andrewhharmon
AIRFLOW-4482,Add execution_date to trigger DAG run API response,"We are using experimental REST API for automating Airflow from Jenkins and our workflow looks like this:
 * Jenkins job triggers DAG run (using _POST /api/experimental/dags/*<dag_id>*/dag_runs_)
 * Airflow API returns response like this: 

{code:java}
 {""message"":""Created <DagRun dag_id @ 2019-05-08 11:42:43+00:00: run_id, externally triggered: True>""}  {code}
 * Jenkins job parses the response for _*execution_date*_ (2019-05-08T11:42:43 for response above)
 * Jenkins job further uses execution_date to check DAG run state using _POST /api/experimental/dags/*<dag_id>*/dag_runs/*<execution_date>*_
 * Also Jenkins job generates DAG run UI link

Here is [the code|https://github.com/doublescoring/jenkins-pipeline-goodness/blob/master/src/main/groovy/airflow.groovy].

Actually it is not a good idea to parse message string for execution_date. So it is proposed to add _*execution_date*_ in DAG run trigger API response as a field. It makes it possible to get execution_date directly from the response without dirty and unpredictable parsing. After this improvement API response will be like this:

 
{code:java}
{'execution_date': '2019-05-08T07:03:09+00:00', 'message': 'Created <DagRun dag_id @ 2019-05-08 07:03:09+00:00: manual__2019-05-08T07:03:09+00:00, externally triggered: True>'}
{code}
 

 ",pshuvalov
AIRFLOW-4480,dagrun_timeout won't trigger callbacks,"AIRFLOW-511 introduced on_success_callback and on_failure_callback callbacks and passed test_dagrun_success_callback and test_dagrun_failure_callback tests. This is great. But for dagrun_timeout, airflow hasn't covered the case. Although there is one line to handle_callback when dag exceeded dagrun_timeout, actually it won't ever trigger any callbacks. The reason is the dag [here|https://github.com/apache/airflow/blob/1.10.3/airflow/jobs.py#L802] has no on_success_callback and on_failure_callback which are both None. These DAGs are from [DagFileProcessor|https://github.com/apache/airflow/blob/1.10.3/airflow/jobs.py#L307] which parsed python files to get DAG ids and then get DAGs with None callbacks.

I've checked airflow database, seems it doesn't store callbacks in the database. Are there any approaches that we can get DAGs with callbacks along with other DAG settings in [SchedulerJob|https://github.com/apache/airflow/blob/1.10.3/airflow/jobs.py#L399]? Or in brief how to trigger callbacks when dagrun_timeout? Thanks.",ahaidrey
AIRFLOW-4478,Operators instantiate many duplicate objects,"`BaseOperator` creates a `Resources` instance, which in turn creates four `Resource` instances. Class creation in python isn't free; creating `Resources` and its child classes takes ~5μs out of a total of ~20μs to instantiate a `BaseOperator` on my system. This time adds up when creating tens of thousands of operators, especially in environments like GCP Cloud Composer that are very sensitive to DAG parse time.

Assuming that most users don't actually configure task resources, since they're only respected by the non-default `CgroupTaskRunner`, we can save time by creating a single `Resources` instance and sharing it across tasks that don't set `resources`. We could do even better by allowing users to pass a `Resources` instance to `BaseOperator` rather than passing a `dict` that's used to instantiate `Resources`, but that would be a breaking change.",fifarzh
AIRFLOW-4477,Add v2.models to BigQueryHook,"Add v2.models to BigQueryHook

Reference: [https://cloud.google.com/bigquery/docs/reference/rest/v2/models]",ryan.yuan
AIRFLOW-4475,Use type-hinting within the k8s executor,"Now that we only support python 3.6+, we should take advantage of type hints to ensure a cleaner and easier to use codebase",dimberman
AIRFLOW-4474,Split K8s executor into multiple files,"As a first step in refactoring the k8s executor for easier development, let's seperate logically isolated components",dimberman
AIRFLOW-4471,Dataproc operator templated fields improvements,"This ticket is about:
 * Fixing bugs in documentation.
 * Adding 'dataproc_jars' as templated field for missing DataProcXXXXXOperators
 * Renaming 'dataproc_xxxx_jars' to dataproc_jars to remove duplication.",martijnvdgrift
AIRFLOW-4468,Add max_overflow parameter for configuration of SQLAlchemy connection pool,"I reckon it would be great if we can set max_overflow. Sometimes, this parameter is actually necessary and useful to make a number of connections more predictable. ",izebit
AIRFLOW-4465,ssh_operator: pass remote_host via ssh_hook constructor ,"The current ssh_operator code constructs an SSHHook and then updates it.

Change to pass in via the constructor and only update if SSHHook is (pre-)defined in the ssh_operator instance.",eepstein
AIRFLOW-4464,Fix case-insensitive id columns in mysql,"By default, string comparisons in mysql are case-insensitive, so the task ids ""foo"" and ""FOO"" are treated as identical. This means that a dag with those task ids will fail to schedule with a sqlalchemy `IntegrityError` using mysql, but not postgres or sqlite. This situation probably doesn't happen often, and users probably shouldn't use task ids that are identical except for case, but I think we should improve the behavior here. A few options:

 
 * Configure sqlalchemy to use a binary collation for string id columns under mysql so that string comparisons are case-sensitive.
 * Require dag and task ids to be unique regardless of case. This would be a breaking change.
 * Document that mysql users should configure mysql to use binary collations for string types by default. This would still show users a 500 if the database isn't configured correctly.

 

I'll submit a pull request with a failing unit test to describe the issue.",jmcarp
AIRFLOW-4463,Short DAG retries lead to a divide by zero-error,"A short retry interval leads to a divide by zero error. To replicate, create a DAG with a short retry interval (1 second), and eventually the following error will appear:

```

Traceback (most recent call last):

...
 File ""/app/airflow/models/taskinstance.py"", line 630, in next_retry_datetime
 modded_hash = min_backoff + hash % min_backoff
ZeroDivisionError: integer division or modulo by zero

```",chris7
AIRFLOW-4462,MSSQL backend broken,"Airflow dag trigger doesn't work mssql azure, mssql 2017 (tested), Other version of mssql must have the same issue. Basically, airflow can't be used on mssql without fixing this issue. Just click a number of manual triggers and it will fail.

Dag trigger woks only when the execution_date is missing the milliseconds part.  when the millisecond part is non-zero upto the last digit, it fails to trigger. 

The problem is execution_date input having microsecond from pyodbc where it compares equality on task_instance table. Since execution_date is up to millisecond precision  in db(sql server also rounds this value), the equality fails on certain values, e.g, 2019-04-28 00:34:26.517, 2019-04-20T18:51:35.033

Stack trace below:

Connected to pydev debugger (build 191.6605.12)
[2019-04-23 21:49:13,361] \{settings.py:182} INFO - settings.configure_orm(): Using pool settings. pool_size=5, pool_recycle=1800, pid=78750
[2019-04-23 21:49:13,523] \{__init__.py:51} INFO - Using executor SequentialExecutor
2019-04-23 21:49:17,318 INFO sqlalchemy.engine.base.Engine SELECT CAST(SERVERPROPERTY('ProductVersion') AS VARCHAR)
[2019-04-23 21:49:17,318] \{log.py:110} INFO - SELECT CAST(SERVERPROPERTY('ProductVersion') AS VARCHAR)
2019-04-23 21:49:17,319 INFO sqlalchemy.engine.base.Engine ()
[2019-04-23 21:49:17,319] \{log.py:110} INFO - ()
2019-04-23 21:49:17,324 INFO sqlalchemy.engine.base.Engine SELECT schema_name()
[2019-04-23 21:49:17,324] \{log.py:110} INFO - SELECT schema_name()
2019-04-23 21:49:17,324 INFO sqlalchemy.engine.base.Engine ()
[2019-04-23 21:49:17,324] \{log.py:110} INFO - ()
2019-04-23 21:49:17,329 INFO sqlalchemy.engine.base.Engine SELECT CAST('test plain returns' AS VARCHAR(60)) AS anon_1
[2019-04-23 21:49:17,329] \{log.py:110} INFO - SELECT CAST('test plain returns' AS VARCHAR(60)) AS anon_1
2019-04-23 21:49:17,329 INFO sqlalchemy.engine.base.Engine ()
[2019-04-23 21:49:17,329] \{log.py:110} INFO - ()
2019-04-23 21:49:17,332 INFO sqlalchemy.engine.base.Engine SELECT CAST('test unicode returns' AS NVARCHAR(60)) AS anon_1
[2019-04-23 21:49:17,332] \{log.py:110} INFO - SELECT CAST('test unicode returns' AS NVARCHAR(60)) AS anon_1
2019-04-23 21:49:17,333 INFO sqlalchemy.engine.base.Engine ()
[2019-04-23 21:49:17,333] \{log.py:110} INFO - ()
2019-04-23 21:49:17,337 INFO sqlalchemy.engine.base.Engine SELECT 1
[2019-04-23 21:49:17,337] \{log.py:110} INFO - SELECT 1
2019-04-23 21:49:17,337 INFO sqlalchemy.engine.base.Engine ()
[2019-04-23 21:49:17,337] \{log.py:110} INFO - ()
2019-04-23 21:49:17,340 INFO sqlalchemy.engine.base.Engine BEGIN (implicit)
[2019-04-23 21:49:17,340] \{log.py:110} INFO - BEGIN (implicit)
2019-04-23 21:49:17,344 INFO sqlalchemy.engine.base.Engine INSERT INTO log (dttm, dag_id, task_id, event, execution_date, owner, extra) OUTPUT inserted.id VALUES (?, ?, ?, ?, ?, ?, ?)
[2019-04-23 21:49:17,344] \{log.py:110} INFO - INSERT INTO log (dttm, dag_id, task_id, event, execution_date, owner, extra) OUTPUT inserted.id VALUES (?, ?, ?, ?, ?, ?, ?)
2019-04-23 21:49:17,344 INFO sqlalchemy.engine.base.Engine (datetime.datetime(2019, 4, 24, 4, 49, 17, 42277, tzinfo=<Timezone [UTC]>), 'tutorial', 'print_date', 'cli_run', <Pendulum [2019-04-20T18:51:35.033000+00:00]>, 'admin', '\{""host_name"": ""Bijays-MBP.hsd1.ca.comcast.net"", ""full_command"": ""[\'/Users/admin/Documents/Development/Java/airflow/airflow/bin/airflow\', \'run\', \ ... (18 characters truncated) ... dmin/Documents/Development/Java/airflow/airflow/example_dags/tutorial.py\', \'--local\', \'tutorial\', \'print_date\', \'2019-04-20 18:51:35.033\']""}')
[2019-04-23 21:49:17,344] \{log.py:110} INFO - (datetime.datetime(2019, 4, 24, 4, 49, 17, 42277, tzinfo=<Timezone [UTC]>), 'tutorial', 'print_date', 'cli_run', <Pendulum [2019-04-20T18:51:35.033000+00:00]>, 'admin', '\{""host_name"": ""Bijays-MBP.hsd1.ca.comcast.net"", ""full_command"": ""[\'/Users/admin/Documents/Development/Java/airflow/airflow/bin/airflow\', \'run\', \ ... (18 characters truncated) ... dmin/Documents/Development/Java/airflow/airflow/example_dags/tutorial.py\', \'--local\', \'tutorial\', \'print_date\', \'2019-04-20 18:51:35.033\']""}')
2019-04-23 21:49:17,351 INFO sqlalchemy.engine.base.Engine COMMIT
[2019-04-23 21:49:17,351] \{log.py:110} INFO - COMMIT
[2019-04-23 21:49:17,366] \{__init__.py:305} INFO - Filling up the DagBag from /Users/admin/Documents/Development/Java/airflow/airflow/example_dags/tutorial.py
2019-04-23 21:49:17,508 INFO sqlalchemy.engine.base.Engine SELECT CAST(SERVERPROPERTY('ProductVersion') AS VARCHAR)
[2019-04-23 21:49:17,508] \{log.py:110} INFO - SELECT CAST(SERVERPROPERTY('ProductVersion') AS VARCHAR)
2019-04-23 21:49:17,508 INFO sqlalchemy.engine.base.Engine ()
[2019-04-23 21:49:17,508] \{log.py:110} INFO - ()
2019-04-23 21:49:17,510 INFO sqlalchemy.engine.base.Engine SELECT schema_name()
[2019-04-23 21:49:17,510] \{log.py:110} INFO - SELECT schema_name()
2019-04-23 21:49:17,511 INFO sqlalchemy.engine.base.Engine ()
[2019-04-23 21:49:17,511] \{log.py:110} INFO - ()
2019-04-23 21:49:17,515 INFO sqlalchemy.engine.base.Engine SELECT CAST('test plain returns' AS VARCHAR(60)) AS anon_1
[2019-04-23 21:49:17,515] \{log.py:110} INFO - SELECT CAST('test plain returns' AS VARCHAR(60)) AS anon_1
2019-04-23 21:49:17,515 INFO sqlalchemy.engine.base.Engine ()
[2019-04-23 21:49:17,515] \{log.py:110} INFO - ()
2019-04-23 21:49:17,518 INFO sqlalchemy.engine.base.Engine SELECT CAST('test unicode returns' AS NVARCHAR(60)) AS anon_1
[2019-04-23 21:49:17,518] \{log.py:110} INFO - SELECT CAST('test unicode returns' AS NVARCHAR(60)) AS anon_1
2019-04-23 21:49:17,518 INFO sqlalchemy.engine.base.Engine ()
[2019-04-23 21:49:17,518] \{log.py:110} INFO - ()
2019-04-23 21:49:17,523 INFO sqlalchemy.engine.base.Engine SELECT 1
[2019-04-23 21:49:17,523] \{log.py:110} INFO - SELECT 1
2019-04-23 21:49:17,523 INFO sqlalchemy.engine.base.Engine ()
[2019-04-23 21:49:17,523] \{log.py:110} INFO - ()
2019-04-23 21:49:17,526 INFO sqlalchemy.engine.base.Engine BEGIN (implicit)
[2019-04-23 21:49:17,526] \{log.py:110} INFO - BEGIN (implicit)
2019-04-23 21:49:17,534 INFO sqlalchemy.engine.base.Engine SELECT TOP 1 task_instance.try_number AS task_instance_try_number, task_instance.task_id AS task_instance_task_id, task_instance.dag_id AS task_instance_dag_id, task_instance.execution_date AS task_instance_execution_date, task_instance.start_date AS task_instance_start_date, task_instance.end_date AS task_instance_end_date, task_instance.duration AS task_instance_duration, task_instance.state AS task_instance_state, task_instance.max_tries AS task_instance_max_tries, task_instance.hostname AS task_instance_hostname, task_instance.unixname AS task_instance_unixname, task_instance.job_id AS task_instance_job_id, task_instance.pool AS task_instance_pool, task_instance.queue AS task_instance_queue, task_instance.priority_weight AS task_instance_priority_weight, task_instance.operator AS task_instance_operator, task_instance.queued_dttm AS task_instance_queued_dttm, task_instance.pid AS task_instance_pid, task_instance.executor_config AS task_instance_executor_config 
FROM task_instance 
WHERE task_instance.dag_id = ? AND task_instance.task_id = ? AND task_instance.execution_date = ?
[2019-04-23 21:49:17,534] \{log.py:110} INFO - SELECT TOP 1 task_instance.try_number AS task_instance_try_number, task_instance.task_id AS task_instance_task_id, task_instance.dag_id AS task_instance_dag_id, task_instance.execution_date AS task_instance_execution_date, task_instance.start_date AS task_instance_start_date, task_instance.end_date AS task_instance_end_date, task_instance.duration AS task_instance_duration, task_instance.state AS task_instance_state, task_instance.max_tries AS task_instance_max_tries, task_instance.hostname AS task_instance_hostname, task_instance.unixname AS task_instance_unixname, task_instance.job_id AS task_instance_job_id, task_instance.pool AS task_instance_pool, task_instance.queue AS task_instance_queue, task_instance.priority_weight AS task_instance_priority_weight, task_instance.operator AS task_instance_operator, task_instance.queued_dttm AS task_instance_queued_dttm, task_instance.pid AS task_instance_pid, task_instance.executor_config AS task_instance_executor_config 
FROM task_instance 
WHERE task_instance.dag_id = ? AND task_instance.task_id = ? AND task_instance.execution_date = ?
2019-04-23 21:49:17,536 INFO sqlalchemy.engine.base.Engine ('tutorial', 'print_date', <Pendulum [2019-04-20T18:51:35.033000+00:00]>)
[2019-04-23 21:49:17,536] \{log.py:110} INFO - ('tutorial', 'print_date', <Pendulum [2019-04-20T18:51:35.033000+00:00]>)
2019-04-23 21:49:17,547 INFO sqlalchemy.engine.base.Engine COMMIT
[2019-04-23 21:49:17,547] \{log.py:110} INFO - COMMIT
2019-04-23 21:49:17,681 INFO sqlalchemy.engine.base.Engine SELECT 1
[2019-04-23 21:49:17,681] \{log.py:110} INFO - SELECT 1
2019-04-23 21:49:17,681 INFO sqlalchemy.engine.base.Engine ()
[2019-04-23 21:49:17,681] \{log.py:110} INFO - ()
2019-04-23 21:49:17,684 INFO sqlalchemy.engine.base.Engine BEGIN (implicit)
[2019-04-23 21:49:17,684] \{log.py:110} INFO - BEGIN (implicit)
2019-04-23 21:49:17,686 INFO sqlalchemy.engine.base.Engine SELECT TOP 1 dag_run.state AS dag_run_state, dag_run.id AS dag_run_id, dag_run.dag_id AS dag_run_dag_id, dag_run.execution_date AS dag_run_execution_date, dag_run.start_date AS dag_run_start_date, dag_run.end_date AS dag_run_end_date, dag_run.run_id AS dag_run_run_id, dag_run.external_trigger AS dag_run_external_trigger, dag_run.conf AS dag_run_conf 
FROM dag_run 
WHERE dag_run.dag_id = ? AND dag_run.execution_date = ?
[2019-04-23 21:49:17,686] \{log.py:110} INFO - SELECT TOP 1 dag_run.state AS dag_run_state, dag_run.id AS dag_run_id, dag_run.dag_id AS dag_run_dag_id, dag_run.execution_date AS dag_run_execution_date, dag_run.start_date AS dag_run_start_date, dag_run.end_date AS dag_run_end_date, dag_run.run_id AS dag_run_run_id, dag_run.external_trigger AS dag_run_external_trigger, dag_run.conf AS dag_run_conf 
FROM dag_run 
WHERE dag_run.dag_id = ? AND dag_run.execution_date = ?
2019-04-23 21:49:17,686 INFO sqlalchemy.engine.base.Engine ('tutorial', <Pendulum [2019-04-20T18:51:35.033000+00:00]>)
[2019-04-23 21:49:17,686] \{log.py:110} INFO - ('tutorial', <Pendulum [2019-04-20T18:51:35.033000+00:00]>)
2019-04-23 21:49:17,691 INFO sqlalchemy.engine.base.Engine COMMIT
[2019-04-23 21:49:17,691] \{log.py:110} INFO - COMMIT
[2019-04-23 21:49:17,701] \{cli.py:517} INFO - Running <TaskInstance: tutorial.print_date 2019-04-20T18:51:35.033000+00:00 [None]> on host bijays-mbp.hsd1.ca.comcast.net
2019-04-23 21:49:36,110 INFO sqlalchemy.engine.base.Engine SELECT 1
2019-04-23 21:49:36,112 INFO sqlalchemy.engine.base.Engine ()
2019-04-23 21:49:36,119 INFO sqlalchemy.engine.base.Engine BEGIN (implicit)
2019-04-23 21:49:36,127 INFO sqlalchemy.engine.base.Engine INSERT INTO job (dag_id, state, job_type, start_date, end_date, latest_heartbeat, executor_class, hostname, unixname) OUTPUT inserted.id VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
2019-04-23 21:49:36,127 INFO sqlalchemy.engine.base.Engine ('tutorial', 'running', 'LocalTaskJob', datetime.datetime(2019, 4, 24, 4, 49, 32, 942901, tzinfo=<Timezone [UTC]>), None, datetime.datetime(2019, 4, 24, 4, 49, 32, 943012, tzinfo=<Timezone [UTC]>), 'SequentialExecutor', 'bijays-mbp.hsd1.ca.comcast.net', 'admin')
2019-04-23 21:49:36,142 INFO sqlalchemy.engine.base.Engine COMMIT
2019-04-23 21:49:59,069 INFO sqlalchemy.engine.base.Engine SELECT 1
2019-04-23 21:49:59,070 INFO sqlalchemy.engine.base.Engine ()
2019-04-23 21:49:59,072 INFO sqlalchemy.engine.base.Engine BEGIN (implicit)
2019-04-23 21:49:59,074 INFO sqlalchemy.engine.base.Engine SELECT TOP 1 dag_run.state AS dag_run_state, dag_run.id AS dag_run_id, dag_run.dag_id AS dag_run_dag_id, dag_run.execution_date AS dag_run_execution_date, dag_run.start_date AS dag_run_start_date, dag_run.end_date AS dag_run_end_date, dag_run.run_id AS dag_run_run_id, dag_run.external_trigger AS dag_run_external_trigger, dag_run.conf AS dag_run_conf 
FROM dag_run 
WHERE dag_run.dag_id = ? AND dag_run.execution_date = ?
2019-04-23 21:49:59,074 INFO sqlalchemy.engine.base.Engine ('tutorial', <Pendulum [2019-04-20T18:51:35.033000+00:00]>)
2019-04-23 21:49:59,078 INFO sqlalchemy.engine.base.Engine COMMIT
2019-04-23 21:50:41,906 INFO sqlalchemy.engine.base.Engine SELECT 1
2019-04-23 21:50:41,908 INFO sqlalchemy.engine.base.Engine ()
2019-04-23 21:50:41,910 INFO sqlalchemy.engine.base.Engine BEGIN (implicit)
2019-04-23 21:50:41,912 INFO sqlalchemy.engine.base.Engine SELECT TOP 1 task_instance.try_number AS task_instance_try_number, task_instance.task_id AS task_instance_task_id, task_instance.dag_id AS task_instance_dag_id, task_instance.execution_date AS task_instance_execution_date, task_instance.start_date AS task_instance_start_date, task_instance.end_date AS task_instance_end_date, task_instance.duration AS task_instance_duration, task_instance.state AS task_instance_state, task_instance.max_tries AS task_instance_max_tries, task_instance.hostname AS task_instance_hostname, task_instance.unixname AS task_instance_unixname, task_instance.job_id AS task_instance_job_id, task_instance.pool AS task_instance_pool, task_instance.queue AS task_instance_queue, task_instance.priority_weight AS task_instance_priority_weight, task_instance.operator AS task_instance_operator, task_instance.queued_dttm AS task_instance_queued_dttm, task_instance.pid AS task_instance_pid, task_instance.executor_config AS task_instance_executor_config 
FROM task_instance 
WHERE task_instance.dag_id = ? AND task_instance.task_id = ? AND task_instance.execution_date = ?
2019-04-23 21:50:41,913 INFO sqlalchemy.engine.base.Engine ('tutorial', 'print_date', <Pendulum [2019-04-20T18:51:35.033000+00:00]>)
2019-04-23 21:50:41,930 INFO sqlalchemy.engine.base.Engine SELECT task_reschedule.id AS task_reschedule_id, task_reschedule.task_id AS task_reschedule_task_id, task_reschedule.dag_id AS task_reschedule_dag_id, task_reschedule.execution_date AS task_reschedule_execution_date, task_reschedule.try_number AS task_reschedule_try_number, task_reschedule.start_date AS task_reschedule_start_date, task_reschedule.end_date AS task_reschedule_end_date, task_reschedule.duration AS task_reschedule_duration, task_reschedule.reschedule_date AS task_reschedule_reschedule_date 
FROM task_reschedule 
WHERE task_reschedule.dag_id = ? AND task_reschedule.task_id = ? AND task_reschedule.execution_date = ? AND task_reschedule.try_number = ? ORDER BY task_reschedule.id ASC
2019-04-23 21:50:41,930 INFO sqlalchemy.engine.base.Engine ('tutorial', 'print_date', <Pendulum [2019-04-20T18:51:35.033000+00:00]>, 1)
2019-04-23 21:50:41,936 INFO sqlalchemy.engine.base.Engine SELECT count(task_instance.task_id) AS count_1 
FROM task_instance 
WHERE task_instance.dag_id = ? AND task_instance.state = ?
2019-04-23 21:50:41,937 INFO sqlalchemy.engine.base.Engine ('tutorial', 'running')
2019-04-23 21:50:41,941 INFO sqlalchemy.engine.base.Engine COMMIT
2019-04-23 21:53:03,611 INFO sqlalchemy.engine.base.Engine SELECT 1
2019-04-23 21:53:03,611 INFO sqlalchemy.engine.base.Engine ()
2019-04-23 21:53:03,614 INFO sqlalchemy.engine.base.Engine BEGIN (implicit)
2019-04-23 21:53:03,616 INFO sqlalchemy.engine.base.Engine SELECT task_instance.try_number AS task_instance_try_number, task_instance.task_id AS task_instance_task_id, task_instance.dag_id AS task_instance_dag_id, task_instance.execution_date AS task_instance_execution_date, task_instance.start_date AS task_instance_start_date, task_instance.end_date AS task_instance_end_date, task_instance.duration AS task_instance_duration, task_instance.state AS task_instance_state, task_instance.max_tries AS task_instance_max_tries, task_instance.hostname AS task_instance_hostname, task_instance.unixname AS task_instance_unixname, task_instance.job_id AS task_instance_job_id, task_instance.pool AS task_instance_pool, task_instance.queue AS task_instance_queue, task_instance.priority_weight AS task_instance_priority_weight, task_instance.operator AS task_instance_operator, task_instance.queued_dttm AS task_instance_queued_dttm, task_instance.pid AS task_instance_pid, task_instance.executor_config AS task_instance_executor_config 
FROM task_instance 
WHERE task_instance.task_id = ? AND task_instance.dag_id = ? AND task_instance.execution_date = ?
2019-04-23 21:53:03,616 INFO sqlalchemy.engine.base.Engine ('print_date', 'tutorial', <Pendulum [2019-04-20T18:51:35.033000+00:00]>)
2019-04-23 21:53:50,370 INFO sqlalchemy.engine.base.Engine INSERT INTO task_instance (task_id, dag_id, execution_date, start_date, end_date, duration, state, try_number, max_tries, hostname, unixname, job_id, pool, queue, priority_weight, operator, queued_dttm, pid, executor_config) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
2019-04-23 21:53:50,372 INFO sqlalchemy.engine.base.Engine ('print_date', 'tutorial', <Pendulum [2019-04-20T18:51:35.033000+00:00]>, datetime.datetime(2019, 4, 24, 4, 50, 41, 924763, tzinfo=<Timezone [UTC]>), None, None, 'running', 1, 1, 'bijays-mbp.hsd1.ca.comcast.net', 'admin', 2046, None, 'default', 3, 'BashOperator', None, 78750, bytearray(b'\x80\x04}\x94.'))
2019-04-23 21:53:50,390 INFO sqlalchemy.engine.base.Engine ROLLBACK
2019-04-23 21:54:58,467 INFO sqlalchemy.engine.base.Engine SELECT 1
2019-04-23 21:54:58,467 INFO sqlalchemy.engine.base.Engine ()
2019-04-23 21:54:58,470 INFO sqlalchemy.engine.base.Engine BEGIN (implicit)
2019-04-23 21:54:58,472 INFO sqlalchemy.engine.base.Engine SELECT job.id AS job_id, job.dag_id AS job_dag_id, job.state AS job_state, job.job_type AS job_job_type, job.start_date AS job_start_date, job.end_date AS job_end_date, job.latest_heartbeat AS job_latest_heartbeat, job.executor_class AS job_executor_class, job.hostname AS job_hostname, job.unixname AS job_unixname 
FROM job 
WHERE job.id = ? AND job.job_type IN (?)
2019-04-23 21:54:58,473 INFO sqlalchemy.engine.base.Engine (2046, 'LocalTaskJob')
2019-04-23 21:54:58,480 INFO sqlalchemy.engine.base.Engine UPDATE job SET state=?, start_date=?, end_date=?, latest_heartbeat=? WHERE job.id = ?
2019-04-23 21:54:58,480 INFO sqlalchemy.engine.base.Engine ('failed', datetime.datetime(2019, 4, 24, 4, 49, 32, 942901, tzinfo=<Timezone [UTC]>), datetime.datetime(2019, 4, 24, 4, 53, 50, 392514, tzinfo=<Timezone [UTC]>), datetime.datetime(2019, 4, 24, 4, 49, 32, 943012, tzinfo=<Timezone [UTC]>), 2046)
2019-04-23 21:54:58,485 INFO sqlalchemy.engine.base.Engine COMMIT
Traceback (most recent call last):
 File ""/usr/local/lib/python3.7/site-packages/sqlalchemy/engine/base.py"", line 1236, in _execute_context
 cursor, statement, parameters, context
 File ""/usr/local/lib/python3.7/site-packages/sqlalchemy/engine/default.py"", line 536, in do_execute
 cursor.execute(statement, parameters)
pyodbc.IntegrityError: ('23000', ""[23000] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Violation of PRIMARY KEY constraint 'PK__task_ins__9BEABD04B1A321BB'. Cannot insert duplicate key in object 'dbo.task_instance'. The duplicate key value is (print_date, tutorial, Apr 20 2019 6:51PM). (2627) (SQLExecDirectW)"")

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
 File ""/Applications/PyCharm.app/Contents/helpers/pydev/pydevd.py"", line 1741, in <module>
 main()
 File ""/Applications/PyCharm.app/Contents/helpers/pydev/pydevd.py"", line 1735, in main
 globals = debugger.run(setup['file'], None, None, is_module)
 File ""/Applications/PyCharm.app/Contents/helpers/pydev/pydevd.py"", line 1135, in run
 pydev_imports.execfile(file, globals, locals) # execute the script
 File ""/Applications/PyCharm.app/Contents/helpers/pydev/_pydev_imps/_pydev_execfile.py"", line 18, in execfile
 exec(compile(contents+""\n"", file, 'exec'), glob, loc)
 File ""/Users/admin/Documents/Development/Java/airflow/airflow/bin/airflow"", line 32, in <module>
 args.func(args)
 File ""/Users/admin/Documents/Development/Java/airflow/airflow/utils/cli.py"", line 74, in wrapper
 return f(*args, **kwargs)
 File ""/Users/admin/Documents/Development/Java/airflow/airflow/bin/cli.py"", line 523, in run
 _run(args, dag, ti)
 File ""/Users/admin/Documents/Development/Java/airflow/airflow/bin/cli.py"", line 437, in _run
 run_job.run()
 File ""/Users/admin/Documents/Development/Java/airflow/airflow/jobs.py"", line 209, in run
 self._execute()
 File ""/Users/admin/Documents/Development/Java/airflow/airflow/jobs.py"", line 2548, in _execute
 pool=self.pool):
 File ""/Users/admin/Documents/Development/Java/airflow/airflow/utils/db.py"", line 73, in wrapper
 return func(*args, **kwargs)
 File ""/Users/admin/Documents/Development/Java/airflow/airflow/models/__init__.py"", line 1365, in _check_and_change_state_before_execution
 session.commit()
 File ""/usr/local/lib/python3.7/site-packages/sqlalchemy/orm/session.py"", line 1023, in commit
 self.transaction.commit()
 File ""/usr/local/lib/python3.7/site-packages/sqlalchemy/orm/session.py"", line 487, in commit
 self._prepare_impl()
 File ""/usr/local/lib/python3.7/site-packages/sqlalchemy/orm/session.py"", line 466, in _prepare_impl
 self.session.flush()
 File ""/usr/local/lib/python3.7/site-packages/sqlalchemy/orm/session.py"", line 2446, in flush
 self._flush(objects)
 File ""/usr/local/lib/python3.7/site-packages/sqlalchemy/orm/session.py"", line 2584, in _flush
 transaction.rollback(_capture_exception=True)
 File ""/usr/local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py"", line 67, in __exit__
 compat.reraise(exc_type, exc_value, exc_tb)
 File ""/usr/local/lib/python3.7/site-packages/sqlalchemy/util/compat.py"", line 277, in reraise
 raise value
 File ""/usr/local/lib/python3.7/site-packages/sqlalchemy/orm/session.py"", line 2544, in _flush
 flush_context.execute()
 File ""/usr/local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py"", line 416, in execute
 rec.execute(self)
 File ""/usr/local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py"", line 583, in execute
 uow,
 File ""/usr/local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py"", line 245, in save_obj
 insert,
 File ""/usr/local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py"", line 1063, in _emit_insert_statements
 c = cached_connections[connection].execute(statement, multiparams)
 File ""/usr/local/lib/python3.7/site-packages/sqlalchemy/engine/base.py"", line 980, in execute
 return meth(self, multiparams, params)
 File ""/usr/local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py"", line 273, in _execute_on_connection
 return connection._execute_clauseelement(self, multiparams, params)
 File ""/usr/local/lib/python3.7/site-packages/sqlalchemy/engine/base.py"", line 1099, in _execute_clauseelement
 distilled_params,
 File ""/usr/local/lib/python3.7/site-packages/sqlalchemy/engine/base.py"", line 1240, in _execute_context
 e, statement, parameters, cursor, context
 File ""/usr/local/lib/python3.7/site-packages/sqlalchemy/engine/base.py"", line 1458, in _handle_dbapi_exception
 util.raise_from_cause(sqlalchemy_exception, exc_info)
 File ""/usr/local/lib/python3.7/site-packages/sqlalchemy/util/compat.py"", line 296, in raise_from_cause
 reraise(type(exception), exception, tb=exc_tb, cause=cause)
 File ""/usr/local/lib/python3.7/site-packages/sqlalchemy/util/compat.py"", line 276, in reraise
 raise value.with_traceback(tb)
 File ""/usr/local/lib/python3.7/site-packages/sqlalchemy/engine/base.py"", line 1236, in _execute_context
 cursor, statement, parameters, context
 File ""/usr/local/lib/python3.7/site-packages/sqlalchemy/engine/default.py"", line 536, in do_execute
 cursor.execute(statement, parameters)
sqlalchemy.exc.IntegrityError: (pyodbc.IntegrityError) ('23000', ""[23000] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Violation of PRIMARY KEY constraint 'PK__task_ins__9BEABD04B1A321BB'. Cannot insert duplicate key in object 'dbo.task_instance'. The duplicate key value is (print_date, tutorial, Apr 20 2019 6:51PM). (2627) (SQLExecDirectW)"") [SQL: 'INSERT INTO task_instance (task_id, dag_id, execution_date, start_date, end_date, duration, state, try_number, max_tries, hostname, unixname, job_id, pool, queue, priority_weight, operator, queued_dttm, pid, executor_config) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)'] [parameters: ('print_date', 'tutorial', <Pendulum [2019-04-20T18:51:35.033000+00:00]>, datetime.datetime(2019, 4, 24, 4, 50, 41, 924763, tzinfo=<Timezone [UTC]>), None, None, 'running', 1, 1, 'bijays-mbp.hsd1.ca.comcast.net', 'admin', 2046, None, 'default', 3, 'BashOperator', None, 78750, bytearray(b'\x80\x04}\x94.'))] (Background on this error at: http://sqlalche.me/e/gkpj)

 ",deo.developer
AIRFLOW-4460,Remove __future__ import in models,"Remove __future__ should being done in https://issues.apache.org/jira/browse/AIRFLOW-4200, but in the same time we refactor models and move some code out of __init__.py. This Jira ticket to remove __future__ in models.",zhongjiajie
AIRFLOW-4459,To fix the issue that the DAG count in /home page may be wrong when DAG count is 0,"When there is no DAG (maybe no DAG file deployed yet, or no matching result when we search), the the DAG count at the right bottom corner may be wrong. It will be ""Showing 1 to 0  of 0 entries"", while it should be ""Showing 0 to 0  of 0 entries"".

On the other hand, if we provide URL argument ""page"" a too big value, let's say we only have 10 DAGs, and we visit ""[http://localhost:8080/home?page=|http://localhost:8080/home?page=10]3"", we may see results like ""Showing 21 to 10 of 10 entries"", which is wrong for sure as well.",xd-deng
AIRFLOW-4457,Enhance Task logs by providing the task context,"In Some scenarios where the airflow logs are being forwarded to external central destination like splunk, it may be cumbersome to search the logs corresponding to task execution amid all the airflow logs from different loggers.

There should be a way to append the task logs with some task context to facilitate searching those logs for the debugging or some other purpose.",kanishk
AIRFLOW-4452,Webserver and Scheduler keep crashing because of slackclient update,"Webserver and Scheduler get into a crash loop if Airflow is installed with slack dependencies.


Airflow relies on slackclient which released a new major version (2.0.0) today ([https://pypi.org/project/slackclient/#history]). This new version seems to be incompatible with Airflow causing the webserver to get into a crash loop.

The root cause of the issue is that Airflow doesn't pin requirements for slackclient:

[https://github.com/apache/airflow/blob/v1-10-stable/setup.py#L229]
{code:java}
slack = ['slackclient>=1.0.0']{code}
 

This is the exception in the logs due to this error:

 
{code:java}
File ""/Users/abhishek.ray/airflow/dags/test_dag.py"", line 3, in <module>
    from airflow.operators import SlackAPIPostOperator
  File ""/Users/abhishek.ray/.virtualenvs/airflow-test/lib/python3.6/site-packages/airflow/utils/helpers.py"", line 372, in __getattr__
    loaded_attribute = self._load_attribute(attribute)
  File ""/Users/abhishek.ray/.virtualenvs/airflow-test/lib/python3.6/site-packages/airflow/utils/helpers.py"", line 336, in _load_attribute
    self._loaded_modules[module] = imp.load_module(module, f, filename, description)
  File ""/Users/abhishek.ray/.virtualenvs/airflow-test/lib/python3.6/imp.py"", line 235, in load_module
    return load_source(name, filename, file)
  File ""/Users/abhishek.ray/.virtualenvs/airflow-test/lib/python3.6/imp.py"", line 172, in load_source
    module = _load(spec)
  File ""/Users/abhishek.ray/.virtualenvs/airflow-test/lib/python3.6/site-packages/airflow/operators/slack_operator.py"", line 24, in <module>
    from airflow.hooks.slack_hook import SlackHook
  File ""/Users/abhishek.ray/.virtualenvs/airflow-test/lib/python3.6/site-packages/airflow/hooks/slack_hook.py"", line 20, in <module>
    from slackclient import SlackClient
  File ""/Users/abhishek.ray/.virtualenvs/airflow-test/lib/python3.6/site-packages/slackclient/__init__.py"", line 1, in <module>
    from .client import SlackClient # noqa
  File ""/Users/abhishek.ray/.virtualenvs/airflow-test/lib/python3.6/site-packages/slackclient/client.py"", line 8, in <module>
    from .server import Server
  File ""/Users/abhishek.ray/.virtualenvs/airflow-test/lib/python3.6/site-packages/slackclient/server.py"", line 14, in <module>
    from websocket import create_connection
ModuleNotFoundError: No module named 'websocket'
{code}
 

 

This is how to reproduce this issue:

Install apache airflow with slack: 
{code:java}
pip install apache-airflow[slack]==1.10.1{code}
 

Create a DAG which uses *SlackAPIPostOperator*
{code:java}
from airflow import DAG
from airflow.operators.bash_operator import BashOperator
from airflow.operators import SlackAPIPostOperator

dag_default_args = {
    ""owner"": ""airflow"",
    ""depends_on_past"": False,
    ""start_date"": datetime(2019, 4, 22),
    ""email"": [""airflow@airflow.com""],
    ""email_on_failure"": False,
    ""email_on_retry"": False,
    ""retries"": 1,
    ""catchup"": True,
}


dag = DAG(""test_dag"", default_args=dag_default_args, schedule_interval=""@daily"")

BashOperator(task_id=""print_date"", bash_command=""date"", dag=dag){code}
 

I think the fix should be pretty straightforward to add a max version for slackclient.",taofeng
AIRFLOW-4451,[AIRFLOW-1814] converts namedtuples args in PythonOperators to lists,"Upgrading to Airflow 1.10.3 from Airflow 1.10.2 removed support for passing in `namedtuple`s as `op_kwargs` or `op_args` to `PythonOperator`. The specific PR that made the breaking change is [https://github.com/apache/airflow/pull/4691]

 ",aetherunbound
AIRFLOW-4450,has_dag_access does not handle form parameters,"With the update to Airflow 1.10.3, we noticed that users that have permission to clear task instances are no longer able to do so from the webserver, and are receiving an ""Access denied"" error. They are able to clear task instances from /taskinstance, but not from /clear.

I believe this is related to the change made in AIRFLOW-4240, which had the unintended side effect of breaking the has_dag_access decorator for POST requests.

This may affect other endpoints.",chris.mclennon
AIRFLOW-4449,Default permissions for custom roles,"By default, there are 4 core airflow user roles. These roles are well made and perform nicely. However, adding new custom roles seems to (by default) apply all ""User"" permissions to the new custom role. I attached some screen-shots showing custom roles being changed by the web server to include default ""User"" permissions. This is an issue as it prevents strict control of specific pipelines. At most, default permissions applied to custom roles should only include viewing privileges. This way the system admins can add read/edit/pause/etc. permissions for specific dags. 

 

I suggest changing the default permissions that are applied to all custom roles to a list of permissions similar to the ""Viewer"" role OR simply do not apply default permissions to custom roles and let admins handle assigning permissions or multiple custom roles to users. The latter is definitely the preferred functionality. 

Please note I am not suggesting a removal on the four base roles that come with airflow, simply different behavior when creating new roles. 

Below is a list of changed permissions to apply to custom roles if it is decided this is the best approach. (very similar to ""Viewer"" role) 

[can tries on Airflow, can graph on Airflow, can task on Airflow, can code on Airflow, can duration on Airflow, can landing times on Airflow, can pickle info on Airflow, can tree on Airflow, can rendered on Airflow, can gantt on Airflow, can blocked on Airflow, can task instances on Airflow, can log on Airflow, can index on Airflow, can dag stats on Airflow, can get logs with metadata on Airflow, can task stats on Airflow, can dag details on Airflow, can list on DagModelView, can show on DagModelView, can version on VersionView, can list on DagRunModelView, menu access on DAG Runs, menu access on Browse, can list on JobModelView, menu access on Jobs, can list on LogModelView, menu access on Logs, can list on SlaMissModelView, menu access on SLA Misses, can list on TaskInstanceModelView, menu access on Task Instances, menu access on Documentation, menu access on Docs, menu access on Version, menu access on About]

 ",taofeng
AIRFLOW-4447,Display task duration as human friendly format in Tree View,,ping.goblue
AIRFLOW-4446,Fix typos detected by github.com/client9/misspell,"Fixing typos is sometimes very hard. It's not so easy to visually review them. Recently, I discovered a very useful tool for it, [misspell](https://github.com/client9/misspell).

This pull request fixes minor typos detected by [misspell](https://github.com/client9/misspell) except for the false positives. If you would like me to work on other files as well, let me know.",a_soldatenko
AIRFLOW-4444,Parallelize handling of executor events,"The _process_executor_events function today performs sequential reading of DAGs when it encounters an event like : ""TI is queued but executor has marked TI as succeeded or failed"". Each such TI is individually parsed in sequence and then handled for failure.

This Jira proposes calling these events ghosts (like zombies, but different), make their discovery stateless, and handle them through the same framework created for zombies to provide parallelization. This will be a departure from existing logic.",kunalshah
AIRFLOW-4442,fix hive_tblproperties in HiveToDruidTransfer,"The param is set to None by default which is wrong as later in the code it tries to do:
{code:java}
hive_tblproperties.items(){code}

I will submit a PR for this",fokko
